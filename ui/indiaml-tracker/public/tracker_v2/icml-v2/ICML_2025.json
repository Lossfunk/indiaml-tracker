{
  "conference": "ICML 2025",
  "focus_country": "India",
  "total_papers": 83,
  "generated_at": "2025-07-06T10:37:29.314068",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "3Z827FtMNe",
      "title": "Great Models Think Alike and this Undermines AI Oversight",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46528",
      "pdf_url": "https://openreview.net/pdf?id=3Z827FtMNe",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shashwat_Goel1",
        "name": "Shashwat Goel",
        "name_site": null,
        "openreview_id": "~Shashwat_Goel1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://shash42.github.io/",
        "dblp_id": "300/8333.html",
        "google_scholar_url": "exaNV-0AAAAJ",
        "orcid": null,
        "linkedin_url": "shashwatgoel42/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Max Planck Institute (Germany)",
        "countries": [
          "Germany"
        ],
        "country_codes": [
          "DE"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u3n5wuRGTa",
      "title": "Discovering a Zero (Zero-Vector Class of Machine Learning)",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In Machine learning, separating data into classes is a very fundamental problem. A mathematical framework around the classes is presented in this work to deepen the understanding of classes. The classes are defined as vectors in a Vector Space, where addition corresponds to the union of classes, and scalar multiplication resembles set complement of classes. The Zero-Vector in the vector space corresponds to a class referred to as the Metta-Class. This discovery enables numerous applications. One such application, termed 'clear learning' in this work, focuses on learning the true nature (manifold) of the data instead of merely learning a boundary sufficient for classification. Another application, called 'unary class learning', involves learning a single class in isolation rather than learning by comparing two or more classes. Additionally, 'set operations on classes' is another application highlighted in this work. Furthermore, Continual Learning of classes is facilitated by smaller networks. The Metta-Class enables neural networks to learn only the data manifold; therefore, it can also be used for generation of new data. Results for the key applications are shown using the MNIST dataset. To further strengthen the claims, some results are also produced using the CIFAR-10 and ImageNet-1k embeddings. The code supporting these applications is publicly available at: github.com/hm-4/Metta-Class.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43749",
      "pdf_url": "https://openreview.net/pdf?id=u3n5wuRGTa",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harikrishna_Metta1",
        "name": "Harikrishna Metta",
        "name_site": "Harikrishna Metta, Venkatesh Babu Radhakrishnan",
        "openreview_id": "~Harikrishna_Metta1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "metta-harikrishna/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Jr5Al16MS",
      "title": "Near Optimal Best Arm Identification for Clustered Bandits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This work investigates the problem of best arm identification for multi-agent multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where each cluster solves a stochastic bandit problem. The mapping between agents and bandits is \\textit{a priori} unknown. Each bandit is associated with $K$ arms, and the goal is to identify the best arm for each agent under a $\\delta$-probably correct ($\\delta$-PC) framework, while minimizing sample complexity and communication overhead. We propose two novel algorithms: \\emph{Clustering then Best Arm Identification} (\\texttt{Cl-BAI}) and \\emph{Best Arm Identification then Clustering} (\\texttt{BAI-Cl}). \\texttt{Cl-BAI} employs a two-phase approach that first clusters agents based on the bandit problems they are learning, followed by identifying the best arm for each cluster. \\texttt{BAI-Cl} reverses the sequence by identifying the best arms first and then clustering agents accordingly. Both algorithms exploit the successive elimination framework to ensure computational efficiency and high accuracy. Theoretical analysis establishes $\\delta$-PC guarantees for both methods, derives bounds on their sample complexity, and provides a lower bound for the problem class. Moreover, when $M$ is small (a constant), we show that the sample complexity of (a variant of) \\texttt{BAI-Cl} is (order-wise) minimax optimal. Experiments on synthetic and real-world (Movie Lens, Yelp) data demonstrates the superior performance of the proposed algorithms in terms of sample and communication efficiency, particularly in settings where $M \\ll N$.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46538",
      "pdf_url": "https://openreview.net/pdf?id=3Jr5Al16MS",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yash1",
        "name": "Yash",
        "name_site": "Yash Kheshwani, Avishek Ghosh, Nikhil Karamchandani",
        "openreview_id": "~Yash1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "yash-keshwani-09b04321b",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CYJlJgEzZs",
      "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40165",
      "pdf_url": "https://openreview.net/pdf?id=CYJlJgEzZs",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Position Papers",
      "author": {
        "id": "~Yash_Goel1",
        "name": "Yash Goel",
        "name_site": "Yash Goel, Ayan Sengupta, Tanmoy Chakraborty",
        "openreview_id": "~Yash_Goel1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "yash-goel-6ba26322a/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 3.3333333333333335,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GYZLed4d3M",
      "title": "Position: Graph Matching Systems Deserve Better Benchmarks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Data sets used in recent work on graph similarity scoring and matching tasks suffer from significant limitations.  Using Graph Edit Distance (GED) as a showcase, we highlight pervasive issues such as train-test leakage and poor generalization, which have misguided the community's understanding and assessment of the capabilities of a method or model.\nThese limitations arise, in part, because preparing labeled data is computationally expensive for combinatorial graph problems.\nWe establish some key properties of GED that enable scalable data augmentation for training, and adversarial test set generation.\nTogether, our analysis, experiments and insights establish \nnew, sound guidelines for designing and evaluating future neural networks, and suggest open challenges for future research.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40161",
      "pdf_url": "https://openreview.net/pdf?id=GYZLed4d3M",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Position Papers",
      "author": {
        "id": "~Indradyumna_Roy1",
        "name": "Indradyumna Roy",
        "name_site": "Indradyumna Roy, Saswat Meher, Eeshaan Jain, Soumen Chakrabarti, Abir De",
        "openreview_id": "~Indradyumna_Roy1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://indradyumna.github.io/",
        "dblp_id": "124/9185.html",
        "google_scholar_url": "qb70i84AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GrF14Q0DNW",
      "title": "Feasible Action Search for Bandit Linear Programs via Thompson Sampling",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We study the 'feasible action search' (FAS) problem for linear bandits, wherein a learner attempts to discover a feasible point for a set of linear constraints $\\Phi_* a \\ge 0,$ without knowledge of the matrix $\\Phi_* \\in \\mathbb{R}^{m \\times d}$. A FAS learner selects a sequence of actions $a_t,$ and uses observations of the form $\\Phi_* a_t + \\mathrm{noise}$ to either find a point with nearly optimal 'safety margin', or detect that the constraints are infeasible, where the safety margin of an action measures its (signed) distance from the constraint boundary. While of interest in its own right, the FAS problem also directly addresses a key deficiency in the extant theory of 'safe linear bandits' (SLBs), by discovering a safe initialisation for low-regret SLB methods.\n\nWe propose and analyse a novel efficient FAS-learner. Our method, FAST, is based on Thompson Sampling. It applies a _coupled_ random perturbation to an estimate of $\\Phi_*,$ and plays a maximin point of a game induced by this perturbed matrix. We prove that FAST stops in $\\tilde{O}(d^3/\\varepsilon^2 M_*^2)$ steps, and incurs $\\tilde{O}(d^3/|M_*|)$ safety costs, to either correctly detect infeasibility, or output a point that is at least $(1-\\varepsilon) M_*$-safe, where $M_*$ is the _optimal safety margin_ of $\\Phi_*$. Further, instantiating prior SLB methods with the output of FAS yields the first SLB methods that incur $\\tilde{O}(\\sqrt{d^3 T/M_*^2})$ regret and $O(1)$ risk without a priori knowledge of a safe action. The main technical novelty lies in the extension of Thompson Sampling to this multiobjective setting, for which we both propose a coupled noise design, and provide an analysis that avoids convexity considerations.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45807",
      "pdf_url": "https://openreview.net/pdf?id=GrF14Q0DNW",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Gangrade1",
        "name": "Aditya Gangrade",
        "name_site": "Aditya Gangrade, Aditya Gopalan, Venkatesh Saligrama, Clay Scott",
        "openreview_id": "~Aditya_Gangrade1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Jnpgx8OzfD",
      "title": "Optimization Proxies using Limited Labeled Data and Training Time -- A Semi-Supervised Bayesian Neural Network Approach",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Constrained optimization problems arise in various engineering systems such as inventory management and power grids. Standard deep neural network (DNN) based machine learning proxies are ineffective in practical settings where labeled data is scarce and training times are limited. We propose a semi-supervised Bayesian Neural Networks (BNNs) based optimization proxy for this complex regime, wherein training commences in a sandwiched fashion, alternating between a supervised learning step for minimizing cost, and an unsupervised learning step for enforcing constraint feasibility. We show that the proposed semi-supervised BNN outperforms DNN architectures on important non-convex constrained optimization problems from energy network operations, achieving up to a tenfold reduction in expected maximum equality gap and halving the inequality gaps. Further, the BNN's ability to provide posterior samples is leveraged to construct practically meaningful probabilistic confidence bounds on performance using a limited validation data, unlike prior methods.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45674",
      "pdf_url": "https://openreview.net/pdf?id=Jnpgx8OzfD",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parikshit_Pareek1",
        "name": "Parikshit Pareek",
        "name_site": "Parikshit Pareek, Abhijith Jayakumar, Kaarthik Sundar, Sidhant Misra, Deepjyoti Deka",
        "openreview_id": "~Parikshit_Pareek1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://psquare-lab.github.io",
        "dblp_id": "249/9445",
        "google_scholar_url": "LIWKqnYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Roorkee (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LB5F02kwAv",
      "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel **Lang**evin **D**ata **Aug**mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45607",
      "pdf_url": "https://openreview.net/pdf?id=LB5F02kwAv",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Tiwary1",
        "name": "Piyush Tiwary",
        "name_site": "Piyush Lalitkumar Tiwary, Kinjawl Bhattacharyya, Prathosh AP",
        "openreview_id": "~Piyush_Tiwary1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://backpropagator.github.io/",
        "dblp_id": null,
        "google_scholar_url": "tUdHYloAAAAJ",
        "orcid": "0000-0002-4499-1059",
        "linkedin_url": "thebackpropogator/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NzoZXju2bL",
      "title": "GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a *program* that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45469",
      "pdf_url": "https://openreview.net/pdf?id=NzoZXju2bL",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samidha_Verma1",
        "name": "Samidha Verma",
        "name_site": "Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu",
        "openreview_id": "~Samidha_Verma1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "YPXPa7QAAAAJ",
        "orcid": null,
        "linkedin_url": "samidha-verma-a5b3b0125/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SrfwiloGQF",
      "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. \nWe demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45200",
      "pdf_url": "https://openreview.net/pdf?id=SrfwiloGQF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manan_Tayal1",
        "name": "Manan Tayal",
        "name_site": "Manan Tayal, Aditya Singh, Shishir Nadubettu Yadukumar, Somil Bansal",
        "openreview_id": "~Manan_Tayal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://tayalmanan28.github.io/",
        "dblp_id": "321/4243",
        "google_scholar_url": "https://scholar.google.com/citations?view_op=list_works",
        "orcid": null,
        "linkedin_url": "manan-tayal",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y4BDcJmb8t",
      "title": "Latent Mamba Operator for Partial Differential Equations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDEs solution.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44930",
      "pdf_url": "https://openreview.net/pdf?id=Y4BDcJmb8t",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karn_Tiwari1",
        "name": "Karn Tiwari",
        "name_site": "Karn Tiwari, Niladri Dutta, N M Anoop Krishnan, Prathosh AP",
        "openreview_id": "~Karn_Tiwari1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "karn3003/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aTQtGq7IyT",
      "title": "Be a Goldfish: Forgetting Bad Conditioning in Sparse Linear Regression via Variational Autoencoders",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Variational Autoencoders (VAEs), a class of latent-variable generative models, have seen extensive use in high-fidelity synthesis tasks, yet their loss landscape remains poorly understood. Prior theoretical works on VAE loss analysis have focused on their latent-space representational capabilities, both in the optimal and limiting cases. Although these insights have guided better VAE designs, they also often restrict VAEs to problem settings where classical algorithms, such as Principal Component Analysis (PCA), can trivially guarantee globally optimal solutions. In this work, we push the boundaries of our understanding of VAEs beyond these traditional regimes to tackle NP-hard sparse inverse problems, for which no classical algorithms exist. Specifically, we examine the nontrivial Sparse Linear Regression (SLR) problem of recovering optimal sparse inputs in the presence of an ill-conditioned design matrix having correlated features. We provably show that, under a linear encoder-decoder architecture incorporating the product of the SLR design matrix with a trainable, sparsity-promoting diagonal matrix, any minimum of VAE loss is guaranteed to be an optimal solution. This property is especially useful for identifying (a) a preconditioning factor that reduces the eigenvalue spread, and (b) the corresponding optimal sparse representation. Lastly, our empirical analysis with different types of design matrices validates these findings and even demonstrates a higher recovery rate at low sparsity where traditional algorithms fail. Overall, this work highlights the flexible nature of the VAE loss, which can be adapted to efficiently solve computationally hard problems under specific constraints.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44794",
      "pdf_url": "https://openreview.net/pdf?id=aTQtGq7IyT",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kuheli_Pratihar1",
        "name": "Kuheli Pratihar",
        "name_site": "Kuheli Pratihar, Debdeep Mukhopadhyay",
        "openreview_id": "~Kuheli_Pratihar1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "259/9138.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=MOGzpJEAAAAJ",
        "orcid": "0000-0003-4486-4903",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cuqvlLBQK6",
      "title": "Sample Efficient Demonstration Selection for In-Context Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The in-context learning paradigm with LLMs has been instrumental in advancing a wide range of natural language processing tasks. The selection of few-shot examples (exemplars / demonstration samples) is essential for constructing effective prompts under context-length budget constraints. In this paper, we formulate the exemplar selection task as a top-m best arms identification problem. A key challenge in this setup is the exponentially large number of arms that need to be evaluated to identify the m-best arms. We propose CASE (Challenger Arm Sampling for Exemplar selection), a novel sample-efficient selective exploration strategy that maintains a shortlist of “challenger” arms, which are current candidates for the top-m arms. In each iteration, only one of the arms from this shortlist or the current top-m set is pulled, thereby reducing sample complexity and, consequently, the number of LLM evaluations. Furthermore, we model the scores of exemplar subsets (arms) using a parameterized linear scoring function, leading to stochastic linear bandits setting. CASE achieves remarkable efficiency gains of up to 7× speedup in runtime while requiring 7× fewer LLM calls (87% reduction) without sacrificing performance compared to state-of-the-art exemplar selection methods. We release our code and data (https://github.com/kiranpurohit/CASE).",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44658",
      "pdf_url": "https://openreview.net/pdf?id=cuqvlLBQK6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kiran_Purohit1",
        "name": "Kiran Purohit",
        "name_site": "Kiran Purohit, Venktesh V, Sourangshu Bhattacharya, Avishek Anand",
        "openreview_id": "~Kiran_Purohit1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://kiranpurohit.github.io/",
        "dblp_id": "293/8210",
        "google_scholar_url": "KvaPPWAAAAAJ",
        "orcid": "0000-0002-5512-3441",
        "linkedin_url": "kiranpurohit789/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Varshita_Kolipaka1",
        "name": "Varshita Kolipaka",
        "name_site": "Varshita Kolipaka, Akshit Sinha, Debangan Mishra, Sumit Kumar, Arvindh Arun, Shashwat Goel, Ponnurangam Kumaraguru",
        "openreview_id": "~Varshita_Kolipaka1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "varshitakolipaka/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kcUNMKqrCg",
      "title": "Conditional Diffusion Model with Nonlinear Data Transformation for Time Series Forecasting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Time-series forecasting finds application across domains such as finance, climate science, and energy systems. We introduce the Conditional Diffusion  with Nonlinear Data Transformation Model (CN-Diff), a generative framework that employs novel nonlinear transformations and learnable conditions in the forward process for time series forecasting. A new loss formulation for training is proposed, along with a detailed derivation of both forward and reverse process. The new additions improve the diffusion model's capacity to capture complex time series patterns, thus simplifying the reverse process. Our novel condition facilitates learning an efficient prior distribution. This also reduces the gap between the true negative log-likelihood and its variational approximation. CN-Diff is shown to perform better than other leading time series models on nine real-world datasets. Ablation studies are conducted to elucidate the role of each component of CN-Diff.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44243",
      "pdf_url": "https://openreview.net/pdf?id=kcUNMKqrCg",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~J_Rishi1",
        "name": "J Rishi",
        "name_site": "RISHI JINKA, Venkata Sai Mothish Gonugunta, Deepak N. Subramani",
        "openreview_id": "~J_Rishi1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0004-5285-6908",
        "linkedin_url": "rishi-j/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian institute of science, Bangalore (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qF6mxani2X",
      "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43951",
      "pdf_url": "https://openreview.net/pdf?id=qF6mxani2X",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saksham_Rastogi2",
        "name": "Saksham Rastogi",
        "name_site": "Saksham Rastogi, Pratyush Maini, Danish Pruthi",
        "openreview_id": "~Saksham_Rastogi2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/codeboy5",
        "dblp_id": null,
        "google_scholar_url": "Ty2EujYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rCJNbDXkvC",
      "title": "Improved Coresets for Vertical Federated Learning: Regularized Linear and Logistic Regressions",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Coreset, as a summary of training data, offers an efficient approach for reducing data processing and storage complexity during training. In the emerging vertical federated learning (VFL) setting, where scattered clients store different data features, it directly reduces communication complexity. In this work, we introduce coresets construction for regularized logistic regression both in centralized and VFL settings. Additionally, we improve the coreset size for regularized linear regression in the VFL setting. We also eliminate the dependency of the coreset size on a property of the data due to the VFL setting. The improvement in the coreset sizes is due to our novel coreset construction algorithms that capture the reduced model complexity due to the added regularization and its subsequent analysis. In experiments, we provide extensive empirical evaluation that backs our theoretical claims. We also report the performance of our coresets by comparing the models trained on the complete data and on the coreset.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43903",
      "pdf_url": "https://openreview.net/pdf?id=rCJNbDXkvC",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Supratim_Shit1",
        "name": "Supratim Shit",
        "name_site": "Supratim Shit, Gurmehak chadha, Surendra kumar, Bapi Chatterjee",
        "openreview_id": "~Supratim_Shit1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/supratims",
        "dblp_id": "185/5317",
        "google_scholar_url": "BWHT4bUAAAAJ",
        "orcid": "0000-0002-6602-6436",
        "linkedin_url": "supratim-shit-95189251/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sqjQ6p56GR",
      "title": "Learning Condensed Graph via Differentiable Atom Mapping for Reaction Yield Prediction",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets.  In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges.  Here, we first  approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR),  which is a supergraph encompassing both the reactants and products. This CGR  serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder.\nOur experiments  show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43812",
      "pdf_url": "https://openreview.net/pdf?id=sqjQ6p56GR",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ankit_Ghosh2",
        "name": "Ankit Ghosh",
        "name_site": "Ankit Ghosh, Gargee Kashyap, Sarthak Mittal, Nupur Jain, Raghavan B Sunoj, Abir De",
        "openreview_id": "~Ankit_Ghosh2",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "ankit-ghosh-5a8221203",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "stfnyxnhAm",
      "title": "A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This work examines average-reward reinforcement learning with general policy parametrization. Existing state-of-the-art (SOTA) guarantees for this problem are either suboptimal or hindered by several challenges, including poor scalability with respect to the size of the state-action space, high iteration complexity, and a significant dependence on knowledge of mixing times and hitting times. To address these limitations, we propose a Multi-level Monte Carlo-based Natural Actor-Critic (MLMC-NAC) algorithm. Our work is the first to achieve a global convergence rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ for average-reward Markov Decision Processes (MDPs) (where $T$ is the horizon length), using an Actor-Critic approach. Moreover, the convergence rate does not scale with the size of the state space, therefore even being applicable to infinite state spaces.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43810",
      "pdf_url": "https://openreview.net/pdf?id=stfnyxnhAm",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swetha_Ganesh1",
        "name": "Swetha Ganesh",
        "name_site": "Swetha Ganesh, Washim Mondal, Vaneet Aggarwal",
        "openreview_id": "~Swetha_Ganesh1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "305/0457",
        "google_scholar_url": "-Crvn3cAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Purdue University (Unknown),Indian Institute of Science (India)",
        "countries": [
          "Unknown",
          "India"
        ],
        "country_codes": [
          "UN",
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tU8QKX4dMI",
      "title": "AlphaVerus: Bootstrapping Formally Verified Code Generation through Self-Improving Translation and Treefinement",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Automated code generation with large language models has gained significant traction, but there remains no guarantee of the correctness of generated code. We aim to use formal verification to provide mathematical guarantees that the generated code is correct. However, generating formally verified code with LLMs is hindered by the scarcity of training data and the complexity of formal proofs. To tackle this challenge, we introduce AlphaVerus, a self-improving framework that bootstraps formally verified code generation by iteratively translating programs from a higher-resource language and leveraging feedback from a verifier. AlphaVerus operates in three phases: exploration of candidate translations, Treefinement -- a novel tree search algorithm for program refinement using verifier feedback, and filtering misaligned specifications and programs to prevent reward hacking. Through this iterative process, AlphaVerus enables the LLaMA-3.1-70B model to generate verified code without human intervention or model finetuning. AlphaVerus shows an ability to generate formally verified solutions for HumanEval and MBPP, laying the groundwork for truly trustworthy code-generation agents.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43780",
      "pdf_url": "https://openreview.net/pdf?id=tU8QKX4dMI",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranjal_Aggarwal1",
        "name": "Pranjal Aggarwal",
        "name_site": "Pranjal Aggarwal, Ameet Deshpande, Karthik Narasimhan",
        "openreview_id": "~Pranjal_Aggarwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/Pranjal2041/",
        "dblp_id": "163/0764",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-2962-1535",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Carnegie Mellon University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u3n5wuRGTa",
      "title": "Discovering a Zero (Zero-Vector Class of Machine Learning)",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In Machine learning, separating data into classes is a very fundamental problem. A mathematical framework around the classes is presented in this work to deepen the understanding of classes. The classes are defined as vectors in a Vector Space, where addition corresponds to the union of classes, and scalar multiplication resembles set complement of classes. The Zero-Vector in the vector space corresponds to a class referred to as the Metta-Class. This discovery enables numerous applications. One such application, termed 'clear learning' in this work, focuses on learning the true nature (manifold) of the data instead of merely learning a boundary sufficient for classification. Another application, called 'unary class learning', involves learning a single class in isolation rather than learning by comparing two or more classes. Additionally, 'set operations on classes' is another application highlighted in this work. Furthermore, Continual Learning of classes is facilitated by smaller networks. The Metta-Class enables neural networks to learn only the data manifold; therefore, it can also be used for generation of new data. Results for the key applications are shown using the MNIST dataset. To further strengthen the claims, some results are also produced using the CIFAR-10 and ImageNet-1k embeddings. The code supporting these applications is publicly available at: github.com/hm-4/Metta-Class.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43749",
      "pdf_url": "https://openreview.net/pdf?id=u3n5wuRGTa",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uxA0GI240s",
      "title": "MOGIC: Metadata-infused Oracle Guidance for Improved Extreme Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Retrieval-augmented classification and generation models benefit from *early-stage fusion* of high-quality text-based metadata, often called memory, but face high latency and noise sensitivity. In extreme classification (XC), where low latency is crucial, existing methods use *late-stage fusion* for efficiency and robustness. To enhance accuracy while maintaining low latency, we propose MOGIC, a novel approach to metadata-infused oracle guidance for XC. We train an early-fusion oracle classifier with access to both query-side and label-side ground-truth metadata in textual form and subsequently use it to guide existing memory-based XC disciple models via regularization. The MOGIC algorithm improves precision@1 and propensity-scored precision@1 of XC disciple models by 1-2% on six standard datasets, at no additional inference-time cost. We show that MOGIC can be used in a plug-and-play manner to enhance memory-free XC models such as NGAME or DEXA. Lastly, we demonstrate the robustness of the MOGIC algorithm to missing and noisy metadata. The code is publicly available at [https://github.com/suchith720/mogic](https://github.com/suchith720/mogic).",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43708",
      "pdf_url": "https://openreview.net/pdf?id=uxA0GI240s",
      "github_url": "",
      "total_authors": 14,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suchith_Chidananda_Prabhu1",
        "name": "Suchith Chidananda Prabhu",
        "name_site": "Suchith Chidananda Prabhu, Bhavyajeet Singh, Anshul Mittal, Siddarth Asokan, Shikhar Mohan, Deepak Saini, Yashoteja Prabhu, Lakshya Kumar, Jian Jiao, Amit Singh, Niket Tandon, Manish Gupta, Sumeet Agarwal, Manik Varma",
        "openreview_id": "~Suchith_Chidananda_Prabhu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://suchith720.github.io",
        "dblp_id": null,
        "google_scholar_url": "-UmeTCYAAAAJ",
        "orcid": "0009-0007-4647-3304",
        "linkedin_url": "suchith-chidananda-prabhu-9a65a9158/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Jr5Al16MS",
      "title": "Near Optimal Best Arm Identification for Clustered Bandits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This work investigates the problem of best arm identification for multi-agent multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where each cluster solves a stochastic bandit problem. The mapping between agents and bandits is \\textit{a priori} unknown. Each bandit is associated with $K$ arms, and the goal is to identify the best arm for each agent under a $\\delta$-probably correct ($\\delta$-PC) framework, while minimizing sample complexity and communication overhead. We propose two novel algorithms: \\emph{Clustering then Best Arm Identification} (\\texttt{Cl-BAI}) and \\emph{Best Arm Identification then Clustering} (\\texttt{BAI-Cl}). \\texttt{Cl-BAI} employs a two-phase approach that first clusters agents based on the bandit problems they are learning, followed by identifying the best arm for each cluster. \\texttt{BAI-Cl} reverses the sequence by identifying the best arms first and then clustering agents accordingly. Both algorithms exploit the successive elimination framework to ensure computational efficiency and high accuracy. Theoretical analysis establishes $\\delta$-PC guarantees for both methods, derives bounds on their sample complexity, and provides a lower bound for the problem class. Moreover, when $M$ is small (a constant), we show that the sample complexity of (a variant of) \\texttt{BAI-Cl} is (order-wise) minimax optimal. Experiments on synthetic and real-world (Movie Lens, Yelp) data demonstrates the superior performance of the proposed algorithms in terms of sample and communication efficiency, particularly in settings where $M \\ll N$.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46538",
      "pdf_url": "https://openreview.net/pdf?id=3Jr5Al16MS",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nikhil_Karamchandani1",
        "name": "Nikhil Karamchandani",
        "name_site": null,
        "openreview_id": "~Nikhil_Karamchandani1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://sites.google.com/site/nikhilkaram/home",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "4Xvkpaikt4",
      "title": "The Missing Alignment Link of In-context Learning on Sequences",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Large language models (LLMs) have demonstrated the capability to perform in-context learning (ICL) for completely unseen tasks in classification or language completion.  Sequence to sequence (seq2seq) is another popular task category with several applications seeking quick adaptation with ICL. We present a systematic analysis of the ICL capability of LLMs on Seq2Seq tasks using a formal structured language-pair. Our study reveals a critical limitation: except for very short input sequences, ICL fails to achieve consistent learning across all output positions. This exposes a fundamental weakness of modern LLMs — their inability to effectively uncover the alignment between input and output sequences. Consequently, this limitation results in incomplete induction heads, which are essential for in-context learning of new discrete mappings.\n\nTo address these limitations, we propose ICA-Tune, a method for focused fine-tuning of an LLM using in-context examples. We present a mechanistic evaluation with two accuracy probes to show how input-output alignment emerges in middle layers of an LLM without direct supervision. This alignment leads to an abrupt jump in the completeness of  the induction heads in higher layers. We show that, compared to standard fine-tuning, ICA-Tune enables more sample efficient learning and better generalization to OOD instances.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46475",
      "pdf_url": "https://openreview.net/pdf?id=4Xvkpaikt4",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sunita_Sarawagi1",
        "name": "Sunita Sarawagi",
        "name_site": null,
        "openreview_id": "~Sunita_Sarawagi1",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://www.cse.iitb.ac.in/~sunita/",
        "dblp_id": "s/SunitaSarawagi",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=Hg4HmTAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A82tIFgJaK",
      "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \\textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46186",
      "pdf_url": "https://openreview.net/pdf?id=A82tIFgJaK",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Richa_Singh1",
        "name": "Richa Singh",
        "name_site": null,
        "openreview_id": "~Richa_Singh1",
        "position": 5,
        "gender": "F",
        "homepage_url": "http://home.iitj.ac.in/~richa/",
        "dblp_id": "75/3512",
        "google_scholar_url": "okqK5UAAAAAJ",
        "orcid": "0000-0003-4060-4573",
        "linkedin_url": "richa-singh-40ba237/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CYJlJgEzZs",
      "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40165",
      "pdf_url": "https://openreview.net/pdf?id=CYJlJgEzZs",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Position Papers",
      "author": {
        "id": "~Tanmoy_Chakraborty2",
        "name": "Tanmoy Chakraborty",
        "name_site": null,
        "openreview_id": "~Tanmoy_Chakraborty2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://tanmoychak.com",
        "dblp_id": "65/2136-2.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=C5S9JnIAAAAJ",
        "orcid": "0000-0002-0210-0369",
        "linkedin_url": "tanmoy-chakraborty-89553324/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 3.3333333333333335,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GYZLed4d3M",
      "title": "Position: Graph Matching Systems Deserve Better Benchmarks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Data sets used in recent work on graph similarity scoring and matching tasks suffer from significant limitations.  Using Graph Edit Distance (GED) as a showcase, we highlight pervasive issues such as train-test leakage and poor generalization, which have misguided the community's understanding and assessment of the capabilities of a method or model.\nThese limitations arise, in part, because preparing labeled data is computationally expensive for combinatorial graph problems.\nWe establish some key properties of GED that enable scalable data augmentation for training, and adversarial test set generation.\nTogether, our analysis, experiments and insights establish \nnew, sound guidelines for designing and evaluating future neural networks, and suggest open challenges for future research.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40161",
      "pdf_url": "https://openreview.net/pdf?id=GYZLed4d3M",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Position Papers",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LB5F02kwAv",
      "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel **Lang**evin **D**ata **Aug**mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45607",
      "pdf_url": "https://openreview.net/pdf?id=LB5F02kwAv",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NzoZXju2bL",
      "title": "GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a *program* that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45469",
      "pdf_url": "https://openreview.net/pdf?id=NzoZXju2bL",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y4BDcJmb8t",
      "title": "Latent Mamba Operator for Partial Differential Equations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDEs solution.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44930",
      "pdf_url": "https://openreview.net/pdf?id=Y4BDcJmb8t",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aTQtGq7IyT",
      "title": "Be a Goldfish: Forgetting Bad Conditioning in Sparse Linear Regression via Variational Autoencoders",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Variational Autoencoders (VAEs), a class of latent-variable generative models, have seen extensive use in high-fidelity synthesis tasks, yet their loss landscape remains poorly understood. Prior theoretical works on VAE loss analysis have focused on their latent-space representational capabilities, both in the optimal and limiting cases. Although these insights have guided better VAE designs, they also often restrict VAEs to problem settings where classical algorithms, such as Principal Component Analysis (PCA), can trivially guarantee globally optimal solutions. In this work, we push the boundaries of our understanding of VAEs beyond these traditional regimes to tackle NP-hard sparse inverse problems, for which no classical algorithms exist. Specifically, we examine the nontrivial Sparse Linear Regression (SLR) problem of recovering optimal sparse inputs in the presence of an ill-conditioned design matrix having correlated features. We provably show that, under a linear encoder-decoder architecture incorporating the product of the SLR design matrix with a trainable, sparsity-promoting diagonal matrix, any minimum of VAE loss is guaranteed to be an optimal solution. This property is especially useful for identifying (a) a preconditioning factor that reduces the eigenvalue spread, and (b) the corresponding optimal sparse representation. Lastly, our empirical analysis with different types of design matrices validates these findings and even demonstrates a higher recovery rate at low sparsity where traditional algorithms fail. Overall, this work highlights the flexible nature of the VAE loss, which can be adapted to efficiently solve computationally hard problems under specific constraints.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44794",
      "pdf_url": "https://openreview.net/pdf?id=aTQtGq7IyT",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debdeep_Mukhopadhyay2",
        "name": "Debdeep Mukhopadhyay",
        "name_site": null,
        "openreview_id": "~Debdeep_Mukhopadhyay2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/debdeepmukhopadhyay/",
        "dblp_id": null,
        "google_scholar_url": "2ELnl9IAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ryan_Cotterell1_2",
        "name": "Ryan Cotterell",
        "name_site": null,
        "openreview_id": "~Ponnurangam_Kumaraguru3",
        "position": 7,
        "gender": null,
        "homepage_url": "https://precog.iiit.ac.in/",
        "dblp_id": "97/5147.html",
        "google_scholar_url": "MfzQyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "ponguru/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kcUNMKqrCg",
      "title": "Conditional Diffusion Model with Nonlinear Data Transformation for Time Series Forecasting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Time-series forecasting finds application across domains such as finance, climate science, and energy systems. We introduce the Conditional Diffusion  with Nonlinear Data Transformation Model (CN-Diff), a generative framework that employs novel nonlinear transformations and learnable conditions in the forward process for time series forecasting. A new loss formulation for training is proposed, along with a detailed derivation of both forward and reverse process. The new additions improve the diffusion model's capacity to capture complex time series patterns, thus simplifying the reverse process. Our novel condition facilitates learning an efficient prior distribution. This also reduces the gap between the true negative log-likelihood and its variational approximation. CN-Diff is shown to perform better than other leading time series models on nine real-world datasets. Ablation studies are conducted to elucidate the role of each component of CN-Diff.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44243",
      "pdf_url": "https://openreview.net/pdf?id=kcUNMKqrCg",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Subramani1",
        "name": "Deepak Subramani",
        "name_site": null,
        "openreview_id": "~Deepak_Subramani1",
        "position": 3,
        "gender": null,
        "homepage_url": "http://cds.iisc.ac.in/faculty/deepakns/",
        "dblp_id": "171/6736",
        "google_scholar_url": "d-V0TTwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qF6mxani2X",
      "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43951",
      "pdf_url": "https://openreview.net/pdf?id=qF6mxani2X",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Danish_Pruthi1",
        "name": "Danish Pruthi",
        "name_site": null,
        "openreview_id": "~Danish_Pruthi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://danishpruthi.com/",
        "dblp_id": "192/7349",
        "google_scholar_url": "JpSx3EMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rCJNbDXkvC",
      "title": "Improved Coresets for Vertical Federated Learning: Regularized Linear and Logistic Regressions",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Coreset, as a summary of training data, offers an efficient approach for reducing data processing and storage complexity during training. In the emerging vertical federated learning (VFL) setting, where scattered clients store different data features, it directly reduces communication complexity. In this work, we introduce coresets construction for regularized logistic regression both in centralized and VFL settings. Additionally, we improve the coreset size for regularized linear regression in the VFL setting. We also eliminate the dependency of the coreset size on a property of the data due to the VFL setting. The improvement in the coreset sizes is due to our novel coreset construction algorithms that capture the reduced model complexity due to the added regularization and its subsequent analysis. In experiments, we provide extensive empirical evaluation that backs our theoretical claims. We also report the performance of our coresets by comparing the models trained on the complete data and on the coreset.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43903",
      "pdf_url": "https://openreview.net/pdf?id=rCJNbDXkvC",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Bapi_Chatterjee1",
        "name": "Bapi Chatterjee",
        "name_site": null,
        "openreview_id": "~Bapi_Chatterjee1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://bapi.github.io/",
        "dblp_id": "117/6988",
        "google_scholar_url": "ILI-_h4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sqjQ6p56GR",
      "title": "Learning Condensed Graph via Differentiable Atom Mapping for Reaction Yield Prediction",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets.  In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges.  Here, we first  approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR),  which is a supergraph encompassing both the reactants and products. This CGR  serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder.\nOur experiments  show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43812",
      "pdf_url": "https://openreview.net/pdf?id=sqjQ6p56GR",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yiZtLtvW9S",
      "title": "Policy Gradient with Tree Expansion",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43515",
      "pdf_url": "https://openreview.net/pdf?id=yiZtLtvW9S",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gal_Chechik1",
        "name": "Gal Chechik",
        "name_site": null,
        "openreview_id": "~Gal_Chechik1",
        "position": 5,
        "gender": null,
        "homepage_url": "https://chechiklab.biu.ac.il/~gal/",
        "dblp_id": "c/GalChechik",
        "google_scholar_url": "Wk2gAZUAAAAJ",
        "orcid": "0000-0001-9164-5303",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Bar-Ilan University (Unknown),NVIDIA (India)",
        "countries": [
          "Unknown",
          "India"
        ],
        "country_codes": [
          "UN",
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NIe74CY9lk",
      "title": "MGD$^3$ : Mode-Guided Dataset Distillation using Diffusion Models",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "Dataset distillation has emerged as an effective strategy, significantly reducing training costs and facilitating more efficient model deployment.\nRecent advances have leveraged generative models to distill datasets by capturing the underlying data distribution. Unfortunately, existing methods require model fine-tuning with distillation losses to encourage diversity and representativeness. However, these methods do not guarantee sample diversity, limiting their performance.\nWe propose a mode-guided diffusion model leveraging a pre-trained diffusion model without the need to fine-tune with distillation losses. Our approach addresses dataset diversity in three stages: Mode Discovery to identify distinct data modes, Mode Guidance to enhance intra-class diversity, and Stop Guidance to mitigate artifacts in synthetic samples that affect performance.\nWe evaluate our approach on ImageNette, ImageIDC, ImageNet-100, and ImageNet-1K, achieving accuracy improvements of 4.4%, 2.9%, 1.6%, and 1.6%, respectively, over state-of-the-art methods. Our method eliminates the need for fine-tuning diffusion models with distillation losses, significantly reducing computational costs.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45507",
      "pdf_url": "https://openreview.net/pdf?id=NIe74CY9lk",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Kumar_Nayak2",
        "name": "Gaurav Kumar Nayak",
        "name_site": null,
        "openreview_id": "~Gaurav_Kumar_Nayak2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/gauravnayak",
        "dblp_id": "241/6244",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=cLCeKTkAAAAJ",
        "orcid": "0000-0002-6406-6178",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Roorkee (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 5.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Akshit_Sinha2",
        "name": "Akshit Sinha",
        "name_site": null,
        "openreview_id": "~Akshit_Sinha2",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0006-0926-5437",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Z827FtMNe",
      "title": "Great Models Think Alike and this Undermines AI Oversight",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46528",
      "pdf_url": "https://openreview.net/pdf?id=3Z827FtMNe",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ryan_Cotterell1_2",
        "name": "Ryan Cotterell",
        "name_site": null,
        "openreview_id": "~Ponnurangam_Kumaraguru3",
        "position": 5,
        "gender": null,
        "homepage_url": "https://precog.iiit.ac.in/",
        "dblp_id": "97/5147.html",
        "google_scholar_url": "MfzQyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "ponguru/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A82tIFgJaK",
      "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \\textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46186",
      "pdf_url": "https://openreview.net/pdf?id=A82tIFgJaK",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjeev_Chiranjeev1",
        "name": "Chiranjeev Chiranjeev",
        "name_site": null,
        "openreview_id": "~Chiranjeev_Chiranjeev1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/iitj.ac.in/chiranjeev/home",
        "dblp_id": "365/6962",
        "google_scholar_url": "cibl9gMAAAAJ",
        "orcid": "0000-0003-3026-1255",
        "linkedin_url": "chiranjeev-chiranjeev-566723175/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NzoZXju2bL",
      "title": "GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a *program* that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45469",
      "pdf_url": "https://openreview.net/pdf?id=NzoZXju2bL",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arushi_Goyal1",
        "name": "Arushi Goyal",
        "name_site": null,
        "openreview_id": "~Arushi_Goyal1",
        "position": 2,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "arushigoyal29",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yiZtLtvW9S",
      "title": "Policy Gradient with Tree Expansion",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43515",
      "pdf_url": "https://openreview.net/pdf?id=yiZtLtvW9S",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Assaf_Hallak1",
        "name": "Assaf Hallak",
        "name_site": null,
        "openreview_id": "~Assaf_Hallak1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "117/9126",
        "google_scholar_url": null,
        "orcid": "0000-0001-7915-9206",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NVIDIA (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SrfwiloGQF",
      "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. \nWe demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45200",
      "pdf_url": "https://openreview.net/pdf?id=SrfwiloGQF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Singh7",
        "name": "Aditya Singh",
        "name_site": null,
        "openreview_id": "~Aditya_Singh7",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "P3Qz_eAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y4BDcJmb8t",
      "title": "Latent Mamba Operator for Partial Differential Equations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDEs solution.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44930",
      "pdf_url": "https://openreview.net/pdf?id=Y4BDcJmb8t",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Niladri_Dutta1",
        "name": "Niladri Dutta",
        "name_site": null,
        "openreview_id": "~Niladri_Dutta1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "niladri-dutta-a59462245?trk=contact-info",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debangan_Mishra1",
        "name": "Debangan Mishra",
        "name_site": null,
        "openreview_id": "~Debangan_Mishra1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "394/5439",
        "google_scholar_url": "PnRWab4AAAAJ",
        "orcid": null,
        "linkedin_url": "debangan-mishra-1a1a34209/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qBtomgvLwn",
      "title": "Validating Mechanistic Interpretations: An Axiomatic Approach",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Mechanistic interpretability aims to reverse engineer the computation performed by a neural network in terms of its internal components. Although there is a growing body of research on mechanistic interpretation of neural networks, the notion of a *mechanistic interpretation* itself is often ad-hoc. Inspired by the notion of abstract interpretation from the program analysis literature that aims to develop approximate semantics for programs, we give a set of axioms that formally characterize a mechanistic interpretation as a description that approximately captures the semantics of the neural network under analysis in a compositional manner. We demonstrate the applicability of these axioms for validating mechanistic interpretations on an existing, well-known interpretability study as well as on a new case study involving a Transformer-based model trained to solve the well-known 2-SAT problem.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43956",
      "pdf_url": "https://openreview.net/pdf?id=qBtomgvLwn",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Zifan_Wang1",
        "name": "Zifan Wang",
        "name_site": null,
        "openreview_id": "~Zifan_Wang1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.zifanw.net",
        "dblp_id": null,
        "google_scholar_url": "HJOP3wMAAAAJ",
        "orcid": null,
        "linkedin_url": "zifan-wang-sail/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Scale AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Z827FtMNe",
      "title": "Great Models Think Alike and this Undermines AI Oversight",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46528",
      "pdf_url": "https://openreview.net/pdf?id=3Z827FtMNe",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Douwe_Kiela1",
        "name": "Douwe Kiela",
        "name_site": null,
        "openreview_id": "~Douwe_Kiela1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://douwekiela.github.io",
        "dblp_id": "136/9140",
        "google_scholar_url": "Q0piorUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Stanford University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.8125,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3B6fF1PxYD",
      "title": "NextCoder: Robust Adaptation of Code LMs to Diverse Code Edits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at http://aka.ms/nextcoder.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46548",
      "pdf_url": "https://openreview.net/pdf?id=3B6fF1PxYD",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhijeet_Awasthi1",
        "name": "Abhijeet Awasthi",
        "name_site": null,
        "openreview_id": "~Abhijeet_Awasthi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~awasthi/",
        "dblp_id": "233/8164",
        "google_scholar_url": "dqHSxE8AAAAJ",
        "orcid": null,
        "linkedin_url": "awasthiabhijeet/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Jr5Al16MS",
      "title": "Near Optimal Best Arm Identification for Clustered Bandits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This work investigates the problem of best arm identification for multi-agent multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where each cluster solves a stochastic bandit problem. The mapping between agents and bandits is \\textit{a priori} unknown. Each bandit is associated with $K$ arms, and the goal is to identify the best arm for each agent under a $\\delta$-probably correct ($\\delta$-PC) framework, while minimizing sample complexity and communication overhead. We propose two novel algorithms: \\emph{Clustering then Best Arm Identification} (\\texttt{Cl-BAI}) and \\emph{Best Arm Identification then Clustering} (\\texttt{BAI-Cl}). \\texttt{Cl-BAI} employs a two-phase approach that first clusters agents based on the bandit problems they are learning, followed by identifying the best arm for each cluster. \\texttt{BAI-Cl} reverses the sequence by identifying the best arms first and then clustering agents accordingly. Both algorithms exploit the successive elimination framework to ensure computational efficiency and high accuracy. Theoretical analysis establishes $\\delta$-PC guarantees for both methods, derives bounds on their sample complexity, and provides a lower bound for the problem class. Moreover, when $M$ is small (a constant), we show that the sample complexity of (a variant of) \\texttt{BAI-Cl} is (order-wise) minimax optimal. Experiments on synthetic and real-world (Movie Lens, Yelp) data demonstrates the superior performance of the proposed algorithms in terms of sample and communication efficiency, particularly in settings where $M \\ll N$.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46538",
      "pdf_url": "https://openreview.net/pdf?id=3Jr5Al16MS",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Avishek_Ghosh2",
        "name": "Avishek Ghosh",
        "name_site": "Avishek Ghosh, Arya Mazumdar",
        "openreview_id": "~Avishek_Ghosh2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/avishekghosh",
        "dblp_id": "98/275",
        "google_scholar_url": "8y0Dg5cAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A82tIFgJaK",
      "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \\textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46186",
      "pdf_url": "https://openreview.net/pdf?id=A82tIFgJaK",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kartik_Thakral1",
        "name": "Kartik Thakral",
        "name_site": null,
        "openreview_id": "~Kartik_Thakral1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/iitj.ac.in/thakralkartik",
        "dblp_id": "256/4230",
        "google_scholar_url": "dSqzxC4AAAAJ",
        "orcid": "0000-0002-2528-9950",
        "linkedin_url": "thakral-kartik?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BGLN%2FUjpdSz2kzFL0fm6ROg%3D%3D",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CYJlJgEzZs",
      "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40165",
      "pdf_url": "https://openreview.net/pdf?id=CYJlJgEzZs",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Position Papers",
      "author": {
        "id": "~Ayan_Sengupta1",
        "name": "Ayan Sengupta",
        "name_site": null,
        "openreview_id": "~Ayan_Sengupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://victor7246.github.io/",
        "dblp_id": null,
        "google_scholar_url": "90EGfboAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 3.3333333333333335,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GYZLed4d3M",
      "title": "Position: Graph Matching Systems Deserve Better Benchmarks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Data sets used in recent work on graph similarity scoring and matching tasks suffer from significant limitations.  Using Graph Edit Distance (GED) as a showcase, we highlight pervasive issues such as train-test leakage and poor generalization, which have misguided the community's understanding and assessment of the capabilities of a method or model.\nThese limitations arise, in part, because preparing labeled data is computationally expensive for combinatorial graph problems.\nWe establish some key properties of GED that enable scalable data augmentation for training, and adversarial test set generation.\nTogether, our analysis, experiments and insights establish \nnew, sound guidelines for designing and evaluating future neural networks, and suggest open challenges for future research.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40161",
      "pdf_url": "https://openreview.net/pdf?id=GYZLed4d3M",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Position Papers",
      "author": {
        "id": "~Eeshaan_Jain1",
        "name": "Eeshaan Jain",
        "name_site": null,
        "openreview_id": "~Eeshaan_Jain1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://eeshaanjain.github.io",
        "dblp_id": null,
        "google_scholar_url": "r5rqqJEAAAAJ",
        "orcid": null,
        "linkedin_url": "eeshaanjain/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "EPFL (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LB5F02kwAv",
      "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel **Lang**evin **D**ata **Aug**mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45607",
      "pdf_url": "https://openreview.net/pdf?id=LB5F02kwAv",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kinjawl_Bhattacharyya1",
        "name": "Kinjawl Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Kinjawl_Bhattacharyya1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "363/9587",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-9617-7350",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NzoZXju2bL",
      "title": "GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a *program* that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45469",
      "pdf_url": "https://openreview.net/pdf?id=NzoZXju2bL",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ananya_Mathur1",
        "name": "Ananya Mathur",
        "name_site": null,
        "openreview_id": "~Ananya_Mathur1",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "ananya-mathur18",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "egqOSr5gGw",
      "title": "Instance-Optimal Pure Exploration for Linear Bandits on Continuous Arms",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper studies a pure exploration problem with linear bandit feedback on continuous arm sets, aiming to identify an $\\epsilon$-optimal arm with high probability. Previous approaches for continuous arm sets have employed instance-independent methods due to technical challenges such as the infinite dimensionality of the space of probability measures and the non-smoothness of the objective function. This paper proposes a novel, tractable algorithm that addresses these challenges by leveraging a reparametrization of the sampling distribution and projected subgradient descent. However, this approach introduces new challenges related to the projection and reconstruction of the distribution from the reparametrization. We address these by focusing on the connection to the approximate Carath\\'eodory  problem. Compared to the original optimization problem on the infinite-dimensional space, our method is tractable, requiring only the solution of quadratic and fractional quadratic problems on the arm set. We establish an instance-dependent optimality for our method, and empirical results on synthetic data demonstrate its superiority over existing instance-independent baselines.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44570",
      "pdf_url": "https://openreview.net/pdf?id=egqOSr5gGw",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yuhei_Umeda1",
        "name": "Yuhei Umeda",
        "name_site": null,
        "openreview_id": "~Yuhei_Umeda1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "59/8030",
        "google_scholar_url": "https://scholar.google.co.jp/citations?user=8NWN3xAAAAAJ",
        "orcid": null,
        "linkedin_url": "yuheiumeda-9868221b2/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu LIMITED (Japan),Kyushu University (India)",
        "countries": [
          "Japan",
          "India"
        ],
        "country_codes": [
          "JP",
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumit_Kumar6",
        "name": "Sumit Kumar",
        "name_site": null,
        "openreview_id": "~Sumit_Kumar6",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "D834jRsAAAAJ",
        "orcid": null,
        "linkedin_url": "sumitkk10/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kcUNMKqrCg",
      "title": "Conditional Diffusion Model with Nonlinear Data Transformation for Time Series Forecasting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Time-series forecasting finds application across domains such as finance, climate science, and energy systems. We introduce the Conditional Diffusion  with Nonlinear Data Transformation Model (CN-Diff), a generative framework that employs novel nonlinear transformations and learnable conditions in the forward process for time series forecasting. A new loss formulation for training is proposed, along with a detailed derivation of both forward and reverse process. The new additions improve the diffusion model's capacity to capture complex time series patterns, thus simplifying the reverse process. Our novel condition facilitates learning an efficient prior distribution. This also reduces the gap between the true negative log-likelihood and its variational approximation. CN-Diff is shown to perform better than other leading time series models on nine real-world datasets. Ablation studies are conducted to elucidate the role of each component of CN-Diff.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44243",
      "pdf_url": "https://openreview.net/pdf?id=kcUNMKqrCg",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~GVS_Mothish1",
        "name": "GVS Mothish",
        "name_site": null,
        "openreview_id": "~GVS_Mothish1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "LT98_oEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yiZtLtvW9S",
      "title": "Policy Gradient with Tree Expansion",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43515",
      "pdf_url": "https://openreview.net/pdf?id=yiZtLtvW9S",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gugan_Thoppe1",
        "name": "Gugan Thoppe",
        "name_site": "Gugan Chandrashekhar Mallika Thoppe, Prashanth L.A., Sanjay Bhat",
        "openreview_id": "~Gugan_Thoppe1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "117/3710",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=X5zV3s8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (Israel)",
        "countries": [
          "Israel"
        ],
        "country_codes": [
          "IL"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MHiTGWDbIb",
      "title": "IT$^3$: Idempotent Test-Time Training",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Deep learning models often struggle when deployed in real-world settings due to distribution shifts between training and test data. While existing approaches like domain adaptation and test-time training (TTT) offer partial solutions, they typically require additional data or domain-specific auxiliary tasks. We present Idempotent Test-Time Training (IT3), a novel approach that enables on-the-fly adaptation to distribution shifts using only the current test instance, without any auxiliary task design. Our key insight is that enforcing idempotence---where repeated applications of a function yield the same result---can effectively replace domain-specific auxiliary tasks used in previous TTT methods. We theoretically connect idempotence to prediction confidence and demonstrate that minimizing the distance between successive applications of our model during inference leads to improved out-of-distribution performance. Extensive experiments across diverse domains (including image classification, aerodynamics prediction, and aerial segmentation) and architectures (MLPs, CNNs, GNNs) show that IT3 consistently outperforms existing approaches while being simpler and more widely applicable. Our results suggest that idempotence provides a universal principle for test-time adaptation that generalizes across domains and architectures.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/45551",
      "pdf_url": "https://openreview.net/pdf?id=MHiTGWDbIb",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gal_Chechik1",
        "name": "Gal Chechik",
        "name_site": null,
        "openreview_id": "~Gal_Chechik1",
        "position": 4,
        "gender": null,
        "homepage_url": "https://chechiklab.biu.ac.il/~gal/",
        "dblp_id": "c/GalChechik",
        "google_scholar_url": "Wk2gAZUAAAAJ",
        "orcid": "0000-0001-9164-5303",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Bar-Ilan University (Unknown),NVIDIA (Unknown)",
        "countries": [
          "Unknown"
        ],
        "country_codes": [
          "UN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cKnzZrIJBR",
      "title": "Learning Fused State Representations for Control from Multi-View Observations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose **M**ulti-view **F**usion **S**tate for **C**ontrol (**MFSC**), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC’s robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance. The project code is available at [https://github.com/zpwdev/MFSC](https://github.com/zpwdev/MFSC).",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44691",
      "pdf_url": "https://openreview.net/pdf?id=cKnzZrIJBR",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yonathan_Efroni2_1",
        "name": "Yonathan Efroni",
        "name_site": null,
        "openreview_id": "~Hongyu_Zang1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://zanghyu.github.io/",
        "dblp_id": "212/2592.html",
        "google_scholar_url": "2kmSy50AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Meituan (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sqjQ6p56GR",
      "title": "Learning Condensed Graph via Differentiable Atom Mapping for Reaction Yield Prediction",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets.  In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges.  Here, we first  approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR),  which is a supergraph encompassing both the reactants and products. This CGR  serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder.\nOur experiments  show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43812",
      "pdf_url": "https://openreview.net/pdf?id=sqjQ6p56GR",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nupur_Jain1",
        "name": "Nupur Jain",
        "name_site": null,
        "openreview_id": "~Nupur_Jain1",
        "position": 4,
        "gender": "F",
        "homepage_url": "https://nupurjain2788.wixsite.com/nupur-jain/about-me",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "nupur-jain-2718b21b1",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36hVB7DEB0",
      "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100% training accuracy in the training process. It is often taken as an example of \"emergence\", where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46553",
      "pdf_url": "https://openreview.net/pdf?id=36hVB7DEB0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mikhail_Belkin1",
        "name": "Mikhail Belkin",
        "name_site": null,
        "openreview_id": "~Parthe_Pandit1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://parthe.github.io",
        "dblp_id": "166/6545",
        "google_scholar_url": "gp_Gdr8AAAAJ",
        "orcid": "0000-0002-2524-8817",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.9999999999999996,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y4BDcJmb8t",
      "title": "Latent Mamba Operator for Partial Differential Equations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDEs solution.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44930",
      "pdf_url": "https://openreview.net/pdf?id=Y4BDcJmb8t",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cuqvlLBQK6",
      "title": "Sample Efficient Demonstration Selection for In-Context Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The in-context learning paradigm with LLMs has been instrumental in advancing a wide range of natural language processing tasks. The selection of few-shot examples (exemplars / demonstration samples) is essential for constructing effective prompts under context-length budget constraints. In this paper, we formulate the exemplar selection task as a top-m best arms identification problem. A key challenge in this setup is the exponentially large number of arms that need to be evaluated to identify the m-best arms. We propose CASE (Challenger Arm Sampling for Exemplar selection), a novel sample-efficient selective exploration strategy that maintains a shortlist of “challenger” arms, which are current candidates for the top-m arms. In each iteration, only one of the arms from this shortlist or the current top-m set is pulled, thereby reducing sample complexity and, consequently, the number of LLM evaluations. Furthermore, we model the scores of exemplar subsets (arms) using a parameterized linear scoring function, leading to stochastic linear bandits setting. CASE achieves remarkable efficiency gains of up to 7× speedup in runtime while requiring 7× fewer LLM calls (87% reduction) without sacrificing performance compared to state-of-the-art exemplar selection methods. We release our code and data (https://github.com/kiranpurohit/CASE).",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44658",
      "pdf_url": "https://openreview.net/pdf?id=cuqvlLBQK6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourangshu_Bhattacharya1",
        "name": "Sourangshu Bhattacharya",
        "name_site": null,
        "openreview_id": "~Sourangshu_Bhattacharya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~sourangshu/",
        "dblp_id": "http://dblp.uni-trier.de/pers/hd/b/Bhattacharya:Sourangshu",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=IixRsP0AAAAJ",
        "orcid": null,
        "linkedin_url": "sourangshubhattacharya",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arvindh_Arun1",
        "name": "Arvindh Arun",
        "name_site": null,
        "openreview_id": "~Arvindh_Arun1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://arvindh75.github.io/",
        "dblp_id": "322/6851",
        "google_scholar_url": "MtlFa6gAAAAJ",
        "orcid": "0000-0003-3469-6539",
        "linkedin_url": "arvindh75/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Stuttgart (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rCJNbDXkvC",
      "title": "Improved Coresets for Vertical Federated Learning: Regularized Linear and Logistic Regressions",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Coreset, as a summary of training data, offers an efficient approach for reducing data processing and storage complexity during training. In the emerging vertical federated learning (VFL) setting, where scattered clients store different data features, it directly reduces communication complexity. In this work, we introduce coresets construction for regularized logistic regression both in centralized and VFL settings. Additionally, we improve the coreset size for regularized linear regression in the VFL setting. We also eliminate the dependency of the coreset size on a property of the data due to the VFL setting. The improvement in the coreset sizes is due to our novel coreset construction algorithms that capture the reduced model complexity due to the added regularization and its subsequent analysis. In experiments, we provide extensive empirical evaluation that backs our theoretical claims. We also report the performance of our coresets by comparing the models trained on the complete data and on the coreset.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43903",
      "pdf_url": "https://openreview.net/pdf?id=rCJNbDXkvC",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Surendra_kumar1",
        "name": "Surendra kumar",
        "name_site": null,
        "openreview_id": "~Surendra_kumar1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "surendra-bishnoi-a549a6247/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A82tIFgJaK",
      "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \\textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/46186",
      "pdf_url": "https://openreview.net/pdf?id=A82tIFgJaK",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mayank_Vatsa1",
        "name": "Mayank Vatsa",
        "name_site": null,
        "openreview_id": "~Mayank_Vatsa1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://home.iitj.ac.in/~mvatsa/",
        "dblp_id": "58/323",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=-DAxp-cAAAAJ",
        "orcid": "0000-0001-5952-2274",
        "linkedin_url": "mayankvatsa/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.25,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GYZLed4d3M",
      "title": "Position: Graph Matching Systems Deserve Better Benchmarks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Data sets used in recent work on graph similarity scoring and matching tasks suffer from significant limitations.  Using Graph Edit Distance (GED) as a showcase, we highlight pervasive issues such as train-test leakage and poor generalization, which have misguided the community's understanding and assessment of the capabilities of a method or model.\nThese limitations arise, in part, because preparing labeled data is computationally expensive for combinatorial graph problems.\nWe establish some key properties of GED that enable scalable data augmentation for training, and adversarial test set generation.\nTogether, our analysis, experiments and insights establish \nnew, sound guidelines for designing and evaluating future neural networks, and suggest open challenges for future research.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/40161",
      "pdf_url": "https://openreview.net/pdf?id=GYZLed4d3M",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Position Papers",
      "author": {
        "id": "~Soumen_Chakrabarti1",
        "name": "Soumen Chakrabarti",
        "name_site": null,
        "openreview_id": "~Soumen_Chakrabarti1",
        "position": 4,
        "gender": "Not Specified",
        "homepage_url": "https://www.cse.iitb.ac.in/~soumen/",
        "dblp_id": "c/SChakrabarti",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=LfF2zfQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.25,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sqjQ6p56GR",
      "title": "Learning Condensed Graph via Differentiable Atom Mapping for Reaction Yield Prediction",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets.  In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges.  Here, we first  approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR),  which is a supergraph encompassing both the reactants and products. This CGR  serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder.\nOur experiments  show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43812",
      "pdf_url": "https://openreview.net/pdf?id=sqjQ6p56GR",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Raghavan_B_Sunoj1",
        "name": "Raghavan B Sunoj",
        "name_site": null,
        "openreview_id": "~Raghavan_B_Sunoj1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.chem.iitb.ac.in/facultyuserview/r-b-sunoj",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-6484-2878",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "epDkt44mkq",
      "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/44563",
      "pdf_url": "https://openreview.net/pdf?id=epDkt44mkq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shashwat_Goel1",
        "name": "Shashwat Goel",
        "name_site": null,
        "openreview_id": "~Shashwat_Goel1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://shash42.github.io/",
        "dblp_id": "300/8333.html",
        "google_scholar_url": "exaNV-0AAAAJ",
        "orcid": null,
        "linkedin_url": "shashwatgoel42/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Max Planck Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8333333333333331,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uxA0GI240s",
      "title": "MOGIC: Metadata-infused Oracle Guidance for Improved Extreme Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Retrieval-augmented classification and generation models benefit from *early-stage fusion* of high-quality text-based metadata, often called memory, but face high latency and noise sensitivity. In extreme classification (XC), where low latency is crucial, existing methods use *late-stage fusion* for efficiency and robustness. To enhance accuracy while maintaining low latency, we propose MOGIC, a novel approach to metadata-infused oracle guidance for XC. We train an early-fusion oracle classifier with access to both query-side and label-side ground-truth metadata in textual form and subsequently use it to guide existing memory-based XC disciple models via regularization. The MOGIC algorithm improves precision@1 and propensity-scored precision@1 of XC disciple models by 1-2% on six standard datasets, at no additional inference-time cost. We show that MOGIC can be used in a plug-and-play manner to enhance memory-free XC models such as NGAME or DEXA. Lastly, we demonstrate the robustness of the MOGIC algorithm to missing and noisy metadata. The code is publicly available at [https://github.com/suchith720/mogic](https://github.com/suchith720/mogic).",
      "tldr": "",
      "site_url": "https://icml.cc/virtual/2025/poster/43708",
      "pdf_url": "https://openreview.net/pdf?id=uxA0GI240s",
      "github_url": "",
      "total_authors": 14,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumeet_Agarwal1",
        "name": "Sumeet Agarwal",
        "name_site": null,
        "openreview_id": "~Sumeet_Agarwal1",
        "position": 13,
        "gender": "M",
        "homepage_url": "http://web.iitd.ac.in/~sumeet",
        "dblp_id": "25/6438",
        "google_scholar_url": "https://scholar.google.co.uk/citations?user=vEsSCZsAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.38461538461538436,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FswxmvMOSG",
      "title": "A New Rejection Sampling Approach to $k\\text{-}\\mathtt{means}$++ with Improved Tradeoffs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The $k$-$\\mathtt{means}$++ seeding algorithm  (Arthur & Vassilvitskii, 2007a)  is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\\mathcal{X} \\subset \\mathbb{R} ^d$ into $k$ clusters.\n The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\\mathcal{X}|kd)$, making it expensive for large datasets. \n In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\\mathtt{means}$++. \n Our first method runs in time $\\tilde{O}(\\mathtt{nnz} (\\mathcal{X}) + \\beta k^2d)$\n while still being $O(\\log k )$ competitive in expectation. Here, $\\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\\mathtt{means}$ cost in expectation and $\\tilde{O}$ hides logarithmic factors in $k$ and $|\\mathcal{X}|$. \n Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\\Omega( m/\\beta)} \\operatorname{Var} (\\mathcal{X})$ in addition to the $O(\\log k)$ guarantee of $k$-$\\mathtt{means}$++ improving upon the result of (Bachem et al., 2016a) who get an additional factor of $m^{-1}\\operatorname{Var}(\\mathcal{X})$  while still running in time $\\tilde{O}(\\mathtt{nnz}(\\mathcal{X}) + mk^2d)$.   We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=FswxmvMOSG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Poojan_Chetan_Shah1",
        "name": "Poojan Chetan Shah",
        "name_site": null,
        "openreview_id": "~Poojan_Chetan_Shah1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "poojan-shah-b077ab25b",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FswxmvMOSG",
      "title": "A New Rejection Sampling Approach to $k\\text{-}\\mathtt{means}$++ with Improved Tradeoffs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The $k$-$\\mathtt{means}$++ seeding algorithm  (Arthur & Vassilvitskii, 2007a)  is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\\mathcal{X} \\subset \\mathbb{R} ^d$ into $k$ clusters.\n The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\\mathcal{X}|kd)$, making it expensive for large datasets. \n In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\\mathtt{means}$++. \n Our first method runs in time $\\tilde{O}(\\mathtt{nnz} (\\mathcal{X}) + \\beta k^2d)$\n while still being $O(\\log k )$ competitive in expectation. Here, $\\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\\mathtt{means}$ cost in expectation and $\\tilde{O}$ hides logarithmic factors in $k$ and $|\\mathcal{X}|$. \n Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\\Omega( m/\\beta)} \\operatorname{Var} (\\mathcal{X})$ in addition to the $O(\\log k)$ guarantee of $k$-$\\mathtt{means}$++ improving upon the result of (Bachem et al., 2016a) who get an additional factor of $m^{-1}\\operatorname{Var}(\\mathcal{X})$  while still running in time $\\tilde{O}(\\mathtt{nnz}(\\mathcal{X}) + mk^2d)$.   We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=FswxmvMOSG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shashwat_Agrawal1",
        "name": "Shashwat Agrawal",
        "name_site": null,
        "openreview_id": "~Shashwat_Agrawal1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "agrawalshashwat/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FswxmvMOSG",
      "title": "A New Rejection Sampling Approach to $k\\text{-}\\mathtt{means}$++ with Improved Tradeoffs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The $k$-$\\mathtt{means}$++ seeding algorithm  (Arthur & Vassilvitskii, 2007a)  is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\\mathcal{X} \\subset \\mathbb{R} ^d$ into $k$ clusters.\n The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\\mathcal{X}|kd)$, making it expensive for large datasets. \n In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\\mathtt{means}$++. \n Our first method runs in time $\\tilde{O}(\\mathtt{nnz} (\\mathcal{X}) + \\beta k^2d)$\n while still being $O(\\log k )$ competitive in expectation. Here, $\\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\\mathtt{means}$ cost in expectation and $\\tilde{O}$ hides logarithmic factors in $k$ and $|\\mathcal{X}|$. \n Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\\Omega( m/\\beta)} \\operatorname{Var} (\\mathcal{X})$ in addition to the $O(\\log k)$ guarantee of $k$-$\\mathtt{means}$++ improving upon the result of (Bachem et al., 2016a) who get an additional factor of $m^{-1}\\operatorname{Var}(\\mathcal{X})$  while still running in time $\\tilde{O}(\\mathtt{nnz}(\\mathcal{X}) + mk^2d)$.   We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=FswxmvMOSG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ragesh_Jaiswal1",
        "name": "Ragesh Jaiswal",
        "name_site": null,
        "openreview_id": "~Ragesh_Jaiswal1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "63/1704",
        "google_scholar_url": null,
        "orcid": "0009-0002-4475-0922",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dwBBhbueYk",
      "title": "GraphFLEx: Structure Learning $\\underline{\\text{F}}$ramework for $\\underline{\\text{L}}$arge $\\underline{\\text{Ex}}$panding $\\underline{\\text{Graph}}$s",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph structure learning is a fundamental problem critical for interpretability and uncovering relationships in data. While graphical data is central to information representation, inferring graph structures remains challenging. Existing methods falter with expanding graphs, requiring costly relearning of the entire structure for new nodes, and face severe computational and memory demands on large graphs. To overcome these challenges, we propose $\\textbf{GraphFLEx}$: a unified framework for structure learning in Large and Expanding Graphs. GraphFLEx efficiently limits potential connections to relevant nodes by leveraging clustering and coarsening techniques, significantly reducing computational costs and enhancing scalability. $\\textbf{GraphFLEx}$ provides 48 flexible methods for graph structure learning by integrating diverse learning, coarsening, and clustering approaches. Extensive experiments with various GNN models demonstrate its effectiveness. Our code is available at https://anonymous.4open.science/r/Scaling_Graph_Learning-5644.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=dwBBhbueYk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohit_Kataria1",
        "name": "Mohit Kataria",
        "name_site": null,
        "openreview_id": "~Mohit_Kataria1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "passenger/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dwBBhbueYk",
      "title": "GraphFLEx: Structure Learning $\\underline{\\text{F}}$ramework for $\\underline{\\text{L}}$arge $\\underline{\\text{Ex}}$panding $\\underline{\\text{Graph}}$s",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph structure learning is a fundamental problem critical for interpretability and uncovering relationships in data. While graphical data is central to information representation, inferring graph structures remains challenging. Existing methods falter with expanding graphs, requiring costly relearning of the entire structure for new nodes, and face severe computational and memory demands on large graphs. To overcome these challenges, we propose $\\textbf{GraphFLEx}$: a unified framework for structure learning in Large and Expanding Graphs. GraphFLEx efficiently limits potential connections to relevant nodes by leveraging clustering and coarsening techniques, significantly reducing computational costs and enhancing scalability. $\\textbf{GraphFLEx}$ provides 48 flexible methods for graph structure learning by integrating diverse learning, coarsening, and clustering approaches. Extensive experiments with various GNN models demonstrate its effectiveness. Our code is available at https://anonymous.4open.science/r/Scaling_Graph_Learning-5644.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=dwBBhbueYk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nikita_Malik1",
        "name": "Nikita Malik",
        "name_site": null,
        "openreview_id": "~Nikita_Malik1",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://sites.google.com/view/nikitamalik/home",
        "dblp_id": null,
        "google_scholar_url": "Nk26UdwAAAAJ",
        "orcid": "0000-0001-8383-7882",
        "linkedin_url": "https://www.linkedin.com/public-profile/settings?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_self_edit_contact-info%3BdMsbsSW9Tl%2BLZP%2BkMI4rfQ%3D%3D",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dwBBhbueYk",
      "title": "GraphFLEx: Structure Learning $\\underline{\\text{F}}$ramework for $\\underline{\\text{L}}$arge $\\underline{\\text{Ex}}$panding $\\underline{\\text{Graph}}$s",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph structure learning is a fundamental problem critical for interpretability and uncovering relationships in data. While graphical data is central to information representation, inferring graph structures remains challenging. Existing methods falter with expanding graphs, requiring costly relearning of the entire structure for new nodes, and face severe computational and memory demands on large graphs. To overcome these challenges, we propose $\\textbf{GraphFLEx}$: a unified framework for structure learning in Large and Expanding Graphs. GraphFLEx efficiently limits potential connections to relevant nodes by leveraging clustering and coarsening techniques, significantly reducing computational costs and enhancing scalability. $\\textbf{GraphFLEx}$ provides 48 flexible methods for graph structure learning by integrating diverse learning, coarsening, and clustering approaches. Extensive experiments with various GNN models demonstrate its effectiveness. Our code is available at https://anonymous.4open.science/r/Scaling_Graph_Learning-5644.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=dwBBhbueYk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dwBBhbueYk",
      "title": "GraphFLEx: Structure Learning $\\underline{\\text{F}}$ramework for $\\underline{\\text{L}}$arge $\\underline{\\text{Ex}}$panding $\\underline{\\text{Graph}}$s",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph structure learning is a fundamental problem critical for interpretability and uncovering relationships in data. While graphical data is central to information representation, inferring graph structures remains challenging. Existing methods falter with expanding graphs, requiring costly relearning of the entire structure for new nodes, and face severe computational and memory demands on large graphs. To overcome these challenges, we propose $\\textbf{GraphFLEx}$: a unified framework for structure learning in Large and Expanding Graphs. GraphFLEx efficiently limits potential connections to relevant nodes by leveraging clustering and coarsening techniques, significantly reducing computational costs and enhancing scalability. $\\textbf{GraphFLEx}$ provides 48 flexible methods for graph structure learning by integrating diverse learning, coarsening, and clustering approaches. Extensive experiments with various GNN models demonstrate its effectiveness. Our code is available at https://anonymous.4open.science/r/Scaling_Graph_Learning-5644.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=dwBBhbueYk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayadeva_Jayadeva1",
        "name": "Jayadeva Jayadeva",
        "name_site": null,
        "openreview_id": "~Jayadeva_Jayadeva1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "58/4288",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "oOaoNaoCJw",
      "title": "Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions (\"model-free\") or dynamics (\"model-based\") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=oOaoNaoCJw",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shivangi_Agarwal2",
        "name": "Shivangi Agarwal",
        "name_site": null,
        "openreview_id": "~Shivangi_Agarwal2",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0000-0002-0240-9454",
        "linkedin_url": "shivangi-agarwal-a05b6111b/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qROnDTwgCr",
      "title": "Identifying key amino acid types that distinguish paralogous proteins using Shapley value based feature subset selection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Paralogous proteins have a common ancestor but have diverged in functionality. Using known machine learning algorithms, we present a data-driven method to identify the key amino acid types that play a role in distinguishing a given pair of proteins that are paralogs.\nWe use an existing Shapley value based feature subset selection algorithm, SVEA, to identify the key amino acid types adequate to distinguish pairs of paralogous proteins. We refer to these as the amino acid feature subset ($AFS$). For a paralog pair, say proteins $P$ and $Q$, its $AFS$ is partitioned based on protein-wise importance as $AFS(P)$ and $AFS(Q)$ using a linear classifier, SVM. To validate the significance of the $AFS$ amino acids, we use multiple domain knowledge based methods : (a) multiple sequence alignment, and/or (b) 3D structure analysis, and/or (c) supporting evidence from biology literature. This method is computationally cheap, requires less data and can be used as an initial data-driven step for further hypothesis-driven experimental study of proteins. We demonstrate the results for 15 pairs of paralogous proteins.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=qROnDTwgCr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranav_Machingal1",
        "name": "Pranav Machingal",
        "name_site": null,
        "openreview_id": "~Pranav_Machingal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.ieor.iitb.ac.in/students/pranav",
        "dblp_id": "275/7773",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=p0ZCnUoAAAAJ",
        "orcid": "0009-0002-7850-9598",
        "linkedin_url": "pranav-machingal-9b6170110/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qROnDTwgCr",
      "title": "Identifying key amino acid types that distinguish paralogous proteins using Shapley value based feature subset selection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Paralogous proteins have a common ancestor but have diverged in functionality. Using known machine learning algorithms, we present a data-driven method to identify the key amino acid types that play a role in distinguishing a given pair of proteins that are paralogs.\nWe use an existing Shapley value based feature subset selection algorithm, SVEA, to identify the key amino acid types adequate to distinguish pairs of paralogous proteins. We refer to these as the amino acid feature subset ($AFS$). For a paralog pair, say proteins $P$ and $Q$, its $AFS$ is partitioned based on protein-wise importance as $AFS(P)$ and $AFS(Q)$ using a linear classifier, SVM. To validate the significance of the $AFS$ amino acids, we use multiple domain knowledge based methods : (a) multiple sequence alignment, and/or (b) 3D structure analysis, and/or (c) supporting evidence from biology literature. This method is computationally cheap, requires less data and can be used as an initial data-driven step for further hypothesis-driven experimental study of proteins. We demonstrate the results for 15 pairs of paralogous proteins.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=qROnDTwgCr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rakesh_Busi1",
        "name": "Rakesh Busi",
        "name_site": null,
        "openreview_id": "~Rakesh_Busi1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0001-0899-6653",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qROnDTwgCr",
      "title": "Identifying key amino acid types that distinguish paralogous proteins using Shapley value based feature subset selection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Paralogous proteins have a common ancestor but have diverged in functionality. Using known machine learning algorithms, we present a data-driven method to identify the key amino acid types that play a role in distinguishing a given pair of proteins that are paralogs.\nWe use an existing Shapley value based feature subset selection algorithm, SVEA, to identify the key amino acid types adequate to distinguish pairs of paralogous proteins. We refer to these as the amino acid feature subset ($AFS$). For a paralog pair, say proteins $P$ and $Q$, its $AFS$ is partitioned based on protein-wise importance as $AFS(P)$ and $AFS(Q)$ using a linear classifier, SVM. To validate the significance of the $AFS$ amino acids, we use multiple domain knowledge based methods : (a) multiple sequence alignment, and/or (b) 3D structure analysis, and/or (c) supporting evidence from biology literature. This method is computationally cheap, requires less data and can be used as an initial data-driven step for further hypothesis-driven experimental study of proteins. We demonstrate the results for 15 pairs of paralogous proteins.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=qROnDTwgCr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Petety_V._Balaji1",
        "name": "Petety V. Balaji",
        "name_site": null,
        "openreview_id": "~Petety_V._Balaji1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.bio.iitb.ac.in/~balaji/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": null,
        "confidence_std": null,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    }
  ]
}