{
  "conference": "ICLR 2022",
  "focus_country": "India",
  "total_papers": 104,
  "generated_at": "2025-07-06T10:37:20.843002",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "XzTtHjgPDsT",
      "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": " Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6382",
      "pdf_url": "https://openreview.net/pdf?id=XzTtHjgPDsT",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 30.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 108,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5JdLZg346Lw",
      "title": "Generative Modeling with Optimal Transport Maps",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a min-max optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6183",
      "pdf_url": "https://openreview.net/pdf?id=5JdLZg346Lw",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Litu_Rout1",
        "name": "Litu Rout",
        "name_site": "Litu Rout, Alexander Korotin, Evgeny Burnaev",
        "openreview_id": "~Litu_Rout1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://liturout.github.io/",
        "dblp_id": "206/6445",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": null,
        "linkedin_url": "litu-rout-sac-isro/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Space Research Organisation (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 84,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ibrUkC-pbis",
      "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as ‘value-set’. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16 x 16 sudoku after being trained on only 9 x 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs.  In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6908",
      "pdf_url": "https://openreview.net/pdf?id=ibrUkC-pbis",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yatin_Nandwani1",
        "name": "Yatin Nandwani",
        "name_site": "Yatin Nandwani, Deepanshu Jindal, Mausam ., Parag Singla",
        "openreview_id": "~Yatin_Nandwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~yatin",
        "dblp_id": "255/7046",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "yatin-nandwani-0804ba9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "irARV_2VFs4",
      "title": "Focus on the Common Good: Group Distributional Robustness Follows",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization (Group-DRO) objective is a better alternative. The starting point of this paper is the observation that though Group-DRO performs better than ERM on minority groups for some benchmark datasets, there are several other datasets where it performs much worse than ERM. Inspired by ideas from the closely related problem of domain generalization, this paper proposes a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind our proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group-DRO. Empirically, we show that our proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups.  Theoretically, we show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6216",
      "pdf_url": "https://openreview.net/pdf?id=irARV_2VFs4",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vihari_Piratla1",
        "name": "Vihari Piratla",
        "name_site": "Vihari Piratla, Praneeth Netrapalli, Sunita Sarawagi",
        "openreview_id": "~Vihari_Piratla1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vihari.github.io/",
        "dblp_id": "161/3626",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=DQddccYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tgcAoUVHRIB",
      "title": "Neural Methods for Logical Reasoning over Knowledge Graphs",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which includes negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to  handle FOL queries with Conjunction, Disjunction and Negation operators. We demonstrate experimentally the performance of our models through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10% relative increase over best performing state of the art and more than 30% over the original method based on single-point vector embeddings.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/5976",
      "pdf_url": "https://openreview.net/pdf?id=tgcAoUVHRIB",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Alfonso_Amayuelas2",
        "name": "Alfonso Amayuelas",
        "name_site": "Alfonso Amayuelas, Shuai Zhang, Xi Rao, Ce Zhang",
        "openreview_id": "~Alfonso_Amayuelas2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.amayuelas.me/",
        "dblp_id": "281/7669",
        "google_scholar_url": "https://scholar.google.dk/citations?user=QGQ2G28AAAAJ",
        "orcid": null,
        "linkedin_url": "alfonsoamayuelas/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of California, Santa Barbara (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 48,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zzk231Ms1Ih",
      "title": "A Theory of Tournament Representations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Real-world tournaments are almost always intransitive. Recent works have noted that parametric models which assume  $d$ dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed $d$ dimensional representations. In this work, we develop a novel theory for understanding parametric tournament representations. Our first contribution is to structurally characterize the class of tournaments that arise out of $d$ dimensional representations. We do this by showing that these tournament classes have forbidden configurations that must necessarily be a union of flip classes, a novel way to partition the set of all tournaments. We further characterize rank $2$ tournaments completely by showing that the associated forbidden flip class contains just $2$ tournaments. Specifically, we show that the rank $2$ tournaments are equivalent to locally transitive tournaments. This insight allows us to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. We also exhibit specific forbidden configurations for rank $4$ tournaments. For a general rank $d$ tournament class, we show that the flip class associated with a coned-doubly regular tournament of size $\\mathcal{O}(\\sqrt{d})$ must be a forbidden configuration. To answer a dual question, using a celebrated result of Froster, we show a lower bound of $\\Theta(\\sqrt{n})$ on the minimum dimension needed to represent all tournaments on $n$ nodes. For any given tournament, we show a novel upper bound on the smallest representation dimension that depends on the least size of the number of unique nodes in any feedback arc set of the flip class associated with a tournament. We show how our results also shed light on the upper bound of sign-rank of matrices. ",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6952",
      "pdf_url": "https://openreview.net/pdf?id=zzk231Ms1Ih",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arun_Rajkumar4",
        "name": "Arun Rajkumar",
        "name_site": "Arun Rajkumar, Vishnu Veerathu, Abdul Mir",
        "openreview_id": "~Arun_Rajkumar4",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "32/11350",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "F5Em8ASCosV",
      "title": "Causal Contextual Bandits with Targeted Interventions",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We study a contextual bandit setting where the learning agent has the ability to perform interventions on targeted subsets of the population, apart from possessing qualitative causal side-information. This novel formalism captures intricacies in real-world scenarios such as software product experimentation where targeted experiments can be conducted. However, this fundamentally changes the set of options that the agent has, compared to standard contextual bandit settings, necessitating new techniques. This is also the first work that integrates causal side-information in a contextual bandit setting, where the agent aims to learn a policy that maps contexts to arms (as opposed to just identifying one best arm). We propose a new algorithm, which we show empirically performs better than baselines on experiments that use purely synthetic data and on real world-inspired experiments. We also prove a bound on regret that theoretically guards performance.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6917",
      "pdf_url": "https://openreview.net/pdf?id=F5Em8ASCosV",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaraman_Ravindran1",
        "name": "Balaraman Ravindran",
        "name_site": null,
        "openreview_id": "~Balaraman_Ravindran1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~ravi",
        "dblp_id": "69/2281",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-5364-7639",
        "linkedin_url": "ravindran-balaraman-427a307",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ibrUkC-pbis",
      "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as ‘value-set’. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16 x 16 sudoku after being trained on only 9 x 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs.  In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6908",
      "pdf_url": "https://openreview.net/pdf?id=ibrUkC-pbis",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1_1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Mausam_Mausam2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~mausam",
        "dblp_id": "30/6391.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-4088-4296",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "irARV_2VFs4",
      "title": "Focus on the Common Good: Group Distributional Robustness Follows",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization (Group-DRO) objective is a better alternative. The starting point of this paper is the observation that though Group-DRO performs better than ERM on minority groups for some benchmark datasets, there are several other datasets where it performs much worse than ERM. Inspired by ideas from the closely related problem of domain generalization, this paper proposes a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind our proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group-DRO. Empirically, we show that our proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups.  Theoretically, we show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6216",
      "pdf_url": "https://openreview.net/pdf?id=irARV_2VFs4",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sunita_Sarawagi1",
        "name": "Sunita Sarawagi",
        "name_site": null,
        "openreview_id": "~Sunita_Sarawagi1",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://www.cse.iitb.ac.in/~sunita/",
        "dblp_id": "s/SunitaSarawagi",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=Hg4HmTAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nrGGfMbY_qK",
      "title": "Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups.\nWhile a handful of work do propose new continual learning setups, they still lack practicality in certain aspects.\nFor better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of blurry task boundaries and subject to inference queries at any moment.\nWe additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment.\nTo address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques.\nOur empirical validation demonstrates that the proposed method outperforms prior arts by large margins. Code and data splits are available at https://github.com/naver-ai/i-Blurry.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6797",
      "pdf_url": "https://openreview.net/pdf?id=nrGGfMbY_qK",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 83,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "XzTtHjgPDsT",
      "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": " Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6382",
      "pdf_url": "https://openreview.net/pdf?id=XzTtHjgPDsT",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Rajiv_Didolkar1",
        "name": "Aniket Rajiv Didolkar",
        "name_site": null,
        "openreview_id": "~Aniket_Rajiv_Didolkar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://github.com/dido1998/",
        "dblp_id": "245/8589",
        "google_scholar_url": "https://scholar.google.ca/citations?user=ekvl5o0AAAAJ",
        "orcid": null,
        "linkedin_url": "aniket-didolkar-7a9b8912a",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Manipal Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 8.88888888888889,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 108,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "XzTtHjgPDsT",
      "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": " Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6382",
      "pdf_url": "https://openreview.net/pdf?id=XzTtHjgPDsT",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Alex_Lamb1_1",
        "name": "Alex Lamb",
        "name_site": null,
        "openreview_id": "~Kartikeya_Badola1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://kartikeya-badola.github.io/",
        "dblp_id": null,
        "google_scholar_url": "1bXieIsAAAAJ",
        "orcid": "0000-0002-2020-9173",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 7.777777777777778,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 108,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CmsfC7u054S",
      "title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian model-based approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for infinite Markov chain modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows inferring the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6691",
      "pdf_url": "https://openreview.net/pdf?id=CmsfC7u054S",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Huawei (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 4.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5i7lJLuhTm",
      "title": "Learning by Directional Gradient Descent",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient descent. However, no prior method for computing the gradient is fully satisfactory, for example consuming too much memory, introducing too much variance, or adding too much bias. In this work, we propose a new learning algorithm that addresses these limitations. The basic idea is to update the parameters of the representation by using the directional derivative along a candidate direction, a quantity that may be computed online with the same computational cost as the representation itself. We consider several different choices of candidate direction, including random selection and approximations to the true gradient, and investigate their performance on several synthetic tasks.  \n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/7001",
      "pdf_url": "https://openreview.net/pdf?id=5i7lJLuhTm",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 43,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kq11Tl1z4",
      "title": "IGLU: Efficient GCN Training via Lazy Updates",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that caches intermediate computations at various GCN layers thus enabling lazy updates that significantly reduce the compute cost of descent. IGLU introduces bounded bias into the gradients but nevertheless converges to a first-order saddle point under standard assumptions such as objective smoothness. Benchmark experiments show that IGLU offers up to 1.2% better accuracy despite requiring up to 88% less compute.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6235",
      "pdf_url": "https://openreview.net/pdf?id=5kq11Tl1z4",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Sinha1",
        "name": "Aditya Sinha",
        "name_site": null,
        "openreview_id": "~Aditya_Sinha1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://adityaasinha28.github.io/",
        "dblp_id": null,
        "google_scholar_url": "5letoXIAAAAJ",
        "orcid": null,
        "linkedin_url": "adityaasinha28/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ibrUkC-pbis",
      "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as ‘value-set’. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16 x 16 sudoku after being trained on only 9 x 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs.  In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6908",
      "pdf_url": "https://openreview.net/pdf?id=ibrUkC-pbis",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vidit_Jain2",
        "name": "Vidit Jain",
        "name_site": null,
        "openreview_id": "~Vidit_Jain2",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "68/5650",
        "google_scholar_url": null,
        "orcid": "0000-0002-7911-1074",
        "linkedin_url": "jvidit/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9Vrb9D0WI4",
      "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16× its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6× its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/7101",
      "pdf_url": "https://openreview.net/pdf?id=9Vrb9D0WI4",
      "github_url": "",
      "total_authors": 40,
      "track_name": "Main Conference",
      "author": {
        "id": "~Matteo_Manica1",
        "name": "Matteo Manica",
        "name_site": null,
        "openreview_id": "~Matteo_Manica1",
        "position": 23,
        "gender": "M",
        "homepage_url": "https://ibm.biz/matteomanica",
        "dblp_id": "194/3100",
        "google_scholar_url": "-20KQZQAAAAJ",
        "orcid": "0000-0002-8872-0269",
        "linkedin_url": "matteo-manica-drugilsberg/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sapienza University of Rome (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.269230769230769,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1949,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "KJztlfGPdwW",
      "title": "Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Solving goal-conditioned tasks with sparse rewards using self-supervised learning is promising because of its simplicity and stability over current reinforcement learning (RL) algorithms. A recent work, called Goal-Conditioned Supervised Learning (GCSL), provides a new learning framework by iteratively relabeling and imitating self-generated experiences. In this paper, we revisit the theoretical property of GCSL --- optimizing a lower bound of the goal reaching objective, and extend GCSL as a novel offline goal-conditioned RL algorithm. The proposed method is named Weighted GCSL (WGCSL), in which we introduce an advanced compound weight consisting of three parts (1) discounted weight for goal relabeling, (2) goal-conditioned exponential advantage weight, and (3) best-advantage weight. Theoretically, WGCSL is proved to optimize an equivalent lower bound of the goal-conditioned RL objective and generates monotonically improved policies via an iterated scheme. The monotonic property holds for any behavior policies, and therefore WGCSL can be applied to both online and offline settings. To evaluate algorithms in the offline goal-conditioned RL setting, we provide a benchmark including a range of point and simulated robot domains. Experiments in the introduced benchmark demonstrate that WGCSL can consistently outperform GCSL and existing state-of-the-art offline methods in the fully offline goal-conditioned setting.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6583",
      "pdf_url": "https://openreview.net/pdf?id=KJztlfGPdwW",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 6,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tsinghua University (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 1.875,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 86,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ibrUkC-pbis",
      "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as ‘value-set’. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16 x 16 sudoku after being trained on only 9 x 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs.  In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2022/poster/6908",
      "pdf_url": "https://openreview.net/pdf?id=ibrUkC-pbis",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "-GU1sfGnM5K",
      "title": "A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "\tWe convert the DeepMind Mathematics Dataset into a reinforcement learning environment by interpreting it as a program synthesis problem. Each action taken in the environment adds an operator or an input into a discrete compute graph. Graphs which compute correct answers yield positive reward, enabling optimization of a policy to construct compute graphs conditioned on problem statements. Baseline models are trained using Double DQN on various subsets of problem types, demonstrating the capability to learn to correctly construct graphs despite the challenges of combinatorial explosion and noisy rewards.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=-GU1sfGnM5K",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Johnny_Ye2",
        "name": "Johnny Ye",
        "name_site": null,
        "openreview_id": "~Alok_Singh1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://alokssingh.github.io/",
        "dblp_id": null,
        "google_scholar_url": "K6ecfUwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Technology Silchar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0q0REJNgtg",
      "title": "Retrieval-Augmented Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Most deep reinforcement learning (RL) algorithms distill experience into parametric behavior policies or value functions via gradient updates. While effective, this approach has several disadvantages: (1) it is computationally expensive, (2) it can take many updates to integrate experiences into the parametric model, (3) experiences that are not fully integrated do not appropriately influence the agent's behavior, and (4) behavior is limited by the capacity of the model. In this paper we explore an alternative paradigm in which we train a network to map a dataset of past experiences to optimal behavior. Specifically, we augment an RL agent with a retrieval process (parameterized as a neural network) that has direct access to a dataset of experiences. This dataset can come from the agent's past experiences, expert demonstrations, or any other relevant source. The retrieval process is trained to retrieve information from the dataset that may be useful in the current context, to help the agent achieve its goal faster and more efficiently. We integrate our method into two different RL agents: an offline DQN agent and an online R2D2 agent. In offline multi-task problems, we show that the retrieval-augmented DQN agent avoids task interference and learns faster than the baseline DQN agent. On Atari, we show that retrieval-augmented R2D2 learns significantly faster than the baseline R2D2 agent and achieves higher scores. We run extensive ablations to measure the contributions of the components of our proposed method.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=0q0REJNgtg",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 12,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 59,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "4JlwgTbmzXQ",
      "title": "EqR: Equivariant Representations for Data-Efficient Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent’s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=4JlwgTbmzXQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal1",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://arnab39.github.io",
        "dblp_id": null,
        "google_scholar_url": "NhWR4yIAAAAJ",
        "orcid": null,
        "linkedin_url": "arnab-mondal-01b522a9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ServiceNow (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6w2zSI9RAnf",
      "title": "Reasoning With Hierarchical Symbols: Reclaiming Symbolic Policies For Visual Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Deep vision models are nowadays widely integrated into visual reinforcement learning (RL) to parameterize the policy networks. However, the learned policies are overparameterized black boxes that lack interpretability, and are usually brittle under input distribution shifts. This work revisits this end-to-end learning pipeline, and proposes an alternative stage-wise approach that features hierarchical reasoning. Specifically, our approach progressively converts a policy network into the interpretable symbolic policy, composed from geometric and numerical symbols and operators. A policy regression algorithm called RoundTourMix is proposed to distill the symbolic rules as teacher-student. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and ``de-noise\" the CNN policy: on six specific environments, our distilled symbolic policy achieved compelling or even higher scores than the CNN based RL agents. Our codes will be fully released upon acceptance.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6w2zSI9RAnf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~S_P_Sharan1",
        "name": "S P Sharan",
        "name_site": null,
        "openreview_id": "~S_P_Sharan1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://spsharan.com/",
        "dblp_id": "324/6204",
        "google_scholar_url": "1NtGcNIAAAAJ",
        "orcid": "0000-0002-6298-6464",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Technology Tiruchirappalli (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ChKNCDB0oYj",
      "title": "Mistake-driven Image Classification with FastGAN and SpinalNet",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Image classification with classes of varying difficulty can cause performance disparity in deep learning models and reduce the overall performance and reliability of the predictions. In this paper, we address the problem of imbalanced performance in image classification, where the trained model has performance deficits in some of the dataset's classes. By employing Generative Adversarial Networks (GANs) to augment these deficit classes, we finetune the model towards a balanced performance among the different classes and an overall better performance on the whole dataset. Specifically, we combine a light-weight GAN method, FastGAN (Liu et al., 2021), for class-wise data augmentation with Progressive SpinalNet (Chopra, 2021) and Sharpness-Aware Minimization (SAM) (Foret et al., 2020) for training. Unlike earlier works, during training, our method focuses on those classes with lowest accuracy after the initial training phase, which leads to better performance. Only these classes are augmented to boost the accuracy. Due to the use of a light-weight GAN method, the GAN-based augmentation is viable and effective for mistake-driven training even for datasets with only few images per class, while simultaneously requiring less computation than other, more complex GAN methods. Our extensive experiments, including ablation studies on all key components, show competitive or better accuracy than the previous state-of-the-art on five datasets with different sizes and image resolutions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ChKNCDB0oYj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sahil_Sahil1",
        "name": "Sahil Sahil",
        "name_site": null,
        "openreview_id": "~Sahil_Sahil1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "sahiliiserb/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FZyZiRYbdK8",
      "title": "Distributionally Robust Learning for Uncertainty Calibration under Domain Shift",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We propose a framework for learning calibrated uncertainties under domain shifts. We consider the case where the source (training) distribution differs significantly from the target (test) distribution. We detect such domain shifts through the use of binary domain classifier and integrate it with the task network and train them jointly end-to-end. The binary domain classifier yields a density ratio that reflects the closeness of a target (test) sample to  the source (training) distribution. We employ it to adjust the uncertainty of prediction in the task network. This idea of using the density ratio is based on the distributionally robust learning (DRL) framework, which accounts for the domain shift through adversarial risk minimization. We demonstrate that our method generates calibrated uncertainties that benefit many downstream tasks, such as unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). In these tasks, methods like self-training and FixMatch use uncertainties to select confident pseudo-labels for re-training. Our experiments show that the introduction of DRL leads to significant improvements in cross-domain performance. We also demonstrate that the estimated density ratios show agreement with the human selection frequencies, suggesting a match with a proxy of human perceived uncertainties. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=FZyZiRYbdK8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Illinois at Chicago (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Fj1Tpym9KxH",
      "title": "A Closer Look at Smoothness in Domain Adversarial Training",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Domain adversarial training has been ubiquitous for achieving invariant representations and is used widely for various domain adaptation tasks. In recent times methods converging to smooth optima have shown improved generalization for supervised learning tasks like classification.  In this work, we analyze the effect of smoothness enhancing formulations on domain adversarial training, the objective of which is a combination of classification and adversarial terms. In contrast to classification loss, our analysis shows that \\textit{converging to smooth minima w.r.t. adversarial loss leads to sub-optimal generalization on the target domain}. Based on the analysis, we introduce the Smooth Domain Adversarial training (SDAT) procedure, which effectively enhances the performance of existing domain adversarial methods for both classification and object detection tasks. Our smoothness analysis also provides insight into the extensive usage of SGD over Adam in domain adversarial training.   ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Fj1Tpym9KxH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Rangwani1",
        "name": "Harsh Rangwani",
        "name_site": null,
        "openreview_id": "~Harsh_Rangwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rangwani-harsh.github.io/about/",
        "dblp_id": "220/0991",
        "google_scholar_url": "OQK0WREAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 156,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Fj1Tpym9KxH",
      "title": "A Closer Look at Smoothness in Domain Adversarial Training",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Domain adversarial training has been ubiquitous for achieving invariant representations and is used widely for various domain adaptation tasks. In recent times methods converging to smooth optima have shown improved generalization for supervised learning tasks like classification.  In this work, we analyze the effect of smoothness enhancing formulations on domain adversarial training, the objective of which is a combination of classification and adversarial terms. In contrast to classification loss, our analysis shows that \\textit{converging to smooth minima w.r.t. adversarial loss leads to sub-optimal generalization on the target domain}. Based on the analysis, we introduce the Smooth Domain Adversarial training (SDAT) procedure, which effectively enhances the performance of existing domain adversarial methods for both classification and object detection tasks. Our smoothness analysis also provides insight into the extensive usage of SGD over Adam in domain adversarial training.   ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Fj1Tpym9KxH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumukh_K_Aithal1",
        "name": "Sumukh K Aithal",
        "name_site": null,
        "openreview_id": "~Sumukh_K_Aithal1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 156,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Fj1Tpym9KxH",
      "title": "A Closer Look at Smoothness in Domain Adversarial Training",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Domain adversarial training has been ubiquitous for achieving invariant representations and is used widely for various domain adaptation tasks. In recent times methods converging to smooth optima have shown improved generalization for supervised learning tasks like classification.  In this work, we analyze the effect of smoothness enhancing formulations on domain adversarial training, the objective of which is a combination of classification and adversarial terms. In contrast to classification loss, our analysis shows that \\textit{converging to smooth minima w.r.t. adversarial loss leads to sub-optimal generalization on the target domain}. Based on the analysis, we introduce the Smooth Domain Adversarial training (SDAT) procedure, which effectively enhances the performance of existing domain adversarial methods for both classification and object detection tasks. Our smoothness analysis also provides insight into the extensive usage of SGD over Adam in domain adversarial training.   ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Fj1Tpym9KxH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 156,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "HmFBdvBkUUY",
      "title": "SpecTRA: Spectral Transformer for Graph Representation Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Transformers have recently been applied in the more generic domain of graphs. For the same, researchers proposed various positional and structural encoding schemes to overcome the limitation of transformers in modeling the positional invariance in graphs and graph topology. Some of these encoding techniques use the spectrum of the graph. In addition to graph topology, graph signals could be multi-channeled and contain heterogeneous information. We argue that transformers cannot model multichannel signals inherently spread over the graph spectrum. To this end, we propose SpecTRA, a novel approach that induces a spectral module into the transformer architecture to enable decomposition of graph spectrum and selectively learn useful information akin to filtering in the frequency domain. Results on standard benchmark datasets show that the proposed method performs comparably or better than existing transformer and GNN based architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=HmFBdvBkUUY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anson_Bastos1",
        "name": "Anson Bastos",
        "name_site": null,
        "openreview_id": "~Anson_Bastos1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "220/4367",
        "google_scholar_url": "is7rRuAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "KVhvw16pvi",
      "title": "TAG: Task-based Accumulated Gradients for Lifelong learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "When an agent encounters a continual stream of new tasks in the lifelong learning setting, it leverages the knowledge it gained from the earlier tasks to help learn the new tasks better. In such a scenario, identifying an efficient knowledge representation becomes a challenging problem. Most research works propose to either store a subset of examples from the past tasks in a replay buffer, dedicate a separate set of parameters to each task or penalize excessive updates over parameters by introducing a regularization term. While existing methods employ the general task-agnostic stochastic gradient descent update rule, we propose a task-aware optimizer that adapts the learning rate based on the relatedness among tasks. We utilize the directions taken by the parameters during the updates by additively accumulating the gradients specific to each task. These task-based accumulated gradients act as a knowledge base that is maintained and updated throughout the stream. We empirically show that our proposed adaptive learning rate not only accounts for catastrophic forgetting but also exhibits knowledge transfer. We also show that our method performs better than several state-of-the-art methods in lifelong learning on complex datasets. Moreover, our method can also be combined with the existing methods and achieve substantial improvement in performance.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=KVhvw16pvi",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaraman_Ravindran1",
        "name": "Balaraman Ravindran",
        "name_site": null,
        "openreview_id": "~Balaraman_Ravindran1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~ravi",
        "dblp_id": "69/2281",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-5364-7639",
        "linkedin_url": "ravindran-balaraman-427a307",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "KdcLdLuIjQT",
      "title": "Goal Randomization for Playing Text-based Games without a Reward Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Playing text-based games requires language understanding and sequential decision making. The objective of a reinforcement learning agent is to behave so as to maximise the sum of a suitable scalar reward function. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose a goal randomization method that uses random basic goals to train a policy in the absence of the reward of environments. Specifically, through simple but effective goal generation, our method learns to continuously propose challenging -- yet temporal and achievable -- goals that allow the agent to learn general skills for acting in a new environment, independent of the task to be solved. In a variety of text-based games, we show that this simple method results in competitive performance for agents. We also show that our method can learn policies that generalize across different text-based games. In further, we demonstrate an interesting result that our method works better than one of state-of-the-art agents GATA, which uses environment rewards for some text-based games.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=KdcLdLuIjQT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Technology Sydney (Australia)",
        "countries": [
          "Australia"
        ],
        "country_codes": [
          "AU"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LYpBYvxIY_R",
      "title": "Cost-Sensitive Hierarchical Classification through Layer-wise Abstentions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost-sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. Directly optimizing the cost-sensitive hierarchical loss is hard, due to its non-convexity, especially when the size of the taxonomy is large. In this paper, we propose a \\textbf{L}ayer-wise \\textbf{A}bstaining Loss \\textbf{M}inimization method (LAM),  a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems.  We prove that there is a bijective mapping between the original hierarchical cost-sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions. We employ the distributionally robust learning framework to solve the learning-to-abstain problems in each layer. We conduct experiments on large-scale bird dataset as well as on cell classification problems. Our results demonstrate that LAM achieves a lower hierarchical cost-sensitive loss in high accuracy regions, compared to previous methods and their modified versions for a fair comparison, even though they are not directly optimizing this loss. For each layer, we also achieve higher accuracy when the overall accuracy is kept fixed across different methods. Furthermore, we also show the flexibility of LAM by proposing a per-class loss-adjustment heuristic to achieve a performance profile. This can be used for cost design to translate user requirements into optimizable cost functions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LYpBYvxIY_R",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 9,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LtI14EpWKH",
      "title": "Tessellated 2D Convolution Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Data-driven (deep) learning approaches for image classification are prone to adversarial attacks. This means that an adversarial crafted image which is sufficiently close (visually indistinguishable) to its representative class can often be misclassified to be a member of a different class. A reason why deep neural approaches exhibits such vulnerability towards adversarial threats is mainly because the abstract representations learned in a data-driven manner often do not correlate well with human perceived features. To mitigate this problem, we propose the tessellated 2d convolution network, a novel divide-and-conquer based approach, which first independently learns the abstract representations of non-overlapping regions within an image, and then learns how to combine these representations to infer its class. It turns out that a non-uniform tiling of an image which ensures that the difference between the maximum and the minimum region sizes is not too large is the most robust way to construct such a tessellated 2d convolution network. This criterion can be achieved, among other schemes, by using a Mondrian tessellation of the input image. Our experiments demonstrate that our tessellated networks provides a more robust defence mechanism against gradient-based adversarial attacks in comparison to conventional deep neural models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LtI14EpWKH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "swarnava.das27@kgpian.iitkgp.ac.in",
        "name": "Swarnava Das",
        "name_site": null,
        "openreview_id": "swarnava.das27@kgpian.iitkgp.ac.in",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LtI14EpWKH",
      "title": "Tessellated 2D Convolution Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Data-driven (deep) learning approaches for image classification are prone to adversarial attacks. This means that an adversarial crafted image which is sufficiently close (visually indistinguishable) to its representative class can often be misclassified to be a member of a different class. A reason why deep neural approaches exhibits such vulnerability towards adversarial threats is mainly because the abstract representations learned in a data-driven manner often do not correlate well with human perceived features. To mitigate this problem, we propose the tessellated 2d convolution network, a novel divide-and-conquer based approach, which first independently learns the abstract representations of non-overlapping regions within an image, and then learns how to combine these representations to infer its class. It turns out that a non-uniform tiling of an image which ensures that the difference between the maximum and the minimum region sizes is not too large is the most robust way to construct such a tessellated 2d convolution network. This criterion can be achieved, among other schemes, by using a Mondrian tessellation of the input image. Our experiments demonstrate that our tessellated networks provides a more robust defence mechanism against gradient-based adversarial attacks in comparison to conventional deep neural models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LtI14EpWKH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pabitra_Mitra1",
        "name": "Pabitra Mitra",
        "name_site": null,
        "openreview_id": "~Pabitra_Mitra1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~pabitra/",
        "dblp_id": "m/PabitraMitra",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=5bXSZPYAAAAJ",
        "orcid": "0000-0002-1908-9813",
        "linkedin_url": "pabitra-mitra-8028235/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Glasgow (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LtI14EpWKH",
      "title": "Tessellated 2D Convolution Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Data-driven (deep) learning approaches for image classification are prone to adversarial attacks. This means that an adversarial crafted image which is sufficiently close (visually indistinguishable) to its representative class can often be misclassified to be a member of a different class. A reason why deep neural approaches exhibits such vulnerability towards adversarial threats is mainly because the abstract representations learned in a data-driven manner often do not correlate well with human perceived features. To mitigate this problem, we propose the tessellated 2d convolution network, a novel divide-and-conquer based approach, which first independently learns the abstract representations of non-overlapping regions within an image, and then learns how to combine these representations to infer its class. It turns out that a non-uniform tiling of an image which ensures that the difference between the maximum and the minimum region sizes is not too large is the most robust way to construct such a tessellated 2d convolution network. This criterion can be achieved, among other schemes, by using a Mondrian tessellation of the input image. Our experiments demonstrate that our tessellated networks provides a more robust defence mechanism against gradient-based adversarial attacks in comparison to conventional deep neural models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LtI14EpWKH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debasis_Ganguly2",
        "name": "Debasis Ganguly",
        "name_site": null,
        "openreview_id": "~Debasis_Ganguly2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://gdebasis.github.io/",
        "dblp_id": "41/7272",
        "google_scholar_url": "FhQENQgAAAAJ",
        "orcid": "0000-0003-0050-7138",
        "linkedin_url": "deb4it/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Lv-G9XqLRRy",
      "title": "Restricted Category Removal from Model Representations using Limited Data",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Deep learning models are trained on multiple categories jointly to solve several real-world problems. However, there can be cases where some of the classes may become restricted in the future and need to be excluded after the model has already been trained on them (Class-level Privacy). It can be due to privacy, ethical or legal concerns. A naive solution is to simply train the model from scratch on the complete training data while leaving out the training samples from the restricted classes (FDR - full data retraining). But this can be a very time-consuming process. Further, this approach will not work well if we no longer have access to the complete training data and instead only have access to very few training data. The objective of this work is to remove the information about the restricted classes from the network representations of all layers using limited data without affecting the prediction power of the model for the remaining classes. Simply fine-tuning the model on the limited available training data for the remaining classes will not be able to sufficiently remove the restricted class information, and aggressive fine-tuning on the limited data may also lead to overfitting. We propose a novel solution to achieve this objective that is significantly faster ($\\sim200\\times$ on ImageNet) than the naive solution. Specifically, we propose a novel technique for identifying the model parameters that are mainly relevant to the restricted classes. We also propose a novel technique that uses the limited training data of the restricted classes to remove the restricted class information from these parameters and uses the limited training data of the remaining classes to reuse these parameters for the remaining classes. The model obtained through our approach behaves as if it was never trained on the restricted classes and performs similar to FDR (which needs the complete training data). We also propose several baseline approaches and compare our approach with them in order to demonstrate its efficacy.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Lv-G9XqLRRy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pratik_Mazumder2",
        "name": "Pratik Mazumder",
        "name_site": null,
        "openreview_id": "~Pravendra_Singh1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/pravendra/",
        "dblp_id": "160/8743",
        "google_scholar_url": "YwDTxJMAAAAJ",
        "orcid": "0000-0003-1001-2219",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Lv-G9XqLRRy",
      "title": "Restricted Category Removal from Model Representations using Limited Data",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Deep learning models are trained on multiple categories jointly to solve several real-world problems. However, there can be cases where some of the classes may become restricted in the future and need to be excluded after the model has already been trained on them (Class-level Privacy). It can be due to privacy, ethical or legal concerns. A naive solution is to simply train the model from scratch on the complete training data while leaving out the training samples from the restricted classes (FDR - full data retraining). But this can be a very time-consuming process. Further, this approach will not work well if we no longer have access to the complete training data and instead only have access to very few training data. The objective of this work is to remove the information about the restricted classes from the network representations of all layers using limited data without affecting the prediction power of the model for the remaining classes. Simply fine-tuning the model on the limited available training data for the remaining classes will not be able to sufficiently remove the restricted class information, and aggressive fine-tuning on the limited data may also lead to overfitting. We propose a novel solution to achieve this objective that is significantly faster ($\\sim200\\times$ on ImageNet) than the naive solution. Specifically, we propose a novel technique for identifying the model parameters that are mainly relevant to the restricted classes. We also propose a novel technique that uses the limited training data of the restricted classes to remove the restricted class information from these parameters and uses the limited training data of the remaining classes to reuse these parameters for the remaining classes. The model obtained through our approach behaves as if it was never trained on the restricted classes and performs similar to FDR (which needs the complete training data). We also propose several baseline approaches and compare our approach with them in order to demonstrate its efficacy.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Lv-G9XqLRRy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohammed_Asad_Karim1",
        "name": "Mohammed Asad Karim",
        "name_site": null,
        "openreview_id": "~PRATIK_MAZUMDER1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/pratikmazumder",
        "dblp_id": "237/9769",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0003-1103-1884",
        "linkedin_url": "pratik-mazumder-553a8567/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OD_dnx57ksK",
      "title": "Momentum Conserving Lagrangian Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Realistic models of physical world rely on differentiable symmetries that, in turn, correspond to conservation laws. Recent works on Lagrangian and Hamiltonian neural networks show that the underlying symmetries of a system can be easily learned by a neural network when provided with an appropriate inductive bias. However, these models still suffer from issues such as inability to generalize to arbitrary system sizes, poor interpretability, and most importantly, inability to learn translational and rotational symmetries, which lead to the conservation laws of linear and angular momentum, respectively. Here, we present a momentum conserving Lagrangian neural network (MCLNN) that learns the Lagrangian of a system, while also preserving the translational and rotational symmetries. We test our approach on linear and non-linear spring systems, and a gravitational system, demonstrating the energy and momentum conservation. We also show that the model developed can generalize to systems of any arbitrary size. Finally, we discuss the interpretability of the MCLNN, which directly provides physical insights into the interactions of multi-particle systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OD_dnx57ksK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ravinder_Bhattoo1",
        "name": "Ravinder Bhattoo",
        "name_site": null,
        "openreview_id": "~Ravinder_Bhattoo1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ravinderbhattoo.github.io",
        "dblp_id": null,
        "google_scholar_url": "lPTdGRMAAAAJ",
        "orcid": "0000-0003-0323-9108",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OD_dnx57ksK",
      "title": "Momentum Conserving Lagrangian Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Realistic models of physical world rely on differentiable symmetries that, in turn, correspond to conservation laws. Recent works on Lagrangian and Hamiltonian neural networks show that the underlying symmetries of a system can be easily learned by a neural network when provided with an appropriate inductive bias. However, these models still suffer from issues such as inability to generalize to arbitrary system sizes, poor interpretability, and most importantly, inability to learn translational and rotational symmetries, which lead to the conservation laws of linear and angular momentum, respectively. Here, we present a momentum conserving Lagrangian neural network (MCLNN) that learns the Lagrangian of a system, while also preserving the translational and rotational symmetries. We test our approach on linear and non-linear spring systems, and a gravitational system, demonstrating the energy and momentum conservation. We also show that the model developed can generalize to systems of any arbitrary size. Finally, we discuss the interpretability of the MCLNN, which directly provides physical insights into the interactions of multi-particle systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OD_dnx57ksK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OD_dnx57ksK",
      "title": "Momentum Conserving Lagrangian Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Realistic models of physical world rely on differentiable symmetries that, in turn, correspond to conservation laws. Recent works on Lagrangian and Hamiltonian neural networks show that the underlying symmetries of a system can be easily learned by a neural network when provided with an appropriate inductive bias. However, these models still suffer from issues such as inability to generalize to arbitrary system sizes, poor interpretability, and most importantly, inability to learn translational and rotational symmetries, which lead to the conservation laws of linear and angular momentum, respectively. Here, we present a momentum conserving Lagrangian neural network (MCLNN) that learns the Lagrangian of a system, while also preserving the translational and rotational symmetries. We test our approach on linear and non-linear spring systems, and a gravitational system, demonstrating the energy and momentum conservation. We also show that the model developed can generalize to systems of any arbitrary size. Finally, we discuss the interpretability of the MCLNN, which directly provides physical insights into the interactions of multi-particle systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OD_dnx57ksK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OVShHe8Ce0",
      "title": "SAU: Smooth activation function using convolution with approximate identities",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Well-known activation functions like ReLU or Leaky ReLU are non-differentiable at the origin. Over the years, many smooth approximations of ReLU have been proposed using various smoothing techniques. We propose new smooth approximations of a non-differentiable activation function by convolving it with approximate identities. In particular, we present smooth approximations of Leaky ReLU and show that they outperform several well-known activation functions in various datasets and models. We call this function Smooth Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with ShuffleNet V2 (2.0x) model on the CIFAR100 dataset.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OVShHe8Ce0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Koushik_Biswas2",
        "name": "Koushik Biswas",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar7",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "06/5441-2",
        "google_scholar_url": "nENVPxUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OVShHe8Ce0",
      "title": "SAU: Smooth activation function using convolution with approximate identities",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Well-known activation functions like ReLU or Leaky ReLU are non-differentiable at the origin. Over the years, many smooth approximations of ReLU have been proposed using various smoothing techniques. We propose new smooth approximations of a non-differentiable activation function by convolving it with approximate identities. In particular, we present smooth approximations of Leaky ReLU and show that they outperform several well-known activation functions in various datasets and models. We call this function Smooth Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with ShuffleNet V2 (2.0x) model on the CIFAR100 dataset.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OVShHe8Ce0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar7",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Shilpak_Banerjee1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/shilpakbanerjee/home",
        "dblp_id": "274/2014",
        "google_scholar_url": "S3NP6lYAAAAJ",
        "orcid": "0000-0003-1036-9576",
        "linkedin_url": "shilpakbanerjee/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OVShHe8Ce0",
      "title": "SAU: Smooth activation function using convolution with approximate identities",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Well-known activation functions like ReLU or Leaky ReLU are non-differentiable at the origin. Over the years, many smooth approximations of ReLU have been proposed using various smoothing techniques. We propose new smooth approximations of a non-differentiable activation function by convolving it with approximate identities. In particular, we present smooth approximations of Leaky ReLU and show that they outperform several well-known activation functions in various datasets and models. We call this function Smooth Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with ShuffleNet V2 (2.0x) model on the CIFAR100 dataset.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OVShHe8Ce0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shilpak_Banerjee1",
        "name": "Shilpak Banerjee",
        "name_site": null,
        "openreview_id": "~Ashish_Kumar_Pandey1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "234/3896.html",
        "google_scholar_url": "XJqGr1UAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OVShHe8Ce0",
      "title": "SAU: Smooth activation function using convolution with approximate identities",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Well-known activation functions like ReLU or Leaky ReLU are non-differentiable at the origin. Over the years, many smooth approximations of ReLU have been proposed using various smoothing techniques. We propose new smooth approximations of a non-differentiable activation function by convolving it with approximate identities. In particular, we present smooth approximations of Leaky ReLU and show that they outperform several well-known activation functions in various datasets and models. We call this function Smooth Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with ShuffleNet V2 (2.0x) model on the CIFAR100 dataset.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OVShHe8Ce0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar_Pandey1",
        "name": "Ashish Kumar Pandey",
        "name_site": null,
        "openreview_id": "~Koushik_Biswas3",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "274/2151.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-9818-8966",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OdTx-22f6H",
      "title": "Utilizing Attention, Linked Blocks, And Pyramid Pooling To Propel Brain Tumor Segmentation In 3D",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We present an approach to detect and segment tumorous regions of the brain by establishing three varied segmentation architectures for multiclass semantic segmentation along with data specific customizations like residual blocks, soft attention mechanism, pyramid pooling, linked architecture and 3D compatibility to work with 3D brain MRI images. The proposed segmentation architectures namely, Attention Residual UNET 3D also referred to as AR-UNET 3D, LinkNet 3D and PSPNet 3D, segment the MRI images and succeed in isolating three classes of tumors. By assigning pixel probabilities, each of these models differentiates between pixels belonging to tumorous and non-tumorous regions of the brain. By experimenting and observing the performance of each of the three architectures using metrics like Dice loss and Dice score, on the BraTS2020 dataset, we successfully establish quality results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OdTx-22f6H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pooja_Ravi1",
        "name": "Pooja Ravi",
        "name_site": null,
        "openreview_id": "~Pooja_Ravi1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "pooja-ravi-9b88861b2/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "SRM Institute of Science and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.6,
        "confidence_std": 0.7999999999999999,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OdTx-22f6H",
      "title": "Utilizing Attention, Linked Blocks, And Pyramid Pooling To Propel Brain Tumor Segmentation In 3D",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We present an approach to detect and segment tumorous regions of the brain by establishing three varied segmentation architectures for multiclass semantic segmentation along with data specific customizations like residual blocks, soft attention mechanism, pyramid pooling, linked architecture and 3D compatibility to work with 3D brain MRI images. The proposed segmentation architectures namely, Attention Residual UNET 3D also referred to as AR-UNET 3D, LinkNet 3D and PSPNet 3D, segment the MRI images and succeed in isolating three classes of tumors. By assigning pixel probabilities, each of these models differentiates between pixels belonging to tumorous and non-tumorous regions of the brain. By experimenting and observing the performance of each of the three architectures using metrics like Dice loss and Dice score, on the BraTS2020 dataset, we successfully establish quality results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OdTx-22f6H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srijarko_Roy1",
        "name": "Srijarko Roy",
        "name_site": null,
        "openreview_id": "~Srijarko_Roy1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "srijarko-roy-9193751b0/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "SRM Institute of Science and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.6,
        "confidence_std": 0.7999999999999999,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OdTx-22f6H",
      "title": "Utilizing Attention, Linked Blocks, And Pyramid Pooling To Propel Brain Tumor Segmentation In 3D",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We present an approach to detect and segment tumorous regions of the brain by establishing three varied segmentation architectures for multiclass semantic segmentation along with data specific customizations like residual blocks, soft attention mechanism, pyramid pooling, linked architecture and 3D compatibility to work with 3D brain MRI images. The proposed segmentation architectures namely, Attention Residual UNET 3D also referred to as AR-UNET 3D, LinkNet 3D and PSPNet 3D, segment the MRI images and succeed in isolating three classes of tumors. By assigning pixel probabilities, each of these models differentiates between pixels belonging to tumorous and non-tumorous regions of the brain. By experimenting and observing the performance of each of the three architectures using metrics like Dice loss and Dice score, on the BraTS2020 dataset, we successfully establish quality results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OdTx-22f6H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Indira_Dutta1",
        "name": "Indira Dutta",
        "name_site": null,
        "openreview_id": "~Indira_Dutta1",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "indira-dutta-775445197",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "SRM Institute of Science and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.6,
        "confidence_std": 0.7999999999999999,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PKdNRKjwL4",
      "title": "DAIR: Data Augmented Invariant Regularization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We prove convergence guarantees for DAIR. We apply it to multiple real-world unsupervised and supervised learning problems involving  domain shift. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost. Furthermore, DAIR is competitive with state-of-the-art methods specifically designed for these problems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=PKdNRKjwL4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shaunak_Ashish_Halbe1",
        "name": "Shaunak Ashish Halbe",
        "name_site": null,
        "openreview_id": "~Shaunak_Ashish_Halbe1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://shaunak27.github.io",
        "dblp_id": "349/7625",
        "google_scholar_url": "7-VApYcAAAAJ",
        "orcid": "0000-0001-7388-6963",
        "linkedin_url": "shaunak-halbe-565a0716b",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "College of Engineering, Pune (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "R11xJsRjA-W",
      "title": "The Connection between Out-of-Distribution Generalization and Privacy of ML Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the goal of generalizing to out-of-distribution (OOD) data, recent domain generalization methods aim to learn ``stable'' feature representations whose effect on the output remains invariant across domains. Given the theoretical connection between generalization and privacy, we ask whether better OOD generalization leads to better privacy for machine learning models, where privacy is measured through robustness to membership inference (MI) attacks. In general, we find that the relationship does not hold. Through extensive evaluation on a synthetic dataset and image datasets like MNIST, Fashion-MNIST, and Chest X-rays, we show that a lower OOD generalization gap does not imply better robustness to MI attacks. Instead, privacy benefits are based on the extent to which a model captures the stable features. A model that captures stable features is more robust to MI attacks than models that exhibit better OOD generalization but do not learn stable features. Further, for the same provable differential privacy guarantees, a model that learns stable features provides higher utility as compared to others. Our results offer the first extensive empirical study connecting stable features and privacy, and also have a takeaway for the domain generalization community; MI attack can be used as a complementary metric to measure model quality.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=R11xJsRjA-W",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "R7APxKhg8dt",
      "title": "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have attempted to leverage structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to scale the knowledge. However, training on symbolic KG entities limits their application in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a task agnostic CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in NLP tasks for generating contextually relevant knowledge in the form of KG paths. We propose a novel dataset comprising of sentence and commonsense path pairs to train CoSe-Co. The knowledge paths inferred by CoSe-Co are diverse, relevant and contain novel entities not present in the underlying KG. Additionally, we show CoSe-Co can be used for KG completion. We augment the generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods (upto ~3% and ~7% respectively) on CSQA, ARC, QASC and OBQA datasets. Further, improved performance is seen in low training data regimes which shows CoSe-Co knowledge helps in generalising better.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=R7APxKhg8dt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rachit_Bansal1",
        "name": "Rachit Bansal",
        "name_site": null,
        "openreview_id": "~Rachit_Bansal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rachitbansal.github.io",
        "dblp_id": "228/6038",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7-x28WYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Delhi Technological University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RSd79AULOu",
      "title": "Fairness-aware Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized. In such a setting, naively minimizing an aggregate loss may introduce bias and disadvantage model performance on certain clients. To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set. By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable. We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees. We further prove generalization bounds for learning with this objective. Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RSd79AULOu",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SN2bkl9f69",
      "title": "Multi-Tailed, Multi-Headed, Spatial Dynamic Memory refined Text-to-Image Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Synthesizing high-quality, realistic images from text-descriptions is a challenging task, and current methods synthesize images from text in a multi-stage manner, typically by first generating a rough initial image and then refining image details at subsequent stages. However, existing methods that follow this paradigm suffer from three important limitations. Firstly, they synthesize initial images without attempting to separate image attributes at a word-level. As a result, object attributes of initial images (that provide a basis for subsequent refinement) are inherently entangled and ambiguous in nature. Secondly, by using common text-representations for all regions, current methods prevent us from interpreting text in fundamentally different ways at different parts of an image. Different image regions are therefore only allowed to assimilate the same type of information from text at each refinement stage. Finally, current methods generate refinement features only once at each refinement stage and attempt to address all image aspects in a single shot. This single-shot refinement limits the precision with which each refinement stage can learn to improve the prior image. Our proposed method introduces three novel components to address these shortcomings: (1) An initial generation stage that explicitly generates separate sets of image features for each word n-gram. (2) A spatial dynamic memory module for refinement of images. (3) An iterative multi-headed mechanism to make it easier to improve upon multiple image aspects. Experimental results demonstrate that our Multi-Headed Spatial Dynamic Memory image refinement with our Multi-Tailed Word-level Initial Generation (MSMT-GAN) performs favourably against the previous state of the art on the CUB and COCO datasets. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=SN2bkl9f69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amrit_Diggavi_Seshadri1",
        "name": "Amrit Diggavi Seshadri",
        "name_site": null,
        "openreview_id": "~Amrit_Diggavi_Seshadri1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SN2bkl9f69",
      "title": "Multi-Tailed, Multi-Headed, Spatial Dynamic Memory refined Text-to-Image Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Synthesizing high-quality, realistic images from text-descriptions is a challenging task, and current methods synthesize images from text in a multi-stage manner, typically by first generating a rough initial image and then refining image details at subsequent stages. However, existing methods that follow this paradigm suffer from three important limitations. Firstly, they synthesize initial images without attempting to separate image attributes at a word-level. As a result, object attributes of initial images (that provide a basis for subsequent refinement) are inherently entangled and ambiguous in nature. Secondly, by using common text-representations for all regions, current methods prevent us from interpreting text in fundamentally different ways at different parts of an image. Different image regions are therefore only allowed to assimilate the same type of information from text at each refinement stage. Finally, current methods generate refinement features only once at each refinement stage and attempt to address all image aspects in a single shot. This single-shot refinement limits the precision with which each refinement stage can learn to improve the prior image. Our proposed method introduces three novel components to address these shortcomings: (1) An initial generation stage that explicitly generates separate sets of image features for each word n-gram. (2) A spatial dynamic memory module for refinement of images. (3) An iterative multi-headed mechanism to make it easier to improve upon multiple image aspects. Experimental results demonstrate that our Multi-Headed Spatial Dynamic Memory image refinement with our Multi-Tailed Word-level Initial Generation (MSMT-GAN) performs favourably against the previous state of the art on the CUB and COCO datasets. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=SN2bkl9f69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaraman_Ravindran1",
        "name": "Balaraman Ravindran",
        "name_site": null,
        "openreview_id": "~Balaraman_Ravindran1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~ravi",
        "dblp_id": "69/2281",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-5364-7639",
        "linkedin_url": "ravindran-balaraman-427a307",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TEKnz3B1jGF",
      "title": "Visio-Linguistic Brain Encoding",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Enabling effective brain-computer interfaces needs understanding how the human brain encodes stimuli across modalities such as visual, language (or text), etc. Brain encoding aims at constructing fMRI brain activity given a stimulus. There exist a plethora of neural encoding models which study brain encoding for single-mode stimuli: visual (pretrained CNNs) or text (pretrained language models). Few recent papers have also obtained separate visual and text representation models and performed late-fusion using simple heuristics. However, previous work has failed to explore: (a) the effectiveness of image Transformer models for encoding visual stimuli, and (b) co-attentive multi-modal modeling for visual and text reasoning. Further, as pretrained image Transformers and multi-modal Transformers have continued to evolve, it is important to understand if they are becoming more brain-like and hence lead to improved brain encoding. In this paper, we systematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT) and multi-modal Transformers (VisualBERT, LXMERT, ViLBERT, and CLIP) for brain encoding. Extensive experiments on two popular datasets, BOLD5000 and Pereira, provide the following insights. (1) To the best of our knowledge, we are the first to investigate the effectiveness of image and multi-modal Transformers for brain encoding. (2) Surprisingly, we observe a better encoding correlation between Transformer model layers and the levels of visual processing in the human brain when compared to CNN architectures. (3) We find that multi-modal Transformers significantly outperform previously proposed single-mode CNNs, image Transformers as well as other previously proposed multi-modal models, thereby establishing new state-of-the-art. The supremacy of visio-linguistic models raises the question of whether the responses elicited in the visual regions are affected implicitly by linguistic processing even when passively viewing images. Future fMRI tasks can verify this computational insight in an appropriate experimental setting. We make our code publicly available.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=TEKnz3B1jGF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "jashn.arora@research.iiit.ac.in",
        "name": "Jashn Arora",
        "name_site": null,
        "openreview_id": "jashn.arora@research.iiit.ac.in",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ti2i204vZON",
      "title": "Learning Representations for Pixel-based Control: What Matters and Why?",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well-curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like the density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Ti2i204vZON",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Utkarsh_Aashu_Mishra2",
        "name": "Utkarsh Aashu Mishra",
        "name_site": null,
        "openreview_id": "~Utkarsh_Aashu_Mishra2",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://utkarshmishra04.github.io/",
        "dblp_id": "274/2706",
        "google_scholar_url": "10HbT44AAAAJ",
        "orcid": "0000-0002-4977-5187",
        "linkedin_url": "utkarshamishra/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 22,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UxBH9j8IE_H",
      "title": "Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=UxBH9j8IE_H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~BITHIKA_PAL1",
        "name": "BITHIKA PAL",
        "name_site": null,
        "openreview_id": "~BITHIKA_PAL1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://sites.google.com/view/bithikapal/",
        "dblp_id": "217/5181",
        "google_scholar_url": "x7-BaeQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UxBH9j8IE_H",
      "title": "Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=UxBH9j8IE_H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arindam_Biswas1",
        "name": "Arindam Biswas",
        "name_site": null,
        "openreview_id": "~Arindam_Biswas1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UxBH9j8IE_H",
      "title": "Revisiting the Lottery Ticket Hypothesis: A Ramanujan Graph Perspective",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Neural networks often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these `lottery ticket' subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning two distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a new magnitude-based pruning algorithm to preserve the above property. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=UxBH9j8IE_H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pabitra_Mitra1",
        "name": "Pabitra Mitra",
        "name_site": null,
        "openreview_id": "~Pabitra_Mitra1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~pabitra/",
        "dblp_id": "m/PabitraMitra",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=5bXSZPYAAAAJ",
        "orcid": "0000-0002-1908-9813",
        "linkedin_url": "pabitra-mitra-8028235/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Trinity College Dublin (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "WGhT5zCamoC",
      "title": "Seq2Tok: Deep Sequence Tokenizer for Retrieval",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Search over sequences is a fundamental problem. Very efficient solutions exist for text sequences, which are made up of discrete tokens chosen from a finite alphabet. Sequences, such as audio, video or sensor readings, are made up of continuous-valued samples with a large sampling rate, making similarity search inefficient. This paper proposes Seq2Tok, a deep sequence tokenizer that converts continuous-valued sequences to discrete tokens that are easier to retrieve via sequence queries. The only information available for training Seq2Tok is pairs of similar sequences, i.e., depending on how we form the pairs, the similarity semantics are learnt. Seq2Tok compresses the query and target sequences into short sequences of tokens that are faster to match. Experiments show consistent performance of Seq2Tok across various audio retrieval tasks, namely, music search (query by humming) and speech keyword search via audio query.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=WGhT5zCamoC",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "adhiraj@iitk.ac.in",
        "name": "Adhiraj Banerjee",
        "name_site": null,
        "openreview_id": "adhiraj@iitk.ac.in",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "WGhT5zCamoC",
      "title": "Seq2Tok: Deep Sequence Tokenizer for Retrieval",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Search over sequences is a fundamental problem. Very efficient solutions exist for text sequences, which are made up of discrete tokens chosen from a finite alphabet. Sequences, such as audio, video or sensor readings, are made up of continuous-valued samples with a large sampling rate, making similarity search inefficient. This paper proposes Seq2Tok, a deep sequence tokenizer that converts continuous-valued sequences to discrete tokens that are easier to retrieve via sequence queries. The only information available for training Seq2Tok is pairs of similar sequences, i.e., depending on how we form the pairs, the similarity semantics are learnt. Seq2Tok compresses the query and target sequences into short sequences of tokens that are faster to match. Experiments show consistent performance of Seq2Tok across various audio retrieval tasks, namely, music search (query by humming) and speech keyword search via audio query.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=WGhT5zCamoC",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vipul_Arora1",
        "name": "Vipul Arora",
        "name_site": null,
        "openreview_id": "~Vipul_Arora1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~vipular",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=SC9YYPAAAAAJ",
        "orcid": "0000-0002-1207-1258",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "WRORN3GUCu",
      "title": "VISCOS Flows: Variational Schur Conditional Sampling with Normalizing Flows",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present a method for conditional sampling for pre-trained normalizing flows when only part of an observation is available. We derive a lower bound to the conditioning variable log-probability using Schur complement properties in the spirit of Gaussian conditional sampling. Our derivation relies on partitioning flow's domain in such a way that the flow restrictions to subdomains remain bijective, which is crucial for the Schur complement application. Simulation from the variational conditional flow then amends to solving an equality constraint. Our contribution is three-fold: a) we provide detailed insights on the choice of variational distributions; b) we discuss how to partition the input space of the flow to preserve bijectivity property; c) we propose a set of methods to optimise the variational distribution. Our numerical results indicate that our sampling method can be successfully applied to invertible residual networks for inference and classification.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=WRORN3GUCu",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Huawei (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Yc64t25hseP",
      "title": "GUIDED MCMC FOR SPARSE BAYESIAN MODELS TO DETECT RARE EVENTS IN IMAGES SANS LABELED DATA",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Detection of rare events in images is a challenging task because of two main problems, the first problem is the lack of labeled data for rare category class and the second problem is a highly imbalanced data problem. Training models in this scenario becomes hard. Unsupervised methods do not apply as we need to detect rare events automatically. Rule-based methods seem to be the only viable solution, but it is tedious to come up with a set of rules covering all corner cases. Even the recently popular zero-shot learning techniques required to be pre-trained on auxiliary datasets. In the given scenario, we propose an approach to provide little guidance from experts as an input into a hierarchical Bayesian model. The guidance influences the Markov chain Monte Carlo (MCMC) based inference technique of the model. After the steady-state is obtained for the underlying Markov chain, it is possible to compute the posterior probability of the presence of the rare event in a given image. The proposed method neither needs any labeled data nor required pre-training, unlike zero-shot learning. The proposed technique has been observed to outperform the state-of-the-art unsupervised image classification techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Yc64t25hseP",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Jain4",
        "name": "Gaurav Jain",
        "name_site": null,
        "openreview_id": "~Gaurav_Jain4",
        "position": 1,
        "gender": null,
        "homepage_url": "https://gauravjain10.github.io/resume/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "gaurav-jain-b12848aa/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Palakkad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 1.479019945774904,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_7YnfGdDVML",
      "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Detection of semantic data types is a very crucial task in data science for automated data cleaning, schema matching, data discovery, semantic data type normalization and sensitive data identification. Existing methods include regular expression-based or dictionary lookup-based methods that are not robust to dirty as well unseen data and are limited to a very less number of semantic data types to predict. Existing Machine Learning methods extract a large number of engineered features from data and build logistic regression, random forest or feedforward neural network for this purpose. In this paper, we introduce DCoM, a collection of multi-input NLP-based deep neural networks to detect semantic data types where instead of extracting a large number of features from the data, we feed the raw values of columns (or instances) to the model as texts. We train DCoM on 686,765 data columns extracted from the VizNet corpus with 78 different semantic data types. DCoM outperforms other contemporary results with a quite significant margin on the same dataset achieving a support-weighted F1 score of 0.925. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_7YnfGdDVML",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhadip_Maji1",
        "name": "Subhadip Maji",
        "name_site": null,
        "openreview_id": "~Subhadip_Maji1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "259/1375",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0002-8802-1572",
        "linkedin_url": "subhadip-maji-66283810a/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_7YnfGdDVML",
      "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Detection of semantic data types is a very crucial task in data science for automated data cleaning, schema matching, data discovery, semantic data type normalization and sensitive data identification. Existing methods include regular expression-based or dictionary lookup-based methods that are not robust to dirty as well unseen data and are limited to a very less number of semantic data types to predict. Existing Machine Learning methods extract a large number of engineered features from data and build logistic regression, random forest or feedforward neural network for this purpose. In this paper, we introduce DCoM, a collection of multi-input NLP-based deep neural networks to detect semantic data types where instead of extracting a large number of features from the data, we feed the raw values of columns (or instances) to the model as texts. We train DCoM on 686,765 data columns extracted from the VizNet corpus with 78 different semantic data types. DCoM outperforms other contemporary results with a quite significant margin on the same dataset achieving a support-weighted F1 score of 0.925. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_7YnfGdDVML",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swapna_sourav_Rout1",
        "name": "Swapna sourav Rout",
        "name_site": null,
        "openreview_id": "~Swapna_sourav_Rout1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "4Kk-P-AAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_B8Jd7Nqs7R",
      "title": "Improved Generalization Bound for Deep Neural Networks Using Geometric Functional Analysis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Understanding how a neural network behaves in multiple domains is the key to further  its  explainability,  generalizability,  and  robustness. In this paper, we prove a novel generalization bound using the fundamental concepts of geometric functional analysis. Specifically, by leveraging the covering number of the training dataset and applying certain geometric inequalities we show that a sharp bound can be obtained. To the best of our knowledge this is the first approach which utilizes covering numbers to estimate such generalization bounds.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_B8Jd7Nqs7R",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Phani_raj_Chinnalingu1",
        "name": "Phani raj Chinnalingu",
        "name_site": null,
        "openreview_id": "~Phani_raj_Chinnalingu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~nextgenwrl/Members.html",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_PlNmPOsUS9",
      "title": "PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The security of Deep Learning classifiers is a critical field of study because of the existence of adversarial attacks. Such attacks usually rely on the principle of transferability, where an adversarial example crafted on a surrogate classifier tends to mislead the target classifier trained on the same dataset even if both classifiers have quite different architecture. Ensemble methods against adversarial attacks demonstrate that an adversarial example is less likely to mislead multiple classifiers in an ensemble having diverse decision boundaries. However, recent ensemble methods have either been shown to be vulnerable to stronger adversaries or shown to lack an end-to-end evaluation. This paper attempts to develop a new ensemble methodology that constructs multiple diverse classifiers using a Pairwise Adversarially Robust Loss (PARL) function during the training procedure. PARL utilizes gradients of each layer with respect to input in every classifier within the ensemble simultaneously. The proposed training procedure enables PARL to achieve higher robustness with high clean example accuracy against black-box transfer attacks compared to the previous ensemble methods. We also evaluate the robustness in the presence of white-box attacks, where adversarial examples are crafted on the target classifier. We present extensive experiments using standard image classification datasets like CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against state-of-the-art adversarial attacks to demonstrate the robustness of the proposed ensemble methodology.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_PlNmPOsUS9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manaar_Alam1",
        "name": "Manaar Alam",
        "name_site": null,
        "openreview_id": "~Manaar_Alam1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://manaaralam.github.io",
        "dblp_id": "192/5163",
        "google_scholar_url": "46jmlGgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_PlNmPOsUS9",
      "title": "PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The security of Deep Learning classifiers is a critical field of study because of the existence of adversarial attacks. Such attacks usually rely on the principle of transferability, where an adversarial example crafted on a surrogate classifier tends to mislead the target classifier trained on the same dataset even if both classifiers have quite different architecture. Ensemble methods against adversarial attacks demonstrate that an adversarial example is less likely to mislead multiple classifiers in an ensemble having diverse decision boundaries. However, recent ensemble methods have either been shown to be vulnerable to stronger adversaries or shown to lack an end-to-end evaluation. This paper attempts to develop a new ensemble methodology that constructs multiple diverse classifiers using a Pairwise Adversarially Robust Loss (PARL) function during the training procedure. PARL utilizes gradients of each layer with respect to input in every classifier within the ensemble simultaneously. The proposed training procedure enables PARL to achieve higher robustness with high clean example accuracy against black-box transfer attacks compared to the previous ensemble methods. We also evaluate the robustness in the presence of white-box attacks, where adversarial examples are crafted on the target classifier. We present extensive experiments using standard image classification datasets like CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against state-of-the-art adversarial attacks to demonstrate the robustness of the proposed ensemble methodology.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_PlNmPOsUS9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubhajit_Datta1",
        "name": "Shubhajit Datta",
        "name_site": null,
        "openreview_id": "~Shubhajit_Datta1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "mgOVQYQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_PlNmPOsUS9",
      "title": "PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The security of Deep Learning classifiers is a critical field of study because of the existence of adversarial attacks. Such attacks usually rely on the principle of transferability, where an adversarial example crafted on a surrogate classifier tends to mislead the target classifier trained on the same dataset even if both classifiers have quite different architecture. Ensemble methods against adversarial attacks demonstrate that an adversarial example is less likely to mislead multiple classifiers in an ensemble having diverse decision boundaries. However, recent ensemble methods have either been shown to be vulnerable to stronger adversaries or shown to lack an end-to-end evaluation. This paper attempts to develop a new ensemble methodology that constructs multiple diverse classifiers using a Pairwise Adversarially Robust Loss (PARL) function during the training procedure. PARL utilizes gradients of each layer with respect to input in every classifier within the ensemble simultaneously. The proposed training procedure enables PARL to achieve higher robustness with high clean example accuracy against black-box transfer attacks compared to the previous ensemble methods. We also evaluate the robustness in the presence of white-box attacks, where adversarial examples are crafted on the target classifier. We present extensive experiments using standard image classification datasets like CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against state-of-the-art adversarial attacks to demonstrate the robustness of the proposed ensemble methodology.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_PlNmPOsUS9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debdeep_Mukhopadhyay2",
        "name": "Debdeep Mukhopadhyay",
        "name_site": null,
        "openreview_id": "~Debdeep_Mukhopadhyay2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/debdeepmukhopadhyay/",
        "dblp_id": null,
        "google_scholar_url": "2ELnl9IAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_PlNmPOsUS9",
      "title": "PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The security of Deep Learning classifiers is a critical field of study because of the existence of adversarial attacks. Such attacks usually rely on the principle of transferability, where an adversarial example crafted on a surrogate classifier tends to mislead the target classifier trained on the same dataset even if both classifiers have quite different architecture. Ensemble methods against adversarial attacks demonstrate that an adversarial example is less likely to mislead multiple classifiers in an ensemble having diverse decision boundaries. However, recent ensemble methods have either been shown to be vulnerable to stronger adversaries or shown to lack an end-to-end evaluation. This paper attempts to develop a new ensemble methodology that constructs multiple diverse classifiers using a Pairwise Adversarially Robust Loss (PARL) function during the training procedure. PARL utilizes gradients of each layer with respect to input in every classifier within the ensemble simultaneously. The proposed training procedure enables PARL to achieve higher robustness with high clean example accuracy against black-box transfer attacks compared to the previous ensemble methods. We also evaluate the robustness in the presence of white-box attacks, where adversarial examples are crafted on the target classifier. We present extensive experiments using standard image classification datasets like CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against state-of-the-art adversarial attacks to demonstrate the robustness of the proposed ensemble methodology.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_PlNmPOsUS9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arijit_Mondal1",
        "name": "Arijit Mondal",
        "name_site": null,
        "openreview_id": "~Arijit_Mondal1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "08/5656.html",
        "google_scholar_url": null,
        "orcid": "0000-0001-5060-1427",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Patna (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_PlNmPOsUS9",
      "title": "PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial Attacks via Pairwise Adversarially Robust Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The security of Deep Learning classifiers is a critical field of study because of the existence of adversarial attacks. Such attacks usually rely on the principle of transferability, where an adversarial example crafted on a surrogate classifier tends to mislead the target classifier trained on the same dataset even if both classifiers have quite different architecture. Ensemble methods against adversarial attacks demonstrate that an adversarial example is less likely to mislead multiple classifiers in an ensemble having diverse decision boundaries. However, recent ensemble methods have either been shown to be vulnerable to stronger adversaries or shown to lack an end-to-end evaluation. This paper attempts to develop a new ensemble methodology that constructs multiple diverse classifiers using a Pairwise Adversarially Robust Loss (PARL) function during the training procedure. PARL utilizes gradients of each layer with respect to input in every classifier within the ensemble simultaneously. The proposed training procedure enables PARL to achieve higher robustness with high clean example accuracy against black-box transfer attacks compared to the previous ensemble methods. We also evaluate the robustness in the presence of white-box attacks, where adversarial examples are crafted on the target classifier. We present extensive experiments using standard image classification datasets like CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against state-of-the-art adversarial attacks to demonstrate the robustness of the proposed ensemble methodology.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_PlNmPOsUS9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Partha_Pratim_Chakrabarti1",
        "name": "Partha Pratim Chakrabarti",
        "name_site": null,
        "openreview_id": "~Partha_Pratim_Chakrabarti1",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://www.iitkgp.ac.in/department/CS/faculty/cs-ppchak",
        "dblp_id": "c/PPChakrabarti.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-3553-8834",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aYSlxlHKEA",
      "title": "Fully Decentralized Model-based Policy Optimization with Networked Agents",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Model-based RL is an effective approach for reducing sample complexity. However, when it comes to multi-agent setting where the number of agent is large, the model estimation can be problematic due to the exponential increased interactions. In this paper, we propose a decentralized model-based reinforcement learning algorithm for networked multi-agent systems, where agents are cooperative and communicate locally with their neighbors. We analyze our algorithm theoretically and derive an upper bound of performance discrepancy caused by model usage, and provide a sufficient condition of monotonic policy improvement. In our experiments, we compare our algorithm against other strong multi-agent baselines and demonstrate that our algorithm not only matches the asymptotic performance of model-free methods but also largely increases its sample efficiency.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=aYSlxlHKEA",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Peking University (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b30Yre8MzuN",
      "title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=b30Yre8MzuN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_Ranjan1",
        "name": "Rishabh Ranjan",
        "name_site": null,
        "openreview_id": "~Rishabh_Ranjan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rishabh-ranjan.github.io",
        "dblp_id": null,
        "google_scholar_url": "NNzQUrcAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b30Yre8MzuN",
      "title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=b30Yre8MzuN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddharth_Grover1",
        "name": "Siddharth Grover",
        "name_site": null,
        "openreview_id": "~Siddharth_Grover1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "siddharth-grover-173853184",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b30Yre8MzuN",
      "title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=b30Yre8MzuN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourav_Medya1",
        "name": "Sourav Medya",
        "name_site": null,
        "openreview_id": "~Sourav_Medya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://souravmedya.github.io/",
        "dblp_id": "178/3021",
        "google_scholar_url": "RCFhOM4AAAAJ",
        "orcid": "0000-0003-0996-2807",
        "linkedin_url": "sourav-medya-35987a49/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Northwestern University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b30Yre8MzuN",
      "title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=b30Yre8MzuN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesan_Chakaravarthy1",
        "name": "Venkatesan Chakaravarthy",
        "name_site": null,
        "openreview_id": "~Venkatesan_Chakaravarthy1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://dblp.org/pid/c/VTChakaravarthy.html",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_3I7KHAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b30Yre8MzuN",
      "title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=b30Yre8MzuN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cAuJrUm8lG",
      "title": "Implicit Equivariance in Convolutional Networks",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Convolutional Neural Networks (CNN) are inherently equivariant under translations, however, they do not have an equivalent embedded mechanism to handle other transformations such as rotations and change in scale. Several approaches have been proposed that make CNNs equivariant under other transformation groups by design. Among these, steerable CNNs have been especially effective. However, these approaches require redesigning standard networks with filters mapped from combinations of predefined basis involving complex analytical functions. We experimentally demonstrate that these restrictions in the choice of basis can lead to model weights that are sub-optimal for the primary deep learning task (e.g. classification). Moreover, such hard-baked explicit formulations make it difficult to design composite networks comprising heterogeneous feature groups. To circumvent such issues, we propose Implicitly Equivariant Networks (IEN) which induce equivariance in the different layers of a standard CNN model by optimizing a multi-objective loss function that combines the  primary loss with an equivariance loss term. Through experiments with VGG and ResNet models on Rot-MNIST , Rot-TinyImageNet, Scale-MNIST and STL-10 datasets, we show that IEN, even with its simple formulation, performs better than steerable networks.  Also, IEN facilitates construction of heterogeneous filter groups allowing to reduce channels in CNNs by factors of over 30% while maintaining performance at par with baselines.  The efficacy of IEN is further validated on the hard problem of visual object tracking. We show that IEN outperforms state-of-the-art rotation equivariant tracking method while providing faster inference speed.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cAuJrUm8lG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suraj_Sharan1",
        "name": "Suraj Sharan",
        "name_site": null,
        "openreview_id": "~Samee_Ur_Rehman1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "surehman/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Dhanbad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cAuJrUm8lG",
      "title": "Implicit Equivariance in Convolutional Networks",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Convolutional Neural Networks (CNN) are inherently equivariant under translations, however, they do not have an equivalent embedded mechanism to handle other transformations such as rotations and change in scale. Several approaches have been proposed that make CNNs equivariant under other transformation groups by design. Among these, steerable CNNs have been especially effective. However, these approaches require redesigning standard networks with filters mapped from combinations of predefined basis involving complex analytical functions. We experimentally demonstrate that these restrictions in the choice of basis can lead to model weights that are sub-optimal for the primary deep learning task (e.g. classification). Moreover, such hard-baked explicit formulations make it difficult to design composite networks comprising heterogeneous feature groups. To circumvent such issues, we propose Implicitly Equivariant Networks (IEN) which induce equivariance in the different layers of a standard CNN model by optimizing a multi-objective loss function that combines the  primary loss with an equivariance loss term. Through experiments with VGG and ResNet models on Rot-MNIST , Rot-TinyImageNet, Scale-MNIST and STL-10 datasets, we show that IEN, even with its simple formulation, performs better than steerable networks.  Also, IEN facilitates construction of heterogeneous filter groups allowing to reduce channels in CNNs by factors of over 30% while maintaining performance at par with baselines.  The efficacy of IEN is further validated on the hard problem of visual object tracking. We show that IEN outperforms state-of-the-art rotation equivariant tracking method while providing faster inference speed.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cAuJrUm8lG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eFP90pzlIz",
      "title": "Towards Achieving Adversarial Robustness Beyond Perceptual Limits",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim towards defending attacks constrained within low magnitude $\\ell_p$ norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not, makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an $\\ell_\\infty$ bound of $16/255$ on CIFAR-10) while outperforming existing defenses (AWP, TRADES and PGD-AT) at standard perturbation bounds ($8/255$) as well.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=eFP90pzlIz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sravanti_Addepalli1",
        "name": "Sravanti Addepalli",
        "name_site": null,
        "openreview_id": "~Sravanti_Addepalli1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "127/7715",
        "google_scholar_url": "MOO12i0AAAAJ",
        "orcid": null,
        "linkedin_url": "sravanti-addepalli/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eFP90pzlIz",
      "title": "Towards Achieving Adversarial Robustness Beyond Perceptual Limits",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim towards defending attacks constrained within low magnitude $\\ell_p$ norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not, makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an $\\ell_\\infty$ bound of $16/255$ on CIFAR-10) while outperforming existing defenses (AWP, TRADES and PGD-AT) at standard perturbation bounds ($8/255$) as well.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=eFP90pzlIz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samyak_Jain1",
        "name": "Samyak Jain",
        "name_site": null,
        "openreview_id": "~Samyak_Jain1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://samyakjain0112.github.io/",
        "dblp_id": "249/4464.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-3785-4782",
        "linkedin_url": "samyak-jain-276738178/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eFP90pzlIz",
      "title": "Towards Achieving Adversarial Robustness Beyond Perceptual Limits",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim towards defending attacks constrained within low magnitude $\\ell_p$ norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not, makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an $\\ell_\\infty$ bound of $16/255$ on CIFAR-10) while outperforming existing defenses (AWP, TRADES and PGD-AT) at standard perturbation bounds ($8/255$) as well.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=eFP90pzlIz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "e_FK_rDajEv",
      "title": "Learning Neural Causal Models with Active Interventions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing scaling properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far, differentiable causal discovery has focused on static datasets of observational or interventional origin. In this work, we introduce an active intervention-targeting mechanism which enables quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across multiple frameworks in a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=e_FK_rDajEv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "GlaxoSmithKline (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 52,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "g9hjVsv3lOC",
      "title": "Deep Neural Networks on EEG signals to predict Attention Score using Gramian Angular Difference Field",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Auditory attention is a selective type of hearing in which people focus their attention intentionally on a specific source of a sound or spoken words whilst ignoring or inhibiting other auditory stimuli. In some sense, the auditory attention score of an individual shows the focus the person can have in auditory tasks. The recent advancements in deep learning and in the non-invasive technologies recording neural activity beg the question, can deep learning along with technologies such as electroencephalography (EEG) be used to predict the auditory attention score of an individual? In this paper, we focus on this very problem of estimating a person's auditory attention level based on their brain's electrical activity captured using 14-channeled EEG signals. More specifically, we deal with attention estimation as a regression problem. The work has been performed on the publicly available Phyaat dataset. The concept of Gramian Angular Difference Field (GADF) has been used to convert time-series EEG data into an image having 14 channels, enabling us to train various deep learning models such as 2D CNN, 3D CNN, and convolutional autoencoders. Their performances have been compared amongst themselves as well as with the work done previously. Amongst the different models we tried, 2D CNN gave the best performance. It outperformed the existing methods by a decent margin of 0.22 mean absolute error (MAE).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=g9hjVsv3lOC",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mahak_Kothari1",
        "name": "Mahak Kothari",
        "name_site": null,
        "openreview_id": "~Mahak_Kothari1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mahak-kothari-001982167/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science, Pilani (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "g9hjVsv3lOC",
      "title": "Deep Neural Networks on EEG signals to predict Attention Score using Gramian Angular Difference Field",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Auditory attention is a selective type of hearing in which people focus their attention intentionally on a specific source of a sound or spoken words whilst ignoring or inhibiting other auditory stimuli. In some sense, the auditory attention score of an individual shows the focus the person can have in auditory tasks. The recent advancements in deep learning and in the non-invasive technologies recording neural activity beg the question, can deep learning along with technologies such as electroencephalography (EEG) be used to predict the auditory attention score of an individual? In this paper, we focus on this very problem of estimating a person's auditory attention level based on their brain's electrical activity captured using 14-channeled EEG signals. More specifically, we deal with attention estimation as a regression problem. The work has been performed on the publicly available Phyaat dataset. The concept of Gramian Angular Difference Field (GADF) has been used to convert time-series EEG data into an image having 14 channels, enabling us to train various deep learning models such as 2D CNN, 3D CNN, and convolutional autoencoders. Their performances have been compared amongst themselves as well as with the work done previously. Amongst the different models we tried, 2D CNN gave the best performance. It outperformed the existing methods by a decent margin of 0.22 mean absolute error (MAE).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=g9hjVsv3lOC",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shreyansh_Joshi1",
        "name": "Shreyansh Joshi",
        "name_site": null,
        "openreview_id": "~Shreyansh_Joshi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://shreyanshjoshi.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "shreyansh-joshi-b7135018a/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science, Pilani (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ht61oVsaya",
      "title": "DESTA: A Framework for Safe Reinforcement Learning with Markov Games of Intervention",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Exploring in an unknown system can place an agent in dangerous situations,\nexposing to potentially catastrophic hazards. Many current approaches for tackling\nsafe learning in reinforcement learning (RL) lead to a trade-off between safe\nexploration and fulfilling the task. Though these methods possibly incur fewer\nsafety violations they often also lead to reduced task performance. In this paper, we\ntake the first step in introducing a generation of RL solvers that learn to minimise\nsafety violations while maximising the task reward to the extend that can be\ntolerated by safe policies. Our approach uses a new two-player framework for safe\nRL called DESTA. The core of DESTA is a novel game between two RL agents:\nSafety Agent that is delegated the task of minimising safety violations and Task\nAgent whose goal is to maximise the reward set by the environment task. Safety\nAgent can selectively take control of the system at any given point to prevent\nsafety violations while Task Agent is free to execute its actions at all other states.\nThis framework enables Safety Agent to learn to take actions that minimise future\nsafety violations (during and after training) by performing safe actions at certain\nstates while Task Agent performs actions that maximise the task performance\neverywhere else. We demonstrate DESTA’s ability to tackle challenging tasks and\ncompare against state-of-the-art RL methods in Safety Gym Benchmarks which\nsimulate real-world physical systems and OpenAI’s Lunar Lander.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ht61oVsaya",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University College London (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kO-wQWwqnO",
      "title": "L2BGAN: An image enhancement model for image quality improvement and image analysis tasks without paired supervision",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The paper presents an image enhancement model,\nL2BGAN, to translate low light images to bright images\nwithout a paired supervision. We introduce the use of geo-\nmetric and lighting consistency along with a contextual loss\ncriterion. These when combined with multiscale color, tex-\nture and edge discriminators prove to provide competitive\nresults. We perform extensive experiments on benchmark\ndatasets to compare our results visually as well as objec-\ntively. We observe the performance of L2BGAN on real time\ndriving datasets which are subject to motion blur, noise and\nother artifacts. We further demonstrate the application of\nimage understanding tasks on our enhanced images using\nDarkFace and ExDark datasets.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=kO-wQWwqnO",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jhilik_Bhattacharya1",
        "name": "Jhilik Bhattacharya",
        "name_site": null,
        "openreview_id": "~Jhilik_Bhattacharya1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://sites.google.com/thapar.edu/jhilikbhattacharya/home",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Thapar Institute of Engineering and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kO-wQWwqnO",
      "title": "L2BGAN: An image enhancement model for image quality improvement and image analysis tasks without paired supervision",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The paper presents an image enhancement model,\nL2BGAN, to translate low light images to bright images\nwithout a paired supervision. We introduce the use of geo-\nmetric and lighting consistency along with a contextual loss\ncriterion. These when combined with multiscale color, tex-\nture and edge discriminators prove to provide competitive\nresults. We perform extensive experiments on benchmark\ndatasets to compare our results visually as well as objec-\ntively. We observe the performance of L2BGAN on real time\ndriving datasets which are subject to motion blur, noise and\nother artifacts. We further demonstrate the application of\nimage understanding tasks on our enhanced images using\nDarkFace and ExDark datasets.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=kO-wQWwqnO",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "ramponi@units.it",
        "name": "Gianni Ramponi",
        "name_site": null,
        "openreview_id": "ramponi@units.it",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Thapar Institute of Engineering & Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "liIJKb1gudP",
      "title": "Center Loss Regularization for Continual Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The ability to learn different tasks sequentially is essential to the development of artificial intelligence. In general, neural networks lack this capability, the major obstacle being catastrophic forgetting. It occurs when the incrementally available information from non-stationary data distributions is continually acquired, disrupting what the model has already learned. Our approach remembers old tasks by projecting the representations of new tasks close to that of old tasks while keeping the decision boundaries unchanged. We employ the center loss as a regularization penalty that enforces new tasks' features to have the same class centers as old tasks and makes the features highly discriminative. This, in turn, leads to the least forgetting of already learned information. This method is easy to implement, requires minimal computational and memory overhead, and allows the neural network to maintain high performance across many sequentially encountered tasks. We also demonstrate that using the center loss in conjunction with the memory replay outperforms other replay-based strategies. Along with standard MNIST variants for continual learning, we apply our method to continual domain adaptation scenarios with the Digits and PACS datasets. We demonstrate that our approach is scalable, effective, and gives competitive performance compared to state-of-the-art continual learning methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=liIJKb1gudP",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kaustubh_Olpadkar1",
        "name": "Kaustubh Olpadkar",
        "name_site": null,
        "openreview_id": "~Kaustubh_Olpadkar1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.4,
        "confidence_std": 0.48989794855663565,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "oPON8TpOQVz",
      "title": "Chameleon Sampling: Diverse and Pure Example Selection for Online Continual Learning with Noisy Labels",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "AI models suffer from continuously changing data distribution and noisy labels when applied to most real-world problems. Although many solutions have addressed issues for each problem of continual learning or noisy label, tackling both issues is of importance and yet underexplored. Here, we address the task of online continual learning with noisy labels, which is a more realistic, practical, and challenging continual learning setup by assuming ground-truth labels may be noisy. Specifically, we argue the importance of both diversity and purity of examples in the episodic memory of continual learning models. To balance diversity and purity in the memory, we propose to combine a novel memory management strategy and robust learning. Specifically, we propose a metric to balance the trade-off between diversity and purity in the episodic memory with noisy labels. We then refurbish or apply unsupervised learning by splitting noisy examples into multiple groups using the Gaussian mixture model for addressing label noise. We validate our approach on four real-world or synthetic benchmark datasets, including two CIFARs, Clothing1M, and mini-WebVision, demonstrate significant improvements over representative methods on this challenging task set-up.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=oPON8TpOQVz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "onwTC5W0XJ",
      "title": "Causally Focused Convolutional Networks Through Minimal Human Guidance",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Convolutional Neural Networks (CNNs) are the state of the art in image classification mainly due to their ability to automatically extract features from the images and in turn, achieve accuracy higher than any method in history. However, the flip side is, they are correlational models which aggressively learn features that highly correlate with the labels. Such features may not be causally related to the labels as per human cognition. For example, in a subset of images, cows can be on grassland, but classifying an image as cow based on the presence of grassland is incorrect. To marginalize out the effect of all possible contextual features we need to gather a huge training dataset, which is not always possible. Moreover, this prohibits the model to justify the decision. This issue has some serious implications in certain domains such as medicine, where the amount of data can be limited but the model is expected to justify its decisions. In order to mitigate this issue, our proposal is to focus CNN to extract features that are causal from a human perspective. We propose a mechanism to accept guidance from humans in the form of activation masks to modify the learning process of CNN. The amount of additional guidance can be small and can be easily formed. Through detailed analysis, we show that this method not only improves the learning of causal features but also helps in learning efficiently with less data. We demonstrate the effectiveness of our method against multiple datasets using quantitative as well as qualitative results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=onwTC5W0XJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rimmon_Saloman_Bhosale1",
        "name": "Rimmon Saloman Bhosale",
        "name_site": null,
        "openreview_id": "~Rimmon_Saloman_Bhosale1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "rimmon/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Palakkad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "reFFte7mA0F",
      "title": "Conditional Expectation based Value Decomposition for Scalable On-Demand Ride Pooling",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Owing to the benefits for customers (lower prices), drivers (higher revenues), aggregation companies (higher revenues) and the environment (fewer vehicles), on-demand ride pooling (e.g., Uber pool, Grab Share) has become quite popular. The significant computational complexity of matching vehicles to combinations of requests has meant that traditional ride pooling approaches are myopic in that they do not consider the impact of current matches on future value for vehicles/drivers.\n\nRecently, Neural Approximate Dynamic Programming (NeurADP) has employed value decomposition with Approximate Dynamic Programming (ADP) to outperform leading approaches by considering the impact of an individual agent's (vehicle) chosen actions on the future value of that agent. However, in order to ensure scalability and facilitate city-scale ride pooling, NeurADP  completely ignores the impact of other agents actions on individual agent/vehicle value. As demonstrated in our experimental results, ignoring the impact of other agents actions on individual value can have a significant impact on the overall performance when there is increased competition among vehicles for demand. Our key contribution is a novel mechanism based on computing conditional expectations through joint conditional probabilities for capturing dependencies on other agents actions without increasing the complexity of training or decision making. We show that our new approach, Conditional Expectation based Value Decomposition (CEVD) outperforms NeurADP by up to 9.76$\\% $in terms of overall requests served, which is a significant improvement on a city wide benchmark taxi dataset. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=reFFte7mA0F",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Avinandan_Bose1",
        "name": "Avinandan Bose",
        "name_site": null,
        "openreview_id": "~Avinandan_Bose1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://avinandan22.github.io/",
        "dblp_id": "305/7490",
        "google_scholar_url": "https://scholar.google.com/citations?pli=1",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rwEv1SklKFt",
      "title": "Poisoned classifiers are not only backdoored, they are fundamentally broken",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Under a commonly-studied backdoor poisoning attack against classification models, an attacker adds a small trigger to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rwEv1SklKFt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddhant_Agarwal1",
        "name": "Siddhant Agarwal",
        "name_site": null,
        "openreview_id": "~Siddhant_Agarwal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://agarwalsiddhant10.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "siddhant-agarwal-688a31156/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "s-b95PMK4E6",
      "title": "Hierarchical Modular Framework for Long Horizon Instruction Following",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Robotic agents performing domestic chores using natural language directives re-quire to learn the complex task of navigating an environment and interacting with objects in it. To address such composite tasks, we propose a hierarchical modular approach to learn agents that navigate and manipulate objects in a  divide-and-conquer manner for the diverse nature of the entailing tasks. Specifically, our policy operates at three levels of hierarchy.  We first infer a sequence of subgoals to be executed based on language instructions by a high-level policy composition controller (PCC). We then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, we infer manipulation actions with the corresponding object masks using the appropriate interaction policy. Our hierarchical agent, named HACR (Hierarchical Approach for Compositional Reasoning), generates a human interpretable and short sequence of sub-objectives, leading to efficient interaction with an environment, and achieves the state-of-the-art performance on the challenging ALFRED benchmark.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=s-b95PMK4E6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "s5yOwPJicj",
      "title": "Carousel Memory: Rethinking the Design of Episodic Memory for Continual Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continual Learning (CL) is an emerging machine learning paradigm that aims to learn from a continuous stream of tasks without forgetting knowledge learned from the previous tasks. To avoid performance decrease caused by forgetting, prior studies exploit episodic memory (EM), which stores a subset of the past observed samples while learning from new non-i.i.d. data. Despite the promising results, since CL is often assumed to execute on mobile or IoT devices, the EM size is bounded by the small hardware memory capacity and makes it infeasible to meet the accuracy requirements for real-world applications. Specifically, all prior CL methods discard samples overflowed from the EM and can never retrieve them back for subsequent training steps, incurring loss of information that would exacerbate catastrophic forgetting. We explore a novel hierarchical EM management strategy to address the forgetting issue. In particular, in mobile and IoT devices, real-time data can be stored not just in high-speed RAMs but in internal storage devices as well, which offer significantly larger capacity than the RAMs. Based on this insight, we propose to exploit the abundant storage to preserve past experiences and alleviate the forgetting by allowing CL to efficiently migrate samples between memory and storage without being interfered by the slow access speed of the storage. We call it Carousel Memory (CarM). As CarM is complementary to existing CL methods, we conduct extensive evaluations of our method with seven popular CL methods and show that CarM significantly improves the accuracy of the methods across different settings by large margins in final average accuracy (up to 28.4%) while retaining the same training efficiency.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=s5yOwPJicj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sTECq7ZjtKX",
      "title": "OSSuM: A Gradient-Free Approach For Pruning Neural Networks At Initialization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Pruning overparameterized neural networks to obtain memory-and-compute-efficient sparse networks is an active area of research. Recent works attempt to prune neural networks at initialization to design sparse networks that can be trained efficiently. In this paper we propose One-Shot Supermasking (OSSuM), a gradient-free, compute-efficient technique to efficiently prune neurons in fully-connected networks. In theory we frame this problem as a neuron subset selection problem, wherein we prune neurons to obtain a better accuracy by optimizing on the cross-entropy loss. In our experiments we show that OSSuM can perform similar to gradient-based pruning techniques at initialization, prior to training. For example, OSSuM can achieve a test set accuracy of $82.4\\%$ on MNIST by pruning a 2-layer fully-connected neural network at initialization with just a single forward-pass over the training data. Further, we empirically demonstrate that OSSuM can be used to efficiently prune trained networks as well. We also propose various variants of OSSuM that can be used to prune deeper neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sTECq7ZjtKX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinu_Sankar_Sadasivan1",
        "name": "Vinu Sankar Sadasivan",
        "name_site": null,
        "openreview_id": "~Vinu_Sankar_Sadasivan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vinusankars.github.io/",
        "dblp_id": "244/8052",
        "google_scholar_url": "y1IKIw0AAAAJ",
        "orcid": null,
        "linkedin_url": "vinusankars/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sTECq7ZjtKX",
      "title": "OSSuM: A Gradient-Free Approach For Pruning Neural Networks At Initialization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Pruning overparameterized neural networks to obtain memory-and-compute-efficient sparse networks is an active area of research. Recent works attempt to prune neural networks at initialization to design sparse networks that can be trained efficiently. In this paper we propose One-Shot Supermasking (OSSuM), a gradient-free, compute-efficient technique to efficiently prune neurons in fully-connected networks. In theory we frame this problem as a neuron subset selection problem, wherein we prune neurons to obtain a better accuracy by optimizing on the cross-entropy loss. In our experiments we show that OSSuM can perform similar to gradient-based pruning techniques at initialization, prior to training. For example, OSSuM can achieve a test set accuracy of $82.4\\%$ on MNIST by pruning a 2-layer fully-connected neural network at initialization with just a single forward-pass over the training data. Further, we empirically demonstrate that OSSuM can be used to efficiently prune trained networks as well. We also propose various variants of OSSuM that can be used to prune deeper neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sTECq7ZjtKX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayesh_Malaviya1",
        "name": "Jayesh Malaviya",
        "name_site": null,
        "openreview_id": "~Jayesh_Malaviya1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "bD93gSwAAAAJ",
        "orcid": null,
        "linkedin_url": "jayeshmalaviya/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Gandhinagar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sTECq7ZjtKX",
      "title": "OSSuM: A Gradient-Free Approach For Pruning Neural Networks At Initialization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Pruning overparameterized neural networks to obtain memory-and-compute-efficient sparse networks is an active area of research. Recent works attempt to prune neural networks at initialization to design sparse networks that can be trained efficiently. In this paper we propose One-Shot Supermasking (OSSuM), a gradient-free, compute-efficient technique to efficiently prune neurons in fully-connected networks. In theory we frame this problem as a neuron subset selection problem, wherein we prune neurons to obtain a better accuracy by optimizing on the cross-entropy loss. In our experiments we show that OSSuM can perform similar to gradient-based pruning techniques at initialization, prior to training. For example, OSSuM can achieve a test set accuracy of $82.4\\%$ on MNIST by pruning a 2-layer fully-connected neural network at initialization with just a single forward-pass over the training data. Further, we empirically demonstrate that OSSuM can be used to efficiently prune trained networks as well. We also propose various variants of OSSuM that can be used to prune deeper neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sTECq7ZjtKX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Dasgupta1",
        "name": "Anirban Dasgupta",
        "name_site": null,
        "openreview_id": "~Anirban_Dasgupta1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/anirbandasgupta",
        "dblp_id": "54/385-1",
        "google_scholar_url": "plJC8R0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Gandhinagar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tBoSm4hUWV",
      "title": "WaveMix: Multi-Resolution Token Mixing for Images",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Even though vision transformers (ViTs) have provided state-of-the-art results on image classification, their requirements of large data, model size, and GPU usage have put them out of reach of most practitioners of computer vision. We present WaveMix as an alternative to self-attention mechanisms in ViT and convolutional neural networks to significantly reduce computational costs and memory footprint without compromising on image classification accuracy. WaveMix uses a multi-level two-dimensional discrete wavelet transform for mixing tokens and aggregating multi-resolution pixel information over long distances, which gives it the following advantages. Firstly, unlike the self-attention mechanism of ViT, WaveMix does not unroll the image. Thus, it has the right inductive bias to utilize the 2-D structure of an image, which reduces the demand for large training data. Additionally, the quadratic complexity with respect to sequence length is also eliminated. Secondly, due to its multi-resolution token-mixing, WaveMix also requires much fewer layers than a CNN does for comparable accuracy. Preliminary results from our experiments on supervised learning using CIFAR-10 dataset show that a four-layer WaveMix model can be 37% more accurate than a ViT with a comparable number of parameters, while consuming only 3% of the latter's GPU RAM and memory. This model also performs better than efficient transformers and models not based on attention, such as, FNet, and MLP Mixer. Scaling up the WaveMix model to achieve a top-1 accuracy of over 85% on CIFAR-10 could be done on a 16 GB GPU, while consuming only 6% of the GPU RAM used by the largest ViT which could fit in that GPU. Our work suggests that research on model structures that exploit the right inductive bias is far from over, and that such models can enable the training of computer vision models in settings with limited GPU resources.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tBoSm4hUWV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranav_Jeevan_P1",
        "name": "Pranav Jeevan P",
        "name_site": null,
        "openreview_id": "~Pranav_Jeevan_P1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://pranavphoenix.github.io/",
        "dblp_id": "296/3727",
        "google_scholar_url": "3GlJQ24AAAAJ",
        "orcid": "0000-0003-4110-9638",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tCx6AefvuPf",
      "title": "Node-Level Differentially Private Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph neural networks (GNNs) are a popular technique for modelling graph-structured data that compute node-level predictions via aggregation of information from the local neighborhood of each node. However, this aggregation implies increased risk of revealing sensitive information, as a node can participate in the inference for multiple nodes. This implies that standard privacy preserving machine learning techniques like differentially private stochastic gradient descent (DP-SGD) – which are designed for situations where each node/data point participate in inference of one point only – either do not apply or lead to inaccurate solutions. In this work, we formally define the problem of learning 1-layer GNNs with node-level privacy, and provide a method for the problem with a strong differential privacy guarantee. Even though each node can be involved in the inference for multiple nodes, by employing a careful sensitivity analysis and a non-trivial extension of the privacy-by-amplification technique, our method is able to provide accurate solutions with solid privacy parameters. Empirical evaluation on standard benchmarks demonstrates that our method is indeed able to learn accurate privacy preserving GNNs, while still outperforming standard non-private methods that completely ignore graph information.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tCx6AefvuPf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Sinha1",
        "name": "Aditya Sinha",
        "name_site": null,
        "openreview_id": "~Aditya_Sinha1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://adityaasinha28.github.io/",
        "dblp_id": null,
        "google_scholar_url": "5letoXIAAAAJ",
        "orcid": null,
        "linkedin_url": "adityaasinha28/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Unknown Institution (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 65,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tCx6AefvuPf",
      "title": "Node-Level Differentially Private Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph neural networks (GNNs) are a popular technique for modelling graph-structured data that compute node-level predictions via aggregation of information from the local neighborhood of each node. However, this aggregation implies increased risk of revealing sensitive information, as a node can participate in the inference for multiple nodes. This implies that standard privacy preserving machine learning techniques like differentially private stochastic gradient descent (DP-SGD) – which are designed for situations where each node/data point participate in inference of one point only – either do not apply or lead to inaccurate solutions. In this work, we formally define the problem of learning 1-layer GNNs with node-level privacy, and provide a method for the problem with a strong differential privacy guarantee. Even though each node can be involved in the inference for multiple nodes, by employing a careful sensitivity analysis and a non-trivial extension of the privacy-by-amplification technique, our method is able to provide accurate solutions with solid privacy parameters. Empirical evaluation on standard benchmarks demonstrates that our method is indeed able to learn accurate privacy preserving GNNs, while still outperforming standard non-private methods that completely ignore graph information.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tCx6AefvuPf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Aggarwal4",
        "name": "Gaurav Aggarwal",
        "name_site": null,
        "openreview_id": "~Gaurav_Aggarwal4",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "14/5218",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9XiIwDQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 65,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tlkHrUlNTiL",
      "title": "Disentangling deep neural networks with rectified linear units using duality",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Despite their success deep neural networks (DNNs) are still largely considered as black boxes. The main issue is that the linear and non-linear operations are entangled in every layer, making it hard to interpret the hidden layer outputs. In this paper, we look at DNNs with rectified linear units (ReLUs), and focus on the gating property (‘on/off’ states) of the ReLUs. We extend the recently developed dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates is characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. In this paper, we present novel results to show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. To address ‘black box’-ness, we propose a novel interpretable counterpart of DNNs with ReLUs namely deep linearly gated networks (DLGN): the pre- activations to the gates are generated by a deep linear network, and the gates are then applied as external masks to learn the weights in a different network. The DLGN is not an alternative architecture per se, but a disentanglement and an interpretable re-arrangement of the computations in a DNN with ReLUs. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the ‘primal’ linearity between the input and the pre-activations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the NPK. We compare the performance of DNN, DGN and DLGN on CIFAR-10 and CIFAR-100 to show that, the DLGN recovers more than 83.5% of the performance of state-of-the-art DNNs, i.e., while entanglement in the DNNs enable their improved performance, the ‘disentangled and interpretable’ computations in the DLGN recovers most part of the performance. This brings us to an interesting question: ‘Is DLGN a universal spectral approximator?’",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tlkHrUlNTiL",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chandra_Shekar_Lakshminarayanan2",
        "name": "Chandra Shekar Lakshminarayanan",
        "name_site": null,
        "openreview_id": "~Chandra_Shekar_Lakshminarayanan2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://iitpkd.ac.in/people/cnarayanan",
        "dblp_id": "143/7535",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wIK1fWFXvU9",
      "title": "Understanding the Interaction of Adversarial Training with Noisy Labels",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Noisy labels (NL) and adversarial examples both undermine trained models, but interestingly they have hitherto been studied independently. A recent adversarial training (AT) study showed that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Given that natural data are clean, this measure reveals an intrinsic geometric property---how far a point is from its nearest class boundary. Based on this breakthrough, in this paper, we figure out how AT would interact with NL. Firstly, we find if a point is too close to its noisy-class boundary (e.g., one step is enough to attack it), this point is likely to be mislabeled, which suggests to adopt the number of PGD steps as a new criterion for sample selection to correct NL. Secondly, we confirm that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction. Hence, AT with NL is helpful for improving even the natural accuracy, which again illustrates the superiority of AT as a general-purpose robust learning criterion.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=wIK1fWFXvU9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 32,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xmrtP-ADzk",
      "title": "Self-Supervised Learning for Binary Networks by Joint Classifier Training",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Despite the great success of self-supervised learning with large floating point networks, such networks are not readily deployable to edge devices.\nTo accelerate deployment of models to edge devices for various downstream tasks by unsupervised representation learning, we propose a self-supervised learning method for binary networks.\nIn particular, we propose to use a randomly initialized classifier attached to a pretrained floating point feature extractor as targets and jointly train it with a binary network.\nFor better training of the binary network, we propose a feature similarity loss, a dynamic balancing scheme of loss terms, and modified multi-stage training.\nWe call our method as BSSL.\nOur empirical validations show that BSSL outperforms self-supervised learning baselines for binary networks in various downstream tasks and outperforms supervised pretraining in certain tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=xmrtP-ADzk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yjxVspo7gXt",
      "title": "Scaling Fair Learning to Hundreds of Intersectional Groups",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Bias mitigation algorithms aim to reduce the performance disparity between different protected groups. Existing techniques focus on settings where there is a small number of protected groups arising from a single protected attribute, such as skin color, gender or age. In real-world applications, however, there are multiple protected attributes yielding a large number of intersectional protected groups. These intersectional groups are particularly prone to severe underrepresentation in datasets. We conduct the first thorough empirical analysis of how existing bias mitigation methods scale to this setting, using large-scale datasets including the ImageNet People Subtree and CelebA. We find that as more protected attributes are introduced to a task, it becomes more important to leverage the protected attribute labels during training to promote fairness. We also find that the use of knowledge distillation, in conjunction with group-specific models, can help scale existing fair learning methods to hundreds of protected intersectional groups and reduce bias. We show on ImageNet's People Subtree that combining these insights can further reduce the bias amplification of fair learning algorithms by 15% ---a surprising reduction given that the dataset has 196 protected groups but fewer than 10% of the training dataset has protected attribute labels.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yjxVspo7gXt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 5,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Princeton University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    }
  ]
}