{
  "conference": "ICLR 2021",
  "focus_country": "India",
  "total_papers": 82,
  "generated_at": "2025-07-06T10:37:20.846744",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "mLcmdlEUxy-",
      "title": "Recurrent Independent Mechanisms",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "We explore the hypothesis that learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes that only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and compete with each other so they are updated only at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for remarkably improved generalization on tasks where some factors of variation differ systematically between training and evaluation.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/3224",
      "pdf_url": "https://openreview.net/pdf?id=mLcmdlEUxy-",
      "github_url": "[![Papers with Code](/images/pwc_icon.svg) 3 community implementations](https://paperswithcode.com/paper/?openreview=mLcmdlEUxy-)",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 7.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 389,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "193sEnKY1ij",
      "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-$k$ predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/3041",
      "pdf_url": "https://openreview.net/pdf?id=193sEnKY1ij",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shyamgopal_Karthik1",
        "name": "Shyamgopal Karthik",
        "name_site": "Shyamgopal Karthik, Ameya Prabhu, Puneet Dokania, Vineet Gandhi",
        "openreview_id": "~Shyamgopal_Karthik1",
        "position": 1,
        "gender": null,
        "homepage_url": "https://sgk98.github.io/",
        "dblp_id": "251/8983",
        "google_scholar_url": "MofhemMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 27,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATp1nW2FuZL",
      "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any \"one\" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks,  demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2941",
      "pdf_url": "https://openreview.net/pdf?id=ATp1nW2FuZL",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yatin_Nandwani1",
        "name": "Yatin Nandwani",
        "name_site": "Yatin Nandwani, Deepanshu Jindal, Mausam ., Parag Singla",
        "openreview_id": "~Yatin_Nandwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~yatin",
        "dblp_id": "255/7046",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "yatin-nandwani-0804ba9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VVdmjgu7pKM",
      "title": "Factorizing Declarative and Procedural Knowledge in Structured, Dynamical Environments",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Modeling a structured, dynamic environment like a video game requires keeping track of the objects and their states (declarative knowledge) as well as predicting how objects behave (procedural knowledge). Black-box models with a monolithic hidden state often fail to apply procedural knowledge consistently and uniformly, i.e., they lack systematicity. For example, in a video game, correct prediction of one enemy's trajectory does not ensure correct prediction of another's. We address this issue via an architecture that factorizes declarative and procedural knowledge and that imposes modularity within each form of knowledge. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources called schemata that prescribe state updates. To use a video game as an illustration, two enemies of the same type will share schemata but will have separate object files to encode their distinct state (e.g., health, position). We propose to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type, including a challenging intuitive physics benchmark.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2917",
      "pdf_url": "https://openreview.net/pdf?id=VVdmjgu7pKM",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 16,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b7g3_ZMHnT0",
      "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also \"explain\" which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG's semantics and structure. Our findings raise doubts about KG-augmented models' ability to reason about KG information and give sensible explanations.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2769",
      "pdf_url": "https://openreview.net/pdf?id=b7g3_ZMHnT0",
      "github_url": "[![github](/images/github_icon.svg) INK-USC/deceive-KG-models](https://github.com/INK-USC/deceive-KG-models)",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "mrigankraman1611@gmail.com",
        "name": "Mrigank Raman",
        "name_site": "Mrigank Raman, Aaron Chan, Siddhant Agarwal, PeiFeng Wang, Hansen Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren",
        "openreview_id": "mrigankraman1611@gmail.com",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 19,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "d8Q1mt2Ghw",
      "title": "Emergent Road Rules In Multi-Agent Driving Environments",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific \"road rules\" that human drivers have agreed to follow. \"Road rules\" include rules that drivers are required to follow by law – such as the requirement that vehicles stop at red lights – as well as more subtle social rules – such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that – instead of hard-coding road rules into self-driving algorithms – a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow.  We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents’ spatial density.  We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2663",
      "pdf_url": "https://openreview.net/pdf?id=d8Q1mt2Ghw",
      "github_url": "[![github](/images/github_icon.svg) fidler-lab/social-driving](https://github.com/fidler-lab/social-driving)",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Avik_Pal1",
        "name": "Avik Pal",
        "name_site": "Avik Pal, Jonah Philion, Yuan-Hong Liao, Sanja Fidler",
        "openreview_id": "~Avik_Pal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://avik-pal.github.io",
        "dblp_id": "230/3687",
        "google_scholar_url": "tAiqiEMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 2.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xCxXwTzx4L1",
      "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Structured pruning methods are among the effective strategies for extracting small resource-efficient convolutional neural networks from their dense counterparts with minimal loss in accuracy. However, most existing methods still suffer from one or more limitations, that include 1) the need for training the dense model from scratch with pruning-related parameters embedded in the architecture, 2) requiring model-specific hyperparameter settings, 3) inability to include budget-related constraint in the training process, and 4) instability under scenarios of extreme pruning. In this paper, we present ChipNet, a deterministic pruning strategy that employs continuous Heaviside function and a novel crispness loss to identify a highly sparse network out of an existing dense network. Our choice of continuous Heaviside function is inspired by the field of design optimization, where the material distribution task is posed as a continuous optimization problem, but only discrete values (0 or 1) are practically feasible and expected as final outcomes. Our approach's flexible design facilitates its use with different choices of budget constraints while maintaining stability for very low target budgets. Experimental results show that ChipNet outperforms state-of-the-art structured pruning methods by remarkable margins of up to 16.1% in terms of accuracy. Further, we show that the masks obtained with ChipNet are transferable across datasets. For certain cases, it was observed that masks transferred from a model trained on feature-rich teacher dataset provide better performance on the student dataset than those obtained by directly pruning on the student data itself.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2768",
      "pdf_url": "https://openreview.net/pdf?id=xCxXwTzx4L1",
      "github_url": "[![github](/images/github_icon.svg) transmuteAI/ChipNet](https://github.com/transmuteAI/ChipNet)",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_Tiwari1",
        "name": "Rishabh Tiwari",
        "name_site": "Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, Deepak Gupta",
        "openreview_id": "~Rishabh_Tiwari1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "261/3179",
        "google_scholar_url": "iJuoc4sAAAAJ",
        "orcid": null,
        "linkedin_url": "rishabh-tiwari16/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Dhanbad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 42,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "193sEnKY1ij",
      "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-$k$ predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/3041",
      "pdf_url": "https://openreview.net/pdf?id=193sEnKY1ij",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineet_Gandhi2",
        "name": "Vineet Gandhi",
        "name_site": null,
        "openreview_id": "~Vineet_Gandhi1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://faculty.iiit.ac.in/~vgandhi/",
        "dblp_id": "117/2021",
        "google_scholar_url": "https://scholar.google.fr/citations?user=PVlBz8oAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 27,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATp1nW2FuZL",
      "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any \"one\" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks,  demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2941",
      "pdf_url": "https://openreview.net/pdf?id=ATp1nW2FuZL",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "O3bqkf_Puys",
      "title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/3101",
      "pdf_url": "https://openreview.net/pdf?id=O3bqkf_Puys",
      "github_url": "[![github](/images/github_icon.svg) hehefan/Point-Spatio-Temporal-Convolution](https://github.com/hehefan/Point-Spatio-Temporal-Convolution)",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 155,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ldxlzGYWDmW",
      "title": "Effective Abstract Reasoning with Dual-Contrast Network",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "As a step towards improving the abstract reasoning capability of machines, we aim to solve Raven’s Progressive Matrices (RPM) with neural networks, since solving RPM puzzles is highly correlated with human intelligence. Unlike previous methods that use auxiliary annotations or assume hidden rules to produce appropriate feature representation, we only use the ground truth answer of each question for model learning,  aiming for an intelligent agent to have a strong learning capability with a small amount of supervision.  Based on the RPM problem formulation,  the correct answer filled into the missing entry of the third row/column has  to  best  satisfy  the  same  rules  shared  between  the  first  two  rows/columns.Thus  we  design  a  simple  yet  effective  Dual-Contrast  Network  (DCNet)  to  exploit the inherent structure of RPM puzzles.  Specifically, a rule contrast module is  designed  to  compare  the  latent  rules  between  the  filled  row/column  and  the first two rows/columns; a choice contrast module is designed to increase the relative differences between candidate choices.  Experimental results on the RAVEN and  PGM  datasets  show  that  DCNet  outperforms  the  state-of-the-art  methods by a large margin of 5.77%.   Further experiments on few training samples and model generalization also show the effectiveness of DCNet.  Code is available at https://github.com/visiontao/dcnet.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2833",
      "pdf_url": "https://openreview.net/pdf?id=ldxlzGYWDmW",
      "github_url": "[![github](/images/github_icon.svg) visiontao/dcnet](https://github.com/visiontao/dcnet)",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 42,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "04LZCAxMSco",
      "title": "Learning a Latent Simplex in Input Sparsity Time",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "We consider the problem of learning a latent $k$-vertex simplex $K\\in\\mathbb{R}^d$, given $\\mathbf{A}\\in\\mathbb{R}^{d\\times n}$, which can be viewed as $n$ data points that are formed by randomly perturbing some latent points in $K$, possibly beyond $K$. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a $k$-vertex latent simplex in time roughly $O(k\\cdot\\text{nnz}(\\mathbf{A}))$, where $\\text{nnz}(\\mathbf{A})$ is the number of non-zeros in $\\mathbf{A}$. We show that the dependence on $k$ in the running time is unnecessary given a natural assumption about the mass of the top $k$ singular values of $\\mathbf{A}$, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. \n\nWe obtain a spectral low-rank approximation to $\\mathbf{A}$ in input-sparsity time and show that the column space thus obtained has small $\\sin\\Theta$ (angular) distance to the right top-$k$ singular space of $\\mathbf{A}$. Our algorithm then selects $k$ points in the low-rank  subspace with the largest inner product (in absolute value) with $k$ carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the $\\Theta(k\\cdot\\text{nnz}(\\mathbf{A}))$ running time.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2553",
      "pdf_url": "https://openreview.net/pdf?id=04LZCAxMSco",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 5.625,
      "reviews": {
        "rating_mean": 8.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mSAKhLYLSsl",
      "title": "Dataset Condensation with Gradient Matching",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2784",
      "pdf_url": "https://openreview.net/pdf?id=mSAKhLYLSsl",
      "github_url": "[![github](/images/github_icon.svg) VICO-UoE/DatasetCondensation](https://github.com/VICO-UoE/DatasetCondensation) + [![Papers with Code](/images/pwc_icon.svg) 3 community implementations](https://paperswithcode.com/paper/?openreview=mSAKhLYLSsl)",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Konda_Reddy_Mopuri3",
        "name": "Konda Reddy Mopuri",
        "name_site": null,
        "openreview_id": "~Konda_Reddy_Mopuri3",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://krmopuri.github.io/",
        "dblp_id": "162/0085",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-8894-7212",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Guwahati (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 5.0,
      "reviews": {
        "rating_mean": 8.333333333333334,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 599,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5l9zj5G7vDY",
      "title": "Spatially Structured Recurrent Modules",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalise well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. To this end, we model the dynamical system as a collection of autonomous but sparsely interacting sub-systems that interact according to a learned topology which is informed by the spatial structure of the underlying system. This gives rise to a class of models that are well suited for capturing the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modelling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalisation to novel tasks without additional training than strong baselines that perform equally well or better on the training distribution. ",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/3333",
      "pdf_url": "https://openreview.net/pdf?id=5l9zj5G7vDY",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 4.2857142857142865,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.5,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "b7g3_ZMHnT0",
      "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also \"explain\" which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG's semantics and structure. Our findings raise doubts about KG-augmented models' ability to reason about KG information and give sensible explanations.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2769",
      "pdf_url": "https://openreview.net/pdf?id=b7g3_ZMHnT0",
      "github_url": "[![github](/images/github_icon.svg) INK-USC/deceive-KG-models](https://github.com/INK-USC/deceive-KG-models)",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddhant_Agarwal1",
        "name": "Siddhant Agarwal",
        "name_site": null,
        "openreview_id": "~Siddhant_Agarwal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://agarwalsiddhant10.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "siddhant-agarwal-688a31156/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tsinghua University (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 3.888888888888889,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 19,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SK7A5pdrgov",
      "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we proposeCausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows  for  fine-grained  control  over  how  similar  different  tasks  (or  task  distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level,  targeting a specific form of generalization (e.g.,  only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2579",
      "pdf_url": "https://openreview.net/pdf?id=SK7A5pdrgov",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Max Planck Institute for Intelligent Systems (Germany)",
        "countries": [
          "Germany"
        ],
        "country_codes": [
          "DE"
        ]
      },
      "sort_score": 3.5714285714285716,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.479019945774904,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 161,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATp1nW2FuZL",
      "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any \"one\" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks,  demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines.\n",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2941",
      "pdf_url": "https://openreview.net/pdf?id=ATp1nW2FuZL",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "deepanshujindal.99@gmail.com",
        "name": "Deepanshu Jindal",
        "name_site": null,
        "openreview_id": "deepanshujindal.99@gmail.com",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xCxXwTzx4L1",
      "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Structured pruning methods are among the effective strategies for extracting small resource-efficient convolutional neural networks from their dense counterparts with minimal loss in accuracy. However, most existing methods still suffer from one or more limitations, that include 1) the need for training the dense model from scratch with pruning-related parameters embedded in the architecture, 2) requiring model-specific hyperparameter settings, 3) inability to include budget-related constraint in the training process, and 4) instability under scenarios of extreme pruning. In this paper, we present ChipNet, a deterministic pruning strategy that employs continuous Heaviside function and a novel crispness loss to identify a highly sparse network out of an existing dense network. Our choice of continuous Heaviside function is inspired by the field of design optimization, where the material distribution task is posed as a continuous optimization problem, but only discrete values (0 or 1) are practically feasible and expected as final outcomes. Our approach's flexible design facilitates its use with different choices of budget constraints while maintaining stability for very low target budgets. Experimental results show that ChipNet outperforms state-of-the-art structured pruning methods by remarkable margins of up to 16.1% in terms of accuracy. Further, we show that the masks obtained with ChipNet are transferable across datasets. For certain cases, it was observed that masks transferred from a model trained on feature-rich teacher dataset provide better performance on the student dataset than those obtained by directly pruning on the student data itself.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2768",
      "pdf_url": "https://openreview.net/pdf?id=xCxXwTzx4L1",
      "github_url": "[![github](/images/github_icon.svg) transmuteAI/ChipNet](https://github.com/transmuteAI/ChipNet)",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Udbhav_Bamba1",
        "name": "Udbhav Bamba",
        "name_site": null,
        "openreview_id": "~Udbhav_Bamba1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://ubamba98.github.com",
        "dblp_id": "261/3097",
        "google_scholar_url": "PgnnH78AAAAJ",
        "orcid": null,
        "linkedin_url": "ubamba98",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Dhanbad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 42,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "-6vS_4Kfz0",
      "title": "Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "For deep neural network accelerators, memory movement is both energetically expensive and can bound computation. Therefore, optimal mapping of tensors to memory hierarchies is critical to performance. The growing complexity of neural networks calls for automated memory mapping instead of manual heuristic approaches; yet the search space of neural network computational graphs have previously been prohibitively large. We introduce Evolutionary Graph Reinforcement Learning (EGRL), a method designed for large search spaces, that combines graph neural networks, reinforcement learning, and evolutionary search. A set of fast, stateless policies guide the evolutionary search to improve its sample-efficiency. We train and validate our approach directly on the Intel NNP-I chip for inference. EGRL outperforms policy-gradient, evolutionary search and dynamic programming baselines on BERT, ResNet-101 and ResNet-50. We additionally achieve 28-78% speed-up compared to the native NNP-I compiler on all three workloads.  ",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2625",
      "pdf_url": "https://openreview.net/pdf?id=-6vS_4Kfz0",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Lc28QAB4ypz",
      "title": "Fast And Slow Learning Of Recurrent Independent Mechanisms",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic way to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the \\textit{selected} modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of  modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the  modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input.  We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2822",
      "pdf_url": "https://openreview.net/pdf?id=Lc28QAB4ypz",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 55,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UwGY2qjqoLD",
      "title": "Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In the present work we study classifiers' decision boundaries via Brownian motion processes in ambient data space and associated probabilistic techniques. Intuitively, our ideas correspond to placing a heat source at the decision boundary and observing how effectively the sample points warm up. We are largely motivated by the search for a soft measure that sheds further light on the decision boundary's geometry. En route, we  bridge aspects of potential theory and geometric analysis (Maz'ya 2011, Grigor'Yan and Saloff-Coste 2002) with active fields of ML research such as adversarial examples and generalization bounds. First, we focus on the geometric behavior of decision boundaries in the light of adversarial attack/defense mechanisms. Experimentally, we observe a certain capacitory trend over different adversarial defense strategies: decision boundaries locally become flatter as measured by isoperimetric inequalities (Ford et al 2019); however, our more sensitive heat-diffusion metrics  extend this analysis and further reveal that some non-trivial geometry invisible to plain distance-based methods is still preserved. Intuitively, we provide evidence that the decision boundaries nevertheless retain many persistent \"wiggly and fuzzy\" regions on a finer scale.\nSecond, we show how Brownian hitting probabilities translate to soft generalization bounds which are in turn connected to compression and noise stability (Arora et al 2018), and these bounds are significantly stronger if the decision boundary has controlled geometric features.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2788",
      "pdf_url": "https://openreview.net/pdf?id=UwGY2qjqoLD",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Lukas_Franken1",
        "name": "Lukas Franken",
        "name_site": null,
        "openreview_id": "~Lukas_Franken1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://github.com/LukasFrankenQ",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iAX0l6Cz8ub",
      "title": "Geometry-aware Instance-reweighted Adversarial Training",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2758",
      "pdf_url": "https://openreview.net/pdf?id=iAX0l6Cz8ub",
      "github_url": "[![Papers with Code](/images/pwc_icon.svg) 2 community implementations](https://paperswithcode.com/paper/?openreview=iAX0l6Cz8ub)",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 1.9999999999999996,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 339,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xCxXwTzx4L1",
      "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Structured pruning methods are among the effective strategies for extracting small resource-efficient convolutional neural networks from their dense counterparts with minimal loss in accuracy. However, most existing methods still suffer from one or more limitations, that include 1) the need for training the dense model from scratch with pruning-related parameters embedded in the architecture, 2) requiring model-specific hyperparameter settings, 3) inability to include budget-related constraint in the training process, and 4) instability under scenarios of extreme pruning. In this paper, we present ChipNet, a deterministic pruning strategy that employs continuous Heaviside function and a novel crispness loss to identify a highly sparse network out of an existing dense network. Our choice of continuous Heaviside function is inspired by the field of design optimization, where the material distribution task is posed as a continuous optimization problem, but only discrete values (0 or 1) are practically feasible and expected as final outcomes. Our approach's flexible design facilitates its use with different choices of budget constraints while maintaining stability for very low target budgets. Experimental results show that ChipNet outperforms state-of-the-art structured pruning methods by remarkable margins of up to 16.1% in terms of accuracy. Further, we show that the masks obtained with ChipNet are transferable across datasets. For certain cases, it was observed that masks transferred from a model trained on feature-rich teacher dataset provide better performance on the student dataset than those obtained by directly pruning on the student data itself.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2021/poster/2768",
      "pdf_url": "https://openreview.net/pdf?id=xCxXwTzx4L1",
      "github_url": "[![github](/images/github_icon.svg) transmuteAI/ChipNet](https://github.com/transmuteAI/ChipNet)",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Amsterdam (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 42,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1TIrbngpW0x",
      "title": "Transformers with Competitive Ensembles of Independent Mechanisms",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated.  This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed.  For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically.  In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation.  This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.  To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention.  Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent.  We study TIM on a large scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.  ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1TIrbngpW0x",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1ibNKMp8SKc",
      "title": "On Disentangled Representations Learned From Correlated Data",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Despite impressive progress in the last decade, it still remains an open challenge to build models that generalize well across multiple tasks and datasets. One path to achieve this is to learn meaningful and compact representations, in which different semantic aspects of data are structurally disentangled. The focus of disentanglement approaches has been on separating independent factors of variation despite the fact that real-world observations are often not structured into meaningful independent causal variables. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of most prominent methods and disentanglement scores on correlated data in a large scale empirical study (including 4260 models). We show that systematically induced correlations in the dataset are being learned and reflected in the latent representations, while widely used disentanglement scores fall short of capturing these latent correlations. Finally, we demonstrate how to disentangle these latent correlations using weak supervision, even if we constrain this supervision to be causally plausible. Our results thus support the argument to learn independent mechanisms rather than independent factors of variations.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1ibNKMp8SKc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 1.699673171197595,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 142,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1sJWR4y1lG",
      "title": "Deep Learning Is Composite Kernel Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works have connected deep learning and kernel methods. In this paper, we show that architectural choices such as convolutional layers with pooling, skip connections, make deep learning a composite kernel learning method, where the kernel is a (architecture dependent) composition of base kernels: even before training, standard deep networks have in-built structural properties that ensure their success. In particular, we build on the recently developed `neural path' framework that characterises the role of gates/masks in fully connected deep networks with ReLU activations. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1sJWR4y1lG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chandra_Shekar_Lakshminarayanan2",
        "name": "Chandra Shekar Lakshminarayanan",
        "name_site": null,
        "openreview_id": "~Chandra_Shekar_Lakshminarayanan2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://iitpkd.ac.in/people/cnarayanan",
        "dblp_id": "143/7535",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 2.25,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "4CxsUBDQJqv",
      "title": "Learning Intrinsic Symbolic Rewards in Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning effective policies for sparse objectives is a key challenge in Deep Reinforcement Learning (RL). A common approach is to design task-related dense rewards to improve task learnability. While such rewards are easily interpreted, they rely on heuristics and domain expertise. Alternate approaches that train neural networks to discover dense surrogate rewards avoid heuristics, but are high-dimensional, black-box solutions offering little interpretability. In this paper, we present a method that discovers dense rewards in the form of low-dimensional symbolic trees - thus making them more tractable for analysis. The trees use simple functional operators to map an agent's observations to a scalar reward, which then supervises the policy gradient learning of a neural network policy. We test our method on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygames environments. We show that the discovered dense rewards are an effective signal for an RL policy to solve the benchmark tasks. Notably, we significantly outperform a widely used, contemporary neural-network based reward-discovery algorithm in all environments considered.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=4CxsUBDQJqv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.666666666666667,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8CjVaaSSVxg",
      "title": "Learning Predictive Communication by Imagination in Networked System Control",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Dealing with multi-agent control in networked systems is one of the biggest challenges in Reinforcement Learning (RL) and limited success has been presented compared to recent deep reinforcement learning in single-agent domain. However, obstacles remain in addressing the delayed global information where each agent learns a decentralized control policy based on local observations and messages from connected neighbors. This paper first considers delayed global information sharing by combining the delayed global information and latent imagination of farsighted states in differentiable communication. Our model allows an agent to imagine its future states and communicate that with its neighbors. The predictive message sent to the connected neighbors reduces the delay in global information.  On the tasks of networked multi-agent traffic control, experimental results show that our model helps stabilize the training of each local agent and outperforms existing algorithms for networked system control.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=8CjVaaSSVxg",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Eindhoven University of Technology (Netherlands)",
        "countries": [
          "Netherlands"
        ],
        "country_codes": [
          "NL"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "98fWAc-sFkv",
      "title": "A Unified Bayesian Framework for Discriminative and Generative Continual Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop on the previous tasks. Two notable directions among the recent advances in continual learning with neural networks are (1) variational Bayes based regularization by learning priors from previous tasks, and, (2) learning the structure of deep networks to adapt to new tasks. So far, these two approaches have been orthogonal. We present a novel Bayesian framework for continual learning based on learning the structure of deep neural networks, addressing the shortcomings of both these approaches. The proposed framework learns the deep structure for each task by learning which weights to be used, and supports inter-task transfer through the overlapping of different sparse subsets of weights learned by different tasks. An appealing aspect of our proposed continual learning framework is that it is applicable to both discriminative (supervised) and generative (unsupervised) settings. Experimental results on supervised and unsupervised benchmarks shows that our model performs comparably or better than recent advances in continual learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=98fWAc-sFkv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "abhi.kumar.chaudhary@gmail.com",
        "name": "Abhishek Kumar",
        "name_site": null,
        "openreview_id": "abhi.kumar.chaudhary@gmail.com",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.479019945774904,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "98fWAc-sFkv",
      "title": "A Unified Bayesian Framework for Discriminative and Generative Continual Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop on the previous tasks. Two notable directions among the recent advances in continual learning with neural networks are (1) variational Bayes based regularization by learning priors from previous tasks, and, (2) learning the structure of deep networks to adapt to new tasks. So far, these two approaches have been orthogonal. We present a novel Bayesian framework for continual learning based on learning the structure of deep neural networks, addressing the shortcomings of both these approaches. The proposed framework learns the deep structure for each task by learning which weights to be used, and supports inter-task transfer through the overlapping of different sparse subsets of weights learned by different tasks. An appealing aspect of our proposed continual learning framework is that it is applicable to both discriminative (supervised) and generative (unsupervised) settings. Experimental results on supervised and unsupervised benchmarks shows that our model performs comparably or better than recent advances in continual learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=98fWAc-sFkv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Rai1",
        "name": "Piyush Rai",
        "name_site": null,
        "openreview_id": "~Piyush_Rai1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitk.ac.in/users/piyush/",
        "dblp_id": "02/525",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=D50grEgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.479019945774904,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ANednkwrr8s",
      "title": "Indirect Supervision to Mitigate Perturbations",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Vulnerability of state-of-the-art computer vision models to image perturbations has drawn considerable attention recently. Often these perturbations are imperceptible to humans because they target the perception of deep neural networks (DNNs) employed in the corresponding computer vision task. Recent studies have revealed that DNNs, which are unable to handle targeted perturbation often fail to handle untargeted perturbations as well such as Gaussian noise. Various techniques in past have been explored to mitigate both these types of perturbations ranging from classical preprocessing to current supervised and self-supervised deep discriminative and generative models. However, a common challenge with most of these techniques is that they approach the problem from a quality enhancement point of view, which is primarily driven by human perception. In addition, the supervised models require a large volume of gold standard unperturbed data, whereas others fail to take into account the feedback of the targeted downstream DNN. We propose to model this problem in indirect supervision framework, where we assume that the gold standard data is missing, however, a variable dependent on it is available and the dependency of the observed variable is stated by the considered downstream DNN. The proposed method maintains the advantages of supervised models while relaxing the requirement of gold standard unperturbed data. To prove its utility, we conduct several experiments on various network architectures for downstream tasks of classification and medical image segmentation. We used MNIST, CIFAR-10-C and ISIC skin lesion dataset in our experiments. In all the experiments, a considerable restoration in the performance of the considered downstream model is observed.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ANednkwrr8s",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mayank_Kumar_Kundalwal1",
        "name": "Mayank Kumar Kundalwal",
        "name_site": null,
        "openreview_id": "~Mayank_Kumar_Kundalwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "V017_oQAAAAJ",
        "orcid": null,
        "linkedin_url": "hostingshades/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ANednkwrr8s",
      "title": "Indirect Supervision to Mitigate Perturbations",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Vulnerability of state-of-the-art computer vision models to image perturbations has drawn considerable attention recently. Often these perturbations are imperceptible to humans because they target the perception of deep neural networks (DNNs) employed in the corresponding computer vision task. Recent studies have revealed that DNNs, which are unable to handle targeted perturbation often fail to handle untargeted perturbations as well such as Gaussian noise. Various techniques in past have been explored to mitigate both these types of perturbations ranging from classical preprocessing to current supervised and self-supervised deep discriminative and generative models. However, a common challenge with most of these techniques is that they approach the problem from a quality enhancement point of view, which is primarily driven by human perception. In addition, the supervised models require a large volume of gold standard unperturbed data, whereas others fail to take into account the feedback of the targeted downstream DNN. We propose to model this problem in indirect supervision framework, where we assume that the gold standard data is missing, however, a variable dependent on it is available and the dependency of the observed variable is stated by the considered downstream DNN. The proposed method maintains the advantages of supervised models while relaxing the requirement of gold standard unperturbed data. To prove its utility, we conduct several experiments on various network architectures for downstream tasks of classification and medical image segmentation. We used MNIST, CIFAR-10-C and ISIC skin lesion dataset in our experiments. In all the experiments, a considerable restoration in the performance of the considered downstream model is observed.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ANednkwrr8s",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "singh.63@iitj.ac.in",
        "name": "Azad Singh",
        "name_site": null,
        "openreview_id": "singh.63@iitj.ac.in",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BnokSKnhC7F",
      "title": "Maximum Reward Formulation In Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Reinforcement learning (RL) algorithms typically deal with maximizing the expected cumulative return (discounted or undiscounted, finite or infinite horizon). However, several crucial applications in the real world, such as drug discovery, do not fit within this framework because an RL agent only needs to identify states (molecules) that achieve the highest reward within a trajectory and does not need to optimize for the expected cumulative return. In this work, we formulate an objective function to maximize the expected maximum reward along a trajectory, derive a novel functional form of the Bellman equation, introduce the corresponding Bellman operators, and provide a proof of convergence. Using this formulation, we achieve state-of-the-art results on the task of molecule generation that mimics a real-world drug discovery pipeline.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=BnokSKnhC7F",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~SaiKrishna_Gottipati1",
        "name": "SaiKrishna Gottipati",
        "name_site": null,
        "openreview_id": "~SaiKrishna_Gottipati1",
        "position": 1,
        "gender": null,
        "homepage_url": "https://saikrishna-1996.github.io",
        "dblp_id": null,
        "google_scholar_url": "9syQKRIAAAAJ",
        "orcid": null,
        "linkedin_url": "saikrishna-1996/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "99andBeyond (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.6,
        "rating_std": 1.0198039027185568,
        "confidence_mean": 3.2,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4PK0rg2eP",
      "title": "Parameter-Efficient Transfer Learning with Diff Pruning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While task-specific finetuning of deep networks pretrained with self-supervision has led to significant empirical advances in NLP, their large size makes the standard finetuning approach difficult to apply to multi-task, memory-constrained settings, as storing the full model parameters for each task become prohibitively expensive. We propose $\\textit{diff pruning}$ as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a task-specific diff vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the $L_0$-norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5$\\%$ of the pretrained model's parameters per task.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4PK0rg2eP",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.479019945774904,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 459,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GEpTemgn7cq",
      "title": "Dependency Structure Discovery from Interventions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is applicable even in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=GEpTemgn7cq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Max Planck Institute for Intelligent Systems (Germany)",
        "countries": [
          "Germany"
        ],
        "country_codes": [
          "DE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "I3zV6igAT9",
      "title": "Quantile Regularization : Towards Implicit Calibration of Regression Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works have shown that most deep learning models are often poorly  calibrated, i.e., they may produce overconfident\npredictions that are wrong, implying that their uncertainty estimates are unreliable. While a  number of approaches have been proposed recently to calibrate classification models, relatively little work exists on calibrating regression models. Isotonic Regression has recently been advocated for regression calibration. We provide a detailed formal analysis of the \\emph{side-effects} of Isotonic Regression when used for regression  calibration. To address this, we  recast quantile calibration as entropy estimation, and leverage this idea to construct a novel quantile regularizer, which can be used in any optimization based probabilisitc regression models. Unlike most of the existing approaches for calibrating regression models, which are based on \\emph{post-hoc} processing of the model's output, and require an additional dataset, our method is trainable in an end-to-end fashion, without requiring an additional dataset. We provide empirical results demonstrating that our approach improves  calibration for regression models trained on diverse architectures that  provide uncertainty estimates, such as Dropout VI, Deep Ensembles",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=I3zV6igAT9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saiteja_Utpala1",
        "name": "Saiteja Utpala",
        "name_site": null,
        "openreview_id": "~Saiteja_Utpala1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "saiteja-utpala/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "I3zV6igAT9",
      "title": "Quantile Regularization : Towards Implicit Calibration of Regression Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works have shown that most deep learning models are often poorly  calibrated, i.e., they may produce overconfident\npredictions that are wrong, implying that their uncertainty estimates are unreliable. While a  number of approaches have been proposed recently to calibrate classification models, relatively little work exists on calibrating regression models. Isotonic Regression has recently been advocated for regression calibration. We provide a detailed formal analysis of the \\emph{side-effects} of Isotonic Regression when used for regression  calibration. To address this, we  recast quantile calibration as entropy estimation, and leverage this idea to construct a novel quantile regularizer, which can be used in any optimization based probabilisitc regression models. Unlike most of the existing approaches for calibrating regression models, which are based on \\emph{post-hoc} processing of the model's output, and require an additional dataset, our method is trainable in an end-to-end fashion, without requiring an additional dataset. We provide empirical results demonstrating that our approach improves  calibration for regression models trained on diverse architectures that  provide uncertainty estimates, such as Dropout VI, Deep Ensembles",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=I3zV6igAT9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Rai1",
        "name": "Piyush Rai",
        "name_site": null,
        "openreview_id": "~Piyush_Rai1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://cse.iitk.ac.in/users/piyush/",
        "dblp_id": "02/525",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=D50grEgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "I6QHpMdZD5k",
      "title": "Learning to Solve Nonlinear Partial Differential Equation Systems To Accelerate MOSFET Simulation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Semiconductor device simulation uses numerical analysis, where a set of coupled nonlinear partial differential equations is solved with the iterative Newton-Raphson method. Since an appropriate initial guess to start the Newton-Raphson method is not available, a solution of practical importance with desired boundary conditions cannot be trivially achieved. Instead, several solutions with intermediate boundary conditions should be calculated to address the nonlinearity and introducing intermediate boundary conditions significantly increases the computation time. In order to accelerate the semiconductor device simulation, we propose to use a neural network to learn an approximate solution for desired boundary conditions. With an initial solution sufficiently close to the final one by a trained neural network, computational cost to calculate several unnecessary solutions is significantly reduced. Specifically, a convolutional neural network for MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor), the most widely used semiconductor device, are trained in a supervised manner to compute the initial solution. Particularly, we propose to consider device grids with varying size and spacing and derive a compact expression of the solution based upon the electrostatic potential. We empirically show that the proposed method accelerates the simulation by more than 12 times. Results from the local linear regression and a fully-connected network are compared and extension to a complex two-dimensional domain is sketched.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=I6QHpMdZD5k",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.0198039027185568,
        "confidence_mean": 3.8,
        "confidence_std": 0.9797958971132712,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Iuq6u10sCdl",
      "title": "$Graph Embedding via Topology and Functional Analysis$",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Graphs have been ubiquitous in Machine Learning due to their versatile nature in modelling real world situations .Graph embedding is an important precursor to using graphs in Machine Learning , and much of performance of algorithms developed later depends heavily on this. However very little theoretical work exists in this area ,  resulting in the proliferation of several benchmarks without any mathematical validation , which is detrimental .In this paper we present an analysis of deterministic graph embedding in general , using tools from Functional Analysis and Topology . We prove several important results pertaining to graph embedding  which may have practical importance .One limitation of our work in it's present form is it's applicable to deterministic embedding approaches only, although we strongly hope to extend it to random graph embedding methods as well in future.We sincerely hope that this work will be beneficial  to researchers working in field of  graph embedding.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Iuq6u10sCdl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 1,
      "track_name": "Main Conference",
      "author": {
        "id": "~Phani_raj_Chinnalingu1",
        "name": "Phani raj Chinnalingu",
        "name_site": null,
        "openreview_id": "~Phani_raj_Chinnalingu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~nextgenwrl/Members.html",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LFs3CnHwfM",
      "title": "A Robust Fuel Optimization Strategy For Hybrid Electric Vehicles: A Deep Reinforcement Learning Based Continuous Time Design Approach",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "This paper deals with the fuel optimization problem for hybrid electric vehicles in reinforcement learning framework. Firstly, considering the hybrid electric vehicle as a completely observable non-linear system with uncertain dynamics, we solve an open-loop  deterministic optimization problem. This is followed by the design of a  deep reinforcement learning based optimal controller for the non-linear system using concurrent learning based system identifier such that the actual states and the control policy are able to track the optimal trajectory and optimal policy, autonomously even in the presence of external disturbances, modeling errors, uncertainties and noise and signigicantly reducing the computational complexity at the same time, which is in sharp contrast to the conventional methods like PID and Model Predictive Control (MPC) as well as traditional RL approaches like ADP, DDP and DQN that mostly depend on a set of pre-defined rules and provide sub-optimal solutions under similar conditions. The low value of the H-infinity ($H_{\\infty})$ performance index of the proposed optimization algorithm addresses the robustness issue. The optimization technique thus proposed is compared with the traditional fuel optimization strategies for hybrid electric vehicles to illustate the efficacy of the proposed method.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LFs3CnHwfM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nilanjan_Mukherjee1",
        "name": "Nilanjan Mukherjee",
        "name_site": null,
        "openreview_id": "~Nilanjan_Mukherjee1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "nilanjan-mukherjee-98805a1b8",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LFs3CnHwfM",
      "title": "A Robust Fuel Optimization Strategy For Hybrid Electric Vehicles: A Deep Reinforcement Learning Based Continuous Time Design Approach",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "This paper deals with the fuel optimization problem for hybrid electric vehicles in reinforcement learning framework. Firstly, considering the hybrid electric vehicle as a completely observable non-linear system with uncertain dynamics, we solve an open-loop  deterministic optimization problem. This is followed by the design of a  deep reinforcement learning based optimal controller for the non-linear system using concurrent learning based system identifier such that the actual states and the control policy are able to track the optimal trajectory and optimal policy, autonomously even in the presence of external disturbances, modeling errors, uncertainties and noise and signigicantly reducing the computational complexity at the same time, which is in sharp contrast to the conventional methods like PID and Model Predictive Control (MPC) as well as traditional RL approaches like ADP, DDP and DQN that mostly depend on a set of pre-defined rules and provide sub-optimal solutions under similar conditions. The low value of the H-infinity ($H_{\\infty})$ performance index of the proposed optimization algorithm addresses the robustness issue. The optimization technique thus proposed is compared with the traditional fuel optimization strategies for hybrid electric vehicles to illustate the efficacy of the proposed method.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LFs3CnHwfM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sudeshna_Sarkar1",
        "name": "Sudeshna Sarkar",
        "name_site": null,
        "openreview_id": "~Sudeshna_Sarkar1",
        "position": 2,
        "gender": "F",
        "homepage_url": "http://cse.iitkgp.ac.in/~sudeshna/",
        "dblp_id": "61/3197",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=AwP_bbsAAAAJ",
        "orcid": "0000-0003-3439-4282",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ODKwX19UjOj",
      "title": "Unsupervised Hierarchical Concept Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Discovering concepts (or temporal abstractions) in an unsupervised manner from demonstration data in the absence of an environment is an important problem. Organizing these discovered concepts hierarchically at different levels of abstraction is useful in discovering patterns, building ontologies, and generating tutorials from demonstration data. However, recent work to discover such concepts without access to any environment does not discover relationships (or a hierarchy) between these discovered concepts.  In this paper, we present a Transformer-based concept abstraction architecture UNHCLE (pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way from demonstration data. We empirically demonstrate how UNHCLE discovers meaningful hierarchies using datasets from Chess and Cooking domains. Finally, we show how UNHCLE learns meaningful language labels for concepts by using demonstration data augmented with natural language for cooking and chess.  All of our code is available at https://github.com/UNHCLE/UNHCLE\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ODKwX19UjOj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumegh_Roychowdhury1",
        "name": "Sumegh Roychowdhury",
        "name_site": null,
        "openreview_id": "~Sumegh_Roychowdhury1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "246/0200",
        "google_scholar_url": "8T4DcYIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "QSMvGB5j5-",
      "title": "Higher-order Structure Prediction in Evolving Graph Simplicial Complexes",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Dynamic graphs are rife with higher-order interactions, such as co-authorship relationships and protein-protein interactions in biological networks, that naturally arise between more than two nodes at once. In spite of the ubiquitous presence of such higher-order interactions, limited attention has been paid to the higher-order counterpart of the popular pairwise link prediction problem. Existing higher-order structure prediction methods are mostly based on heuristic feature extraction procedures, which work well in practice but lack theoretical guarantees. Such heuristics are primarily focused on predicting links in a static snapshot of the graph. Moreover, these heuristic-based methods fail to effectively utilize and benefit from the knowledge of latent substructures already present within the higher-order structures. In this paper, we overcome these obstacles by capturing higher-order interactions succinctly as simplices, model their neighborhood by face-vectors, and develop a nonparametric kernel estimator for simplices that views the evolving graph from the perspective of a time process (i.e., a sequence of graph snapshots). Our method substantially outperforms several baseline higher-order prediction methods. As a theoretical achievement, we prove the consistency and asymptotic normality in terms of Wasserstein distance of our estimator using Stein's method.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=QSMvGB5j5-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manohar_Kaul1",
        "name": "Manohar Kaul",
        "name_site": null,
        "openreview_id": "~Manohar_Kaul1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://manukaul.github.io/",
        "dblp_id": "29/10735",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=jNroyK4AAAAJ",
        "orcid": null,
        "linkedin_url": "manu-k-72b936287/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RDpTZpubOh7",
      "title": "Safety Aware Reinforcement Learning (SARL)",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RDpTZpubOh7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SVP44gujOBL",
      "title": "A Simple Approach To Define Curricula For Training Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In practice, sequence of mini-batches generated by uniform sampling of examples from the entire data is used for training neural networks. Curriculum learning is a training strategy that sorts the training examples by their difficulty and gradually exposes them to the learner. In this work, we propose two novel curriculum learning algorithms and empirically show their improvements in performance with convolutional and fully-connected neural networks on multiple real image datasets. Our dynamic curriculum learning algorithm tries to reduce the distance between the network weight and an optimal weight at any training step by greedily sampling examples with gradients that are directed towards the optimal weight. The curriculum ordering determined by our dynamic algorithm achieves a training speedup of $\\sim 45\\%$ in our experiments. We also introduce a new task-specific curriculum learning strategy that uses statistical measures such as standard deviation and entropy values to score the difficulty of data points in natural image datasets. We show that this new approach yields a mean training speedup of $\\sim 43\\%$ in the experiments we perform. Further, we also use our algorithms to learn why curriculum learning works. Based on our study, we argue that curriculum learning removes noisy examples from the initial phases of training, and gradually exposes them to the learner acting like a regularizer that helps in improving the generalization ability of the learner.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=SVP44gujOBL",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinu_Sankar_Sadasivan1",
        "name": "Vinu Sankar Sadasivan",
        "name_site": null,
        "openreview_id": "~Vinu_Sankar_Sadasivan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vinusankars.github.io/",
        "dblp_id": "244/8052",
        "google_scholar_url": "y1IKIw0AAAAJ",
        "orcid": null,
        "linkedin_url": "vinusankars/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Gandhinagar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.5,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SVP44gujOBL",
      "title": "A Simple Approach To Define Curricula For Training Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In practice, sequence of mini-batches generated by uniform sampling of examples from the entire data is used for training neural networks. Curriculum learning is a training strategy that sorts the training examples by their difficulty and gradually exposes them to the learner. In this work, we propose two novel curriculum learning algorithms and empirically show their improvements in performance with convolutional and fully-connected neural networks on multiple real image datasets. Our dynamic curriculum learning algorithm tries to reduce the distance between the network weight and an optimal weight at any training step by greedily sampling examples with gradients that are directed towards the optimal weight. The curriculum ordering determined by our dynamic algorithm achieves a training speedup of $\\sim 45\\%$ in our experiments. We also introduce a new task-specific curriculum learning strategy that uses statistical measures such as standard deviation and entropy values to score the difficulty of data points in natural image datasets. We show that this new approach yields a mean training speedup of $\\sim 43\\%$ in the experiments we perform. Further, we also use our algorithms to learn why curriculum learning works. Based on our study, we argue that curriculum learning removes noisy examples from the initial phases of training, and gradually exposes them to the learner acting like a regularizer that helps in improving the generalization ability of the learner.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=SVP44gujOBL",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Dasgupta1",
        "name": "Anirban Dasgupta",
        "name_site": null,
        "openreview_id": "~Anirban_Dasgupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/anirbandasgupta",
        "dblp_id": "54/385-1",
        "google_scholar_url": "plJC8R0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Gandhinagar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.5,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UEtNMTl6yN",
      "title": "Neural Pooling for Graph Neural Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Tasks such as graph classification, require graph pooling to learn graph-level representations from constituent node representations. In this work, we propose two novel methods using fully connected neural network layers for graph pooling, namely Neural Pooling Method 1 and 2. Our proposed methods have the ability to handle variable number of nodes in different graphs, and are also invariant to the isomorphic structures of graphs. In addition, compared to existing graph pooling methods, our proposed methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful. We perform experiments on graph classification tasks in the bio-informatics and social network domains to determine the effectiveness of our proposed methods. Experimental results show that our methods lead to an absolute increase of upto 1.2% in classification accuracy over previous works and a general decrease in standard deviation across multiple runs indicating greater reliability. Experimental results also indicate that this improvement in performance is consistent across several datasets.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=UEtNMTl6yN",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sai_Sree_Harsha1",
        "name": "Sai Sree Harsha",
        "name_site": null,
        "openreview_id": "~Sai_Sree_Harsha1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sreesai1412.github.io",
        "dblp_id": "313/8920",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "sreesai1412/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Technology, Karnataka (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 4.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y9lsZcQ2Fcc",
      "title": "Learning compositional structures for deep learning: why routing-by-agreement is necessary",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": " A formal description of the compositionality of neural networks is associated directly with the formal grammar-structure of the objects it seeks to represent. This formal grammar-structure specifies the kind of components that make up an object, and also the configurations they are allowed to be in. In other words, objects can be described as a parse-tree of its components - a structure that can be seen as a candidate for building connection-patterns among neurons in neural networks. We present a formal grammar description of convolutional neural networks and capsule networks that shows how capsule networks can enforce such parse-tree structures, while CNNs do not. Specifically, we show that the entropy of routing coefficients in the dynamic routing algorithm controls this ability. Thus, we introduce the entropy of routing weights as a loss function for better compositionality among capsules. We show by experiments, on data with a compositional structure, that the use of this loss enables capsule networks to better detect changes in compositionality.  Our experiments show that as the entropy of the routing weights increases, the ability to detect changes in compositionality reduces. We see that, without routing, capsule networks perform similar to convolutional neural networks in that both these models perform badly at detecting changes in compositionality. Our results indicate that routing is an important part of capsule networks - effectively answering recent work that has questioned its necessity. We also, by experiments on SmallNORB, CIFAR-10, and FashionMNIST, show that this loss keeps the accuracy of capsule network models  comparable to models that do not use it . ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Y9lsZcQ2Fcc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sairaam_Venkatraman1",
        "name": "Sairaam Venkatraman",
        "name_site": null,
        "openreview_id": "~Sairaam_Venkatraman1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "225/5317",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sri Sathya Sai Institute of Higher Learning (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y9lsZcQ2Fcc",
      "title": "Learning compositional structures for deep learning: why routing-by-agreement is necessary",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": " A formal description of the compositionality of neural networks is associated directly with the formal grammar-structure of the objects it seeks to represent. This formal grammar-structure specifies the kind of components that make up an object, and also the configurations they are allowed to be in. In other words, objects can be described as a parse-tree of its components - a structure that can be seen as a candidate for building connection-patterns among neurons in neural networks. We present a formal grammar description of convolutional neural networks and capsule networks that shows how capsule networks can enforce such parse-tree structures, while CNNs do not. Specifically, we show that the entropy of routing coefficients in the dynamic routing algorithm controls this ability. Thus, we introduce the entropy of routing weights as a loss function for better compositionality among capsules. We show by experiments, on data with a compositional structure, that the use of this loss enables capsule networks to better detect changes in compositionality.  Our experiments show that as the entropy of the routing weights increases, the ability to detect changes in compositionality reduces. We see that, without routing, capsule networks perform similar to convolutional neural networks in that both these models perform badly at detecting changes in compositionality. Our results indicate that routing is an important part of capsule networks - effectively answering recent work that has questioned its necessity. We also, by experiments on SmallNORB, CIFAR-10, and FashionMNIST, show that this loss keeps the accuracy of capsule network models  comparable to models that do not use it . ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Y9lsZcQ2Fcc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "ankitanandjmp95@gmail.com",
        "name": "Ankit Anand",
        "name_site": null,
        "openreview_id": "ankitanandjmp95@gmail.com",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sri Sathya Sai Institute of Higher Learning (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Y9lsZcQ2Fcc",
      "title": "Learning compositional structures for deep learning: why routing-by-agreement is necessary",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": " A formal description of the compositionality of neural networks is associated directly with the formal grammar-structure of the objects it seeks to represent. This formal grammar-structure specifies the kind of components that make up an object, and also the configurations they are allowed to be in. In other words, objects can be described as a parse-tree of its components - a structure that can be seen as a candidate for building connection-patterns among neurons in neural networks. We present a formal grammar description of convolutional neural networks and capsule networks that shows how capsule networks can enforce such parse-tree structures, while CNNs do not. Specifically, we show that the entropy of routing coefficients in the dynamic routing algorithm controls this ability. Thus, we introduce the entropy of routing weights as a loss function for better compositionality among capsules. We show by experiments, on data with a compositional structure, that the use of this loss enables capsule networks to better detect changes in compositionality.  Our experiments show that as the entropy of the routing weights increases, the ability to detect changes in compositionality reduces. We see that, without routing, capsule networks perform similar to convolutional neural networks in that both these models perform badly at detecting changes in compositionality. Our results indicate that routing is an important part of capsule networks - effectively answering recent work that has questioned its necessity. We also, by experiments on SmallNORB, CIFAR-10, and FashionMNIST, show that this loss keeps the accuracy of capsule network models  comparable to models that do not use it . ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Y9lsZcQ2Fcc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~S_Balasubramanian1",
        "name": "S Balasubramanian",
        "name_site": null,
        "openreview_id": "~S_Balasubramanian1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "hyZbP6QAAAAJ",
        "orcid": "0000-0001-8947-4840",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sri Sathya Sai Institute of Higher Learning (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Zc36Mbb8G6",
      "title": "Data Instance Prior for Transfer Learning in GANs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation and image editing tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Zc36Mbb8G6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Puneet_Mangla1",
        "name": "Puneet Mangla",
        "name_site": null,
        "openreview_id": "~Puneet_Mangla1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "puneet-mangla/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Zc36Mbb8G6",
      "title": "Data Instance Prior for Transfer Learning in GANs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation and image editing tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Zc36Mbb8G6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaji_Krishnamurthy1_1",
        "name": "Balaji Krishnamurthy",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_HsKf3YaWpG",
      "title": "Uniform Priors for Data-Efficient Transfer",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_HsKf3YaWpG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_qJXkf347k",
      "title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_qJXkf347k",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Brijraj_Singh1",
        "name": "Brijraj Singh",
        "name_site": null,
        "openreview_id": "~Brijraj_Singh1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Samsung (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aIg2i1IKv0w",
      "title": "Learning Task-Relevant Features via Contrastive Input Morphing",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "A fundamental challenge in artificial intelligence is learning useful representations of data that yield good performance on a downstream classification task, without overfitting to spurious input features. Extracting task-relevant predictive information becomes particularly challenging for high-dimensional, noisy, real-world data. We propose Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance via a triplet loss. Empirically, we demonstrate the efficacy of our approach on various tasks which typically suffer from the presence of spurious correlations, and show that CIM improves the performance of other representation learning methods such as variational information bottleneck (VIB) when used in conjunction.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=aIg2i1IKv0w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.5,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bMzj6hXL2VJ",
      "title": "Ordering-Based Causal Discovery with Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery. However, searching the space of directed graphs directly and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the method to the small problems. In this work, we alternatively consider searching an ordering by RL from the variable ordering space that is much smaller than that of directed graphs, which also helps avoid dealing with acyclicity. Specifically, we formulate the ordering search problem as a Markov decision process, and then use different reward designs to optimize the ordering generating model. A generated ordering is then processed using variable selection methods to obtain the final directed acyclic graph. In contrast to other causal discovery methods, our method can also utilize a pretrained model to accelerate training. We conduct experiments on both synthetic and real-world datasets, and show that the proposed method outperforms other baselines on important metrics even on large graph tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=bMzj6hXL2VJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Huawei (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 79,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cKnKJcTPRcV",
      "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cKnKJcTPRcV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Amsterdam (Netherlands)",
        "countries": [
          "Netherlands"
        ],
        "country_codes": [
          "NL"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 111,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eIPsmKwTrIe",
      "title": "Using Deep Reinforcement Learning to Train and Evaluate Instructional Sequencing Policies for an Intelligent Tutoring System",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present STEP, a novel Deep Reinforcement Learning solution to the problem of learning instructional sequencing.  STEP has three components: 1. Simulate the student by fitting a knowledge tracing model to data logged by an intelligent tutoring system. 2. Train instructional sequencing policies by using Proximal Policy Optimization. 3. Evaluate the learned instructional policies by estimating their local and global impact on learning gains.  STEP leverages the student model by representing the student’s knowledge state as a probability vector of knowing each skill and using the student’s estimated learning gains as its reward function to evaluate candidate policies.  A learned policy represents a mapping from each state to an action that maximizes the reward, i.e. the upward distance to the next state in the multi-dimensional space. We use STEP to discover and evaluate potential improvements to a literacy and numeracy tutor used by hundreds of children in Tanzania.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=eIPsmKwTrIe",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jithendaraa_Subramanian1",
        "name": "Jithendaraa Subramanian",
        "name_site": null,
        "openreview_id": "~Jithendaraa_Subramanian1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://jithendaraa.github.io/",
        "dblp_id": "281/6755",
        "google_scholar_url": "s0BzYvYAAAAJ",
        "orcid": null,
        "linkedin_url": "jithendaraa-subramanian-85a22b176/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Technology, Tiruchirappalli (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gp5Uzbl-9C-",
      "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves have known semantics or are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. In this work, we systematically evaluate the agent's ability to learn underlying causal structure. We note that existing environments for studying causal induction are poorly suited for this  objective because they have complicated task-specific causal graphs with many confounding factors. Hence, to facilitate research in learning the representation of high-level variables as well as causal structure among these  variables, we present a suite of RL environments created to systematically probe the ability of methods to identify variables as well as causal structure among those variables. We evaluate various representation learning algorithms from literature and found that  explicitly incorporating structure and modularity in the model can help causal induction in model-based reinforcement learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=gp5Uzbl-9C-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Rajiv_Didolkar1",
        "name": "Aniket Rajiv Didolkar",
        "name_site": null,
        "openreview_id": "~Aniket_Rajiv_Didolkar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://github.com/dido1998/",
        "dblp_id": "245/8589",
        "google_scholar_url": "https://scholar.google.ca/citations?user=ekvl5o0AAAAJ",
        "orcid": null,
        "linkedin_url": "aniket-didolkar-7a9b8912a",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Manipal Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 55,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gp5Uzbl-9C-",
      "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves have known semantics or are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. In this work, we systematically evaluate the agent's ability to learn underlying causal structure. We note that existing environments for studying causal induction are poorly suited for this  objective because they have complicated task-specific causal graphs with many confounding factors. Hence, to facilitate research in learning the representation of high-level variables as well as causal structure among these  variables, we present a suite of RL environments created to systematically probe the ability of methods to identify variables as well as causal structure among those variables. We evaluate various representation learning algorithms from literature and found that  explicitly incorporating structure and modularity in the model can help causal induction in model-based reinforcement learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=gp5Uzbl-9C-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Montreal (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 55,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "klB-8BpR5N",
      "title": "BLANK",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "BLANK",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=klB-8BpR5N",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumya_Banerjee1",
        "name": "Soumya Banerjee",
        "name_site": null,
        "openreview_id": "~Soumya_Banerjee1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.cse.iitk.ac.in/users/soumyab/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.7320508075688772,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mhEd8uOyNTI",
      "title": "Representational correlates of hierarchical phrase structure in deep language models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While contextual representations from Transformer-based architectures have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of input perturbation-based analyses of representations from Transformer networks pretrained on self-supervised objectives. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of Transformer representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. We also connect our probe results to the Transformer architecture by relating the attention mechanism to syntactic distance between two words. Results from the three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. In particular, sensitivity to local phrase structure increases along deeper layers. Based on our analysis of attention, we show that this is at least partly explained by generally larger attention weights between syntactically distant words.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=mhEd8uOyNTI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 5,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.6,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pbUcKxmiM54",
      "title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=pbUcKxmiM54",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kalidas_Yeturu1",
        "name": "Kalidas Yeturu",
        "name_site": null,
        "openreview_id": "~Kalidas_Yeturu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://iittp.ac.in/dr-kalidas-yeturu",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pbUcKxmiM54",
      "title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=pbUcKxmiM54",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manish_Kumar_Srivastava1",
        "name": "Manish Kumar Srivastava",
        "name_site": null,
        "openreview_id": "~Manish_Kumar_Srivastava1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://github.com/000c000l",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Tirupati (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pdsec2YIOCx",
      "title": "Untangle: Critiquing Disentangled Recommendations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The core principle behind most collaborative filtering methods is to embed users and items in latent spaces, where individual dimensions are learned independently of any particular item attributes. It is thus difficult for users to control their recommendations based on particular aspects (critiquing). In this work, we propose Untangle: a recommendation model that gives users control over the recommendation list with respect to specific item attributes, (e.g.:less violent, funnier movies) that have a causal relationship in user preferences. Untangle uses a refined training procedure by training (i) a (partially) supervised β-VAE that disentangles the item representations and (ii) a second phase which optimized to generate recommendations for users. Untangle gives control on critiquing recommendations based on users preferences, without sacrificing on recommendation accuracy. Moreover only a tiny fraction of labeled items is needed to create disentangled preference representations over attributes. \n\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=pdsec2YIOCx",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Preksha_Nema1",
        "name": "Preksha Nema",
        "name_site": null,
        "openreview_id": "~Preksha_Nema1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=hmoy8ssAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.5,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pdsec2YIOCx",
      "title": "Untangle: Critiquing Disentangled Recommendations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The core principle behind most collaborative filtering methods is to embed users and items in latent spaces, where individual dimensions are learned independently of any particular item attributes. It is thus difficult for users to control their recommendations based on particular aspects (critiquing). In this work, we propose Untangle: a recommendation model that gives users control over the recommendation list with respect to specific item attributes, (e.g.:less violent, funnier movies) that have a causal relationship in user preferences. Untangle uses a refined training procedure by training (i) a (partially) supervised β-VAE that disentangles the item representations and (ii) a second phase which optimized to generate recommendations for users. Untangle gives control on critiquing recommendations based on users preferences, without sacrificing on recommendation accuracy. Moreover only a tiny fraction of labeled items is needed to create disentangled preference representations over attributes. \n\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=pdsec2YIOCx",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Filip_Radlinski1",
        "name": "Filip Radlinski",
        "name_site": null,
        "openreview_id": "~Preksha_Nema2",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.5,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qRdED5QjM9e",
      "title": "Distributionally Robust Learning for Unsupervised Domain Adaptation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We propose a  distributionally robust learning (DRL) method for unsupervised domain adaptation (UDA)  that scales to modern computer-vision benchmarks.  DRL can be naturally formulated as a competitive two-player game between a predictor and an adversary that is allowed to   corrupt the labels, subject to certain constraints, and reduces to incorporating  a density ratio between the source and target domains (under the standard log loss).  This formulation motivates the use of two neural networks that are jointly trained --- a discriminative network between the source and target domains  for density-ratio estimation, in addition to the standard classification network. The use of a density ratio in DRL prevents the model from being overconfident on target inputs far away from the source domain. Thus,  DRL   provides conservative confidence estimation in the target domain, even when the target labels are not available. This conservatism motivates the use of DRL in  self-training for sample selection, and we term the approach distributionally robust self-training (DRST). In our experiments, DRST generates more calibrated  probabilities and achieves state-of-the-art self-training accuracy on benchmark datasets. We demonstrate that DRST captures  shape features more effectively, and reduces the extent of distributional shift during self-training. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=qRdED5QjM9e",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "California Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "r7L91opmsr",
      "title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Effective variational inference crucially depends on a flexible variational family of distributions. Recent work has explored sequential Monte-Carlo (SMC) methods to construct variational distributions, which can, in principle, approximate the target posterior arbitrarily well, which is especially appealing for models with inherent sequential structure. However, SMC, which represents the posterior using a weighted set of particles, often suffers from particle weight degeneracy, leading to a large variance of the resulting estimators. To address this issue, we present a novel approach that leverages the idea of \\emph{partial} rejection control (PRC) for developing a robust variational inference (VI) framework. In addition to developing a superior VI bound, we propose a novel marginal likelihood estimator constructed via a dice-enterprise: a generalization of the Bernoulli factory to construct unbiased estimators for SMC-PRC. The resulting variational lower bound can be optimized efficiently with respect to the variational parameters and generalizes several existing approaches in the VI literature into a single framework. We show theoretical properties of the lower bound and report experiments on various sequential models, such as the Gaussian state-space model and variational RNN, on which our approach outperforms existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=r7L91opmsr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rahul_Sharma1",
        "name": "Rahul Sharma",
        "name_site": null,
        "openreview_id": "~Rahul_Sharma1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "r7L91opmsr",
      "title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Effective variational inference crucially depends on a flexible variational family of distributions. Recent work has explored sequential Monte-Carlo (SMC) methods to construct variational distributions, which can, in principle, approximate the target posterior arbitrarily well, which is especially appealing for models with inherent sequential structure. However, SMC, which represents the posterior using a weighted set of particles, often suffers from particle weight degeneracy, leading to a large variance of the resulting estimators. To address this issue, we present a novel approach that leverages the idea of \\emph{partial} rejection control (PRC) for developing a robust variational inference (VI) framework. In addition to developing a superior VI bound, we propose a novel marginal likelihood estimator constructed via a dice-enterprise: a generalization of the Bernoulli factory to construct unbiased estimators for SMC-PRC. The resulting variational lower bound can be optimized efficiently with respect to the variational parameters and generalizes several existing approaches in the VI literature into a single framework. We show theoretical properties of the lower bound and report experiments on various sequential models, such as the Gaussian state-space model and variational RNN, on which our approach outperforms existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=r7L91opmsr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumya_Banerjee1",
        "name": "Soumya Banerjee",
        "name_site": null,
        "openreview_id": "~Soumya_Banerjee1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitk.ac.in/users/soumyab/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "r7L91opmsr",
      "title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Effective variational inference crucially depends on a flexible variational family of distributions. Recent work has explored sequential Monte-Carlo (SMC) methods to construct variational distributions, which can, in principle, approximate the target posterior arbitrarily well, which is especially appealing for models with inherent sequential structure. However, SMC, which represents the posterior using a weighted set of particles, often suffers from particle weight degeneracy, leading to a large variance of the resulting estimators. To address this issue, we present a novel approach that leverages the idea of \\emph{partial} rejection control (PRC) for developing a robust variational inference (VI) framework. In addition to developing a superior VI bound, we propose a novel marginal likelihood estimator constructed via a dice-enterprise: a generalization of the Bernoulli factory to construct unbiased estimators for SMC-PRC. The resulting variational lower bound can be optimized efficiently with respect to the variational parameters and generalizes several existing approaches in the VI literature into a single framework. We show theoretical properties of the lower bound and report experiments on various sequential models, such as the Gaussian state-space model and variational RNN, on which our approach outperforms existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=r7L91opmsr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dootika_Vats1",
        "name": "Dootika Vats",
        "name_site": null,
        "openreview_id": "~Dootika_Vats1",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://home.iitk.ac.in/~dootika/",
        "dblp_id": null,
        "google_scholar_url": "sZT_wEAAAAA",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "r7L91opmsr",
      "title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Effective variational inference crucially depends on a flexible variational family of distributions. Recent work has explored sequential Monte-Carlo (SMC) methods to construct variational distributions, which can, in principle, approximate the target posterior arbitrarily well, which is especially appealing for models with inherent sequential structure. However, SMC, which represents the posterior using a weighted set of particles, often suffers from particle weight degeneracy, leading to a large variance of the resulting estimators. To address this issue, we present a novel approach that leverages the idea of \\emph{partial} rejection control (PRC) for developing a robust variational inference (VI) framework. In addition to developing a superior VI bound, we propose a novel marginal likelihood estimator constructed via a dice-enterprise: a generalization of the Bernoulli factory to construct unbiased estimators for SMC-PRC. The resulting variational lower bound can be optimized efficiently with respect to the variational parameters and generalizes several existing approaches in the VI literature into a single framework. We show theoretical properties of the lower bound and report experiments on various sequential models, such as the Gaussian state-space model and variational RNN, on which our approach outperforms existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=r7L91opmsr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Rai1",
        "name": "Piyush Rai",
        "name_site": null,
        "openreview_id": "~Piyush_Rai1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cse.iitk.ac.in/users/piyush/",
        "dblp_id": "02/525",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=D50grEgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sAX7Z7uIJ_Y",
      "title": "Calibrated Adversarial Refinement for Stochastic Semantic Segmentation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Ambiguities in images or unsystematic annotation can lead to multiple valid solutions in semantic segmentation. To learn a distribution over predictions, recent work has explored the use of probabilistic networks. However, these do not necessarily capture the empirical distribution accurately. In this work, we aim to learn a calibrated multimodal predictive distribution, where the empirical frequency of the sampled predictions closely reflects that of the corresponding labels in the training set. To this end, we propose a novel two-stage, cascaded strategy for calibrated adversarial refinement. In the first stage, we explicitly model the data with a categorical likelihood. In the second, we train an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. We demonstrate the utility and versatility of the approach by attaining state-of-the-art results on the multigrader LIDC dataset and a modified Cityscapes dataset. In addition, we use a toy regression dataset to show that our framework is not confined to semantic segmentation, and the core design can be adapted to other tasks requiring learning a calibrated predictive distribution.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sAX7Z7uIJ_Y",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "TomTom (Netherlands)",
        "countries": [
          "Netherlands"
        ],
        "country_codes": [
          "NL"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 20,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sTcyRPRQ2VT",
      "title": "An Automated Domain Understanding Technique for Knowledge Graph Generation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Domain-specific Knowledge Graph (KG) generation is a labor intensive task usually orchestrated and supervised by subject matter experts. Herein, we propose a strategy to automate the generation process following a two-step approach. Initially, the structure of the domain of interest is inferred from the corpus in the form of a metagraph. Afterwards, once the domain structure has been discovered, named entity recognition (NER) and relation extraction (RE) models can be used to generate a domain-specific KG.  We argue why the automated definition of the domain's structure as a first step is beneficial both in terms of  construction time and quality of the generated graph. Furthermore, we present a machine learning approach, based on Transformers, to infer the structure of a corpus's domain. The proposed method is extensively validated on three public datasets (WebNLG, NYT and DocRED) by comparing it with two reference methods using CNNs and RNNs. Lastly, we demonstrate how this work lays the foundation for fully automated and unsupervised KG generation.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sTcyRPRQ2VT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Matteo_Manica1",
        "name": "Matteo Manica",
        "name_site": null,
        "openreview_id": "~Matteo_Manica1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ibm.biz/matteomanica",
        "dblp_id": "194/3100",
        "google_scholar_url": "-20KQZQAAAAJ",
        "orcid": "0000-0002-8872-0269",
        "linkedin_url": "matteo-manica-drugilsberg/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.3333333333333335,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "vCEhC7nOb6",
      "title": "Inductive Bias of Gradient Descent for Exponentially Weight Normalized Smooth Homogeneous Neural Nets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We analyze the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. Our analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rate. The asymptotic convergence rate of the loss in this setting is given by $\\Theta(\\frac{1}{t(\\log t)^2})$, and is independent of the depth of the network. We contrast these results with the inductive bias of standard weight normalization (SWN) and unnormalized architectures, and demonstrate their implications on synthetic data sets.Experimental results on simple data sets and architectures support our claim on sparse EWN solutions, even with SGD. This demonstrates its potential applications in learning prunable neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=vCEhC7nOb6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Depen_Morwani1",
        "name": "Depen Morwani",
        "name_site": null,
        "openreview_id": "~Depen_Morwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "277/5200",
        "google_scholar_url": "vOngxFUAAAAJ",
        "orcid": null,
        "linkedin_url": "depen-morwani-070298122/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "vCEhC7nOb6",
      "title": "Inductive Bias of Gradient Descent for Exponentially Weight Normalized Smooth Homogeneous Neural Nets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We analyze the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. Our analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rate. The asymptotic convergence rate of the loss in this setting is given by $\\Theta(\\frac{1}{t(\\log t)^2})$, and is independent of the depth of the network. We contrast these results with the inductive bias of standard weight normalization (SWN) and unnormalized architectures, and demonstrate their implications on synthetic data sets.Experimental results on simple data sets and architectures support our claim on sparse EWN solutions, even with SGD. This demonstrates its potential applications in learning prunable neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=vCEhC7nOb6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harish_Guruprasad_Ramaswamy1",
        "name": "Harish Guruprasad Ramaswamy",
        "name_site": null,
        "openreview_id": "~Harish_Guruprasad_Ramaswamy1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "126/1729",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xrUySgB5ZOK",
      "title": "Learning Visual Representations for Transfer Learning by Suppressing Texture",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works have shown that features obtained from supervised training of CNNs may over-emphasize texture rather than encoding high-level information.  In self-supervised learning, in particular, texture as a low-level cue may provide shortcuts that prevent the network from learning higher-level representations.  To address these problems we propose to use classic methods based on anisotropic diffusion to augment training using images with suppressed texture. This simple method helps retain important edge information and suppress texture at the same time. \nWe report our observations for fully supervised and self-supervised learning tasks like MoCoV2 and Jigsaw and achieve state-of-the-art results on object detection and image classification with eight diverse datasets. \nOur method is particularly effective for transfer learning tasks and we observed improved performance on five standard transfer learning datasets. \nThe large improvements on the Sketch-ImageNet dataset, DTD dataset and\nadditional visual analyses of saliency maps suggest that our approach helps in learning better representations that transfer well.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=xrUySgB5ZOK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland, College Park (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 17,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yEnaS6yOkxy",
      "title": "Class Balancing GAN with a Classifier in the Loop",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datasets. We find that the existing GANs and their training regimes which work well on balanced datasets fail to be effective in case of imbalanced (i.e. long-tailed) datasets. In this work we introduce a novel and theoretically motivated Class Balancing regularizer for training GANs. Our regularizer makes use of the knowledge from a pre-trained classifier to ensure balanced learning of all the classes in the dataset. This is achieved via modelling the effective class frequency based on the exponential forgetting observed in neural networks and encouraging the GAN to focus on underrepresented classes. We demonstrate the utility of our contribution in two diverse scenarios: (i) Learning representations for long-tailed distributions, where we achieve better performance than existing approaches, and (ii) Generation of Universal Adversarial Perturbations (UAPs) in the data-free scenario for the large scale datasets, where we bridge the gap between data-driven and data-free approaches for crafting UAPs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yEnaS6yOkxy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Rangwani1",
        "name": "Harsh Rangwani",
        "name_site": null,
        "openreview_id": "~Harsh_Rangwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rangwani-harsh.github.io/about/",
        "dblp_id": "220/0991",
        "google_scholar_url": "OQK0WREAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 28,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yEnaS6yOkxy",
      "title": "Class Balancing GAN with a Classifier in the Loop",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datasets. We find that the existing GANs and their training regimes which work well on balanced datasets fail to be effective in case of imbalanced (i.e. long-tailed) datasets. In this work we introduce a novel and theoretically motivated Class Balancing regularizer for training GANs. Our regularizer makes use of the knowledge from a pre-trained classifier to ensure balanced learning of all the classes in the dataset. This is achieved via modelling the effective class frequency based on the exponential forgetting observed in neural networks and encouraging the GAN to focus on underrepresented classes. We demonstrate the utility of our contribution in two diverse scenarios: (i) Learning representations for long-tailed distributions, where we achieve better performance than existing approaches, and (ii) Generation of Universal Adversarial Perturbations (UAPs) in the data-free scenario for the large scale datasets, where we bridge the gap between data-driven and data-free approaches for crafting UAPs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yEnaS6yOkxy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Konda_Reddy_Mopuri3",
        "name": "Konda Reddy Mopuri",
        "name_site": null,
        "openreview_id": "~Konda_Reddy_Mopuri3",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://krmopuri.github.io/",
        "dblp_id": "162/0085",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-8894-7212",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Guwahati (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 28,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yEnaS6yOkxy",
      "title": "Class Balancing GAN with a Classifier in the Loop",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datasets. We find that the existing GANs and their training regimes which work well on balanced datasets fail to be effective in case of imbalanced (i.e. long-tailed) datasets. In this work we introduce a novel and theoretically motivated Class Balancing regularizer for training GANs. Our regularizer makes use of the knowledge from a pre-trained classifier to ensure balanced learning of all the classes in the dataset. This is achieved via modelling the effective class frequency based on the exponential forgetting observed in neural networks and encouraging the GAN to focus on underrepresented classes. We demonstrate the utility of our contribution in two diverse scenarios: (i) Learning representations for long-tailed distributions, where we achieve better performance than existing approaches, and (ii) Generation of Universal Adversarial Perturbations (UAPs) in the data-free scenario for the large scale datasets, where we bridge the gap between data-driven and data-free approaches for crafting UAPs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yEnaS6yOkxy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 28,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zFM0Uo_GnYE",
      "title": "On the Importance of Looking at the Manifold",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=zFM0Uo_GnYE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Matteo_Manica1",
        "name": "Matteo Manica",
        "name_site": null,
        "openreview_id": "~Matteo_Manica1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://ibm.biz/matteomanica",
        "dblp_id": "194/3100",
        "google_scholar_url": "-20KQZQAAAAJ",
        "orcid": "0000-0002-8872-0269",
        "linkedin_url": "matteo-manica-drugilsberg/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zgGmAx9ZcY",
      "title": "Learning the Connections in Direct Feedback Alignment",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Feedback alignment was proposed to address the biological implausibility of the backpropagation algorithm which requires the transportation of the weight transpose during the backwards pass. The idea was later built upon with the proposal of direct feedback alignment (DFA), which propagates the error directly from the output layer to each hidden layer in the backward path using a fixed random weight matrix. This contribution was significant because it allowed for the parallelization of the backwards pass by the use of these feedback connections. However, just as feedback alignment, DFA does not perform well in deep convolutional networks. We propose to learn the backward weight matrices in DFA, adopting the methodology of Kolen-Pollack learning, to improve training and inference accuracy in deep convolutional neural networks by updating the direct feedback connections such that they come to estimate the forward path. The proposed method improves the accuracy of learning by direct feedback connections and reduces the gap between parallel training to serial training by means of backpropagation.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=zgGmAx9ZcY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NAVER Corporation (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zsKWh2pRSBK",
      "title": "Poisoned classifiers are not only backdoored, they are fundamentally broken",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Under a commonly-studied “backdoor” poisoning attack against classification models, an attacker adds a small “trigger” to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is fundamentally incorrect. We demonstrate that anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a recent process called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images. We demonstrate the effectiveness of our attack through extensive experiments on ImageNet and TrojAI datasets, including a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers can in fact look entirely different from the original trigger, highlighting that the backdoor actually learned by the classifier differs substantially from the trigger image itself. Thus, we argue that there is no such thing as a “secret” backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=zsKWh2pRSBK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddhant_Agarwal1",
        "name": "Siddhant Agarwal",
        "name_site": null,
        "openreview_id": "~Siddhant_Agarwal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://agarwalsiddhant10.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "siddhant-agarwal-688a31156/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    }
  ]
}