{
  "conference": "ICLR 2023",
  "focus_country": "India",
  "total_papers": 179,
  "generated_at": "2025-07-06T10:37:20.836555",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "sCYXJr3QJM8",
      "title": "Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.",
      "tldr": "Adapt a GAN trained on a single large-scale source dataset to multiple target domains containing very few examples without re-training the pretrained source generator.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11530",
      "pdf_url": "https://openreview.net/pdf?id=sCYXJr3QJM8",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal2",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal2",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "MZ8N49AAAAAJ",
        "orcid": "0000-0001-7297-374X",
        "linkedin_url": "arnab-mondal-a4448a18/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu Research and Development Center (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3OR2tbtnYC-",
      "title": "Near-optimal Policy Identification in Active Reinforcement Learning",
      "status": "Top-5%",
      "normalized_status": "oral",
      "abstract": "Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \\emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \\emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required. ",
      "tldr": "We propose a novel kernelized LSVI algorithm for active reinforcement learning which provably identifies a near-optimal policy uniformly over the entire state space.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11757",
      "pdf_url": "https://openreview.net/pdf?id=3OR2tbtnYC-",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ilija_Bogunovic2",
        "name": "Ilija Bogunovic",
        "name_site": null,
        "openreview_id": "~Ilija_Bogunovic1",
        "position": 8,
        "gender": "M",
        "homepage_url": "http://ilijabogunovic.com/",
        "dblp_id": "142/2725",
        "google_scholar_url": "xMvt3NEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Swiss Federal Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 20.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "-lGvSmht7a",
      "title": "Sequential Gradient Coding For Straggler Mitigation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In distributed computing, slower nodes (stragglers) usually become a bottleneck. Gradient Coding (GC), introduced by Tandon et al., is an efficient technique that uses principles of error-correcting codes to distribute gradient computation in the presence of stragglers. In this paper, we consider the distributed computation of a sequence of gradients $\\{g(1),g(2),\\ldots,g(J)\\}$, where processing of each gradient $g(t)$ starts in round-$t$ and finishes by round-$(t+T)$. Here $T\\geq 0$ denotes a delay parameter. For the GC scheme, coding is only across computing nodes and this results in a solution where $T=0$. On the other hand, having $T>0$ allows for designing schemes which exploit the temporal dimension as well. In this work, we propose two schemes that demonstrate improved performance compared to GC. Our first scheme combines GC with selective repetition of previously unfinished tasks and achieves improved straggler mitigation. In our second scheme, which constitutes our main contribution, we apply GC  to a subset of the tasks and repetition for the remainder of the tasks. We then multiplex these two classes of tasks across workers and rounds in an adaptive manner, based on past straggler patterns. Using theoretical analysis, we demonstrate that our second scheme achieves significant reduction in the computational load. In our experiments, we study a practical setting of concurrently training multiple neural networks over an AWS Lambda cluster involving 256 worker nodes, where our framework naturally applies. We demonstrate that the latter scheme can yield a 16\\% improvement in runtime over the baseline GC scheme, in the presence of naturally occurring, non-simulated stragglers.\n",
      "tldr": "We propose to improve gradient coding by exploiting the temporal dimension while training deep learning models in distributed cloud systems.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11099",
      "pdf_url": "https://openreview.net/pdf?id=-lGvSmht7a",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nikhil_Krishnan_Muralee_Krishnan1",
        "name": "Nikhil Krishnan Muralee Krishnan",
        "name_site": "Nikhil Krishnan Muralee Krishnan, MohammadReza Ebrahimi, Ashish Khisti",
        "openreview_id": "~Nikhil_Krishnan_Muralee_Krishnan1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "ZQJxEtEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3VKiaagxw1S",
      "title": "Gradient Boosting Performs Gaussian Process Inference",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process' posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection.",
      "tldr": "We prove that gradient boosting converges to a Gaussian process' posterior mean and can be transformed into a sampler from the posterior, which leads to improved knowledge uncertainty estimates.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11849",
      "pdf_url": "https://openreview.net/pdf?id=3VKiaagxw1S",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aleksei_Ustimenko1",
        "name": "Aleksei Ustimenko",
        "name_site": "Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova",
        "openreview_id": "~Aleksei_Ustimenko1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "242/3873",
        "google_scholar_url": "OES5pK4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ShareChat (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATLEl_izD87",
      "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.",
      "tldr": "Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases",
      "site_url": "https://iclr.cc/virtual/2023/poster/11168",
      "pdf_url": "https://openreview.net/pdf?id=ATLEl_izD87",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suresh_Bishnoi1",
        "name": "Suresh Bishnoi",
        "name_site": "Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, N. M. Anoop Krishnan",
        "openreview_id": "~Suresh_Bishnoi1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~srz208500/",
        "dblp_id": "329/6194",
        "google_scholar_url": "Wy6q2QwAAAAJ",
        "orcid": null,
        "linkedin_url": "sureshb1999/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_nGgzQjzaRy",
      "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
      "tldr": "A new few-shot prompting approach to solve complex task by decomposing complex tasks into a shared library of prompts.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10871",
      "pdf_url": "https://openreview.net/pdf?id=_nGgzQjzaRy",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Khot1",
        "name": "Tushar Khot",
        "name_site": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal",
        "openreview_id": "~Tushar_Khot1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://allenai.org/team/tushark/",
        "dblp_id": "83/8117",
        "google_scholar_url": "_8mkIjgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Allen Institute for Artificial Intelligence (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 452,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cDYRS5iZ16f",
      "title": "Learning to Grow Pretrained Models for Efficient Transformer Training",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map  the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of  (linear) width- and  depth-growth operators, and further employ a  Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO)  can save up to 50% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models.",
      "tldr": "Learning to grow smaller, extant models to enable faster training of newer, larger transformers.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11759",
      "pdf_url": "https://openreview.net/pdf?id=cDYRS5iZ16f",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 9,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 67,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mhnHqRqcjYU",
      "title": "DFPC: Data flow driven pruning of coupled channels without data.",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Modern, multi-branched neural network architectures often possess complex interconnections between layers, which we call coupled channels (CCs). Structured pruning of CCs in these multi-branch networks is an under-researched problem, as most existing works are typically designed for pruning single-branch models like VGG-nets. While these methods yield accurate subnetworks, the improvements in inference times when applied to multi-branch networks are comparatively modest, as these methods do not prune CCs, which we observe contribute significantly to inference time. For instance, layers with CCs as input or output take more than 66% of the inference time in ResNet-50. Moreover, pruning in the data-free regime, where data is not used for pruning, is gaining traction owing to privacy concerns and computational costs associated with fine-tuning. Motivated by this, we study the problem of pruning CCs in the data-free regime. To facilitate the development of algorithms to prune CCs, we define Data Flow Couplings (DFCs) to enumerate the layers that constitute coupled connections and the associated transformation. Additionally, saliencies for pruning CCs cannot be gauged in isolation, as there may be discrepancies among the layerwise importance of CCs using conventional scoring strategies. This necessitates finding grouped saliencies to gauge the importance of all corresponding coupled elements in a network. We thus propose the Backwards Graph-based Saliency Computation (BGSC) algorithm, a data-free method that computes saliencies by estimating an upper bound to the reconstruction error of intermediate layers; we call this pruning strategy Data Flow driven Pruning of Coupled channels (DFPC). Finally, we show the efficacy of DFPC for models trained on standard datasets. Since we pruned coupled channels, we achieve up to 1.66x improvements in inference time for ResNet-101 trained on CIFAR-10 with a 5% accuracy drop without fine-tuning. With access to the ImageNet training set, we achieve significant improvements over the data-free method and see an improvement of at least 47.1% in speedup for a 2.3% accuracy drop for ResNet-50 against our baselines.",
      "tldr": "We propose a novel data-free algorithm to accelerate neural networks via pruning coupled channels.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10681",
      "pdf_url": "https://openreview.net/pdf?id=mhnHqRqcjYU",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tanay_Narshana1",
        "name": "Tanay Narshana",
        "name_site": "Tanay Narshana, Chaitanya Murti, Chiranjib Bhattacharyya",
        "openreview_id": "~Tanay_Narshana1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "350/3876",
        "google_scholar_url": "d4YFxEcAAAAJ",
        "orcid": null,
        "linkedin_url": "tanay-narshana/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Observe.AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sCYXJr3QJM8",
      "title": "Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.",
      "tldr": "Adapt a GAN trained on a single large-scale source dataset to multiple target domains containing very few examples without re-training the pretrained source generator.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11530",
      "pdf_url": "https://openreview.net/pdf?id=sCYXJr3QJM8",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sZI1Oj9KBKy",
      "title": "TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Achieving structured, data-free sparsity of deep neural networks (DNNs) remains an open area of research.  In this work, we address the challenge of pruning filters without access to the original training set or loss function. We propose the discriminative filters hypothesis, that well-trained models possess discriminative filters, and any non-discriminative filters can be pruned without impacting the predictive performance of the classifier. Based on this hypothesis, we propose a new paradigm for pruning neural networks: distributional pruning, wherein we only require access to the distributions that generated the original datasets. Our approach to solving the problem of formalising and quantifying the discriminating ability of filters is through the total variation (TV) distance between the class-conditional distributions of the filter outputs. We present empirical results that, using this definition of discriminability, support our hypothesis on a variety of datasets and architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to which a layer possesses a mixture of discriminative and non-discriminative filters. We empirically demonstrate that the LDIFF score is indicative of the performance of random pruning for a given layer, and thereby indicates the extent to which a layer may be pruned. Our main contribution is a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning. We extend this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the TVSPrune on a variety of datasets, and show that in some cases, we can prune up to 60% of parameters with only a 2% loss of accuracy without any fine-tuning of the model, beating the nearest baseline by almost 10%.",
      "tldr": "We use the total variation distance between the class conditional distributions of filter outputs for structured pruning of neural networks.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10682",
      "pdf_url": "https://openreview.net/pdf?id=sZI1Oj9KBKy",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chaitanya_Murti1",
        "name": "Chaitanya Murti",
        "name_site": null,
        "openreview_id": "~Chaitanya_Murti1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Observe.AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uyqks-LILZX",
      "title": "Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11053",
      "pdf_url": "https://openreview.net/pdf?id=uyqks-LILZX",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 28,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "v8Mi8KU6056",
      "title": "wav2tok: Deep Sequence Tokenizer for Audio Retrieval",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Search over audio sequences is a fundamental problem. In this paper, we propose a method to extract concise discrete representations for audio that can be used for efficient retrieval. Our motivation comes from orthography which represents speech of a given language in a concise and distinct discrete form. The proposed method, wav2tok, learns such representations for any kind of audio, speech or non-speech, from pairs of similar audio. wav2tok compresses the query and target sequences into shorter sequences of tokens that are faster to match. The learning method makes use of CTC loss and expectation-maximization algorithm, which are generally used for supervised automatic speech recognition and for learning discrete latent variables, respectively. Experiments show the consistent performance of wav2tok across two audio retrieval tasks: music search (query by humming) and speech search via audio query, outperforming state-of-the-art baselines.",
      "tldr": "Represent query and target sequences as compressed token sequences for quick retrieval; similarity semantics are learned from sequence pairs",
      "site_url": "https://iclr.cc/virtual/2023/poster/11698",
      "pdf_url": "https://openreview.net/pdf?id=v8Mi8KU6056",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Adhiraj_Banerjee1",
        "name": "Adhiraj Banerjee",
        "name_site": "Adhiraj Banerjee, Vipul Arora",
        "openreview_id": "~Adhiraj_Banerjee1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "9de4XZEAAAAJ",
        "orcid": null,
        "linkedin_url": "adhiraj-banerjee-0051b01/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xE-LtsE-xx",
      "title": "Is Attention All That NeRF Needs?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/",
      "tldr": "We present Generalizable NeRF Transformer (GNT), a pure, unified transformer-based architecture that efficiently reconstructs Neural Radiance Fields (NeRFs) on the fly.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11493",
      "pdf_url": "https://openreview.net/pdf?id=xE-LtsE-xx",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mukund_Varma_T1",
        "name": "Mukund Varma T",
        "name_site": "Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang",
        "openreview_id": "~Mukund_Varma_T1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mukundvarmat/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 56,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yyLvxYBJV1B",
      "title": "AnyDA: Anytime Domain Adaptation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Unsupervised domain adaptation is an open and challenging problem in computer vision. While existing research shows encouraging results in addressing cross-domain distribution shift on common benchmarks, they are often constrained to testing under a specific target setting, limiting their impact for many real-world applications. In this paper, we introduce a simple yet effective framework for anytime domain adaptation that is executable with dynamic resource constraints to achieve accuracy-efficiency trade-offs under domain-shifts. We achieve this by training a single shared network using both labeled source and unlabeled data, with switchable depth, width and input resolutions on the fly to enable testing under a wide range of computation budgets. Starting with a teacher network trained from a label-rich source domain, we utilize bootstrapped recursive knowledge distillation as a nexus between source and target domains to jointly train the student network with switchable subnetworks. Experiments on multiple datasets well demonstrate the superiority of our approach over state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11817",
      "pdf_url": "https://openreview.net/pdf?id=yyLvxYBJV1B",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Omprakash_Chakraborty1",
        "name": "Omprakash Chakraborty",
        "name_site": "Omprakash Chakraborty, Aadarsh Sahoo, Rameswar Panda, Abir Das",
        "openreview_id": "~Omprakash_Chakraborty1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "182/4466.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Z0uiqiIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zH9GcZ3ZGXu",
      "title": "Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \\emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \\emph{Feature  Replication  Hypothesis}, coupled with the \\emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \\emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \\emph{FRR} in linear layer training (\\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \\emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\\% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme distribution shifts. Moreover, we demonstrate noteworthy gains over existing SOTA methods on the standard OOD benchmark DomainBed as well.",
      "tldr": "We propose a regularizer that enforces the reconstruction of features from the output logits of neural networks, in order to overcome Simplicity Bias and boost their OOD generalization.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11687",
      "pdf_url": "https://openreview.net/pdf?id=zH9GcZ3ZGXu",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sravanti_Addepalli1",
        "name": "Sravanti Addepalli",
        "name_site": null,
        "openreview_id": "~Sravanti_Addepalli1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "127/7715",
        "google_scholar_url": "MOO12i0AAAAJ",
        "orcid": null,
        "linkedin_url": "sravanti-addepalli/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATLEl_izD87",
      "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.",
      "tldr": "Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases",
      "site_url": "https://iclr.cc/virtual/2023/poster/11168",
      "pdf_url": "https://openreview.net/pdf?id=ATLEl_izD87",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CtS2Rs_aYk",
      "title": "Stay Moral and Explore: Learn to Behave Morally in Text-based Games",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Reinforcement learning (RL) in text-based games has developed rapidly and achieved promising results. However, little effort has been expended to design agents that pursue objectives while behaving morally, which is a critical issue in the field of autonomous agents. In this paper, we propose a general framework named Moral Awareness Adaptive Learning (MorAL) that enhances the morality capacity of an agent using a plugin moral-aware learning model. The framework allows the agent to execute task learning and morality learning adaptively. The agent selects trajectories from past experiences during task learning. Meanwhile, the trajectories are used to conduct self-imitation learning with a moral-enhanced objective. In order to achieve the trade-off between morality and task progress, the agent uses the combination of task policy and moral policy for action selection. We evaluate on the Jiminy Cricket benchmark, a set of text-based games with various scenes and dense morality annotations. Our experiments demonstrate that, compared with strong contemporary value alignment approaches,  the proposed framework improves task performance while reducing immoral behaviours in various games.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11130",
      "pdf_url": "https://openreview.net/pdf?id=CtS2Rs_aYk",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Nk2pDtuhTq",
      "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning $0.035\\%$ as many task-specific parameters.",
      "tldr": "We propose multitask prompt tuning which learns a single transferable prompt by decomposing and distilling knowledge from multiple task-specific source prompts.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11659",
      "pdf_url": "https://openreview.net/pdf?id=Nk2pDtuhTq",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 6,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 128,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "P5Z-Zl9XJ7",
      "title": "Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The structure of proteins involves 3D geometry of amino acid coordinates and 1D sequence of peptide chains. The 3D structure exhibits irregularity because amino acids are distributed unevenly in Euclidean space and their coordinates are continuous variables. In contrast, the 1D structure is regular because amino acids are arranged uniformly in the chains and their sequential positions (orders) are discrete variables. Moreover, geometric coordinates and sequential orders are in two types of spaces and their units of length are incompatible. These inconsistencies make it challenging to capture the 3D and 1D structures while avoiding the impact of sequence and geometry modeling on each other. This paper proposes a Continuous-Discrete Convolution (CDConv) that uses irregular and regular approaches to model the geometry and sequence structures, respectively. Specifically, CDConv employs independent learnable weights for different regular sequential displacements but directly encodes geometric displacements due to their irregularity. In this way, CDConv significantly improves protein modeling by reducing the impact of geometric irregularity on sequence modeling. Extensive experiments on a range of tasks, including protein fold classification, enzyme reaction classification, gene ontology term prediction and enzyme commission number prediction, demonstrate the effectiveness of the proposed CDConv. ",
      "tldr": "This paper proposes a Continuous-Discrete Convolution (CDConv) for the (3+1)D geometry-sequence strutuere modeling in proteins.",
      "site_url": "https://iclr.cc/virtual/2023/poster/12241",
      "pdf_url": "https://openreview.net/pdf?id=P5Z-Zl9XJ7",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 50,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "T2Ncx_PN2K",
      "title": "In-Situ Text-Only Adaptation of Speech Models with Low-Overhead Speech Imputations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Fast and accurate adaptation of automatic speech recognition (ASR) systems using only text data in the target domain is a problem of long-standing practical relevance. Text-only adaptation was easy in traditional cascaded ASR systems with completely decoupled acoustic and language models. Recently, the RNNTransducer (RNN-T) has emerged as a default ASR model because of its high accuracy, low latency, and capability of supporting streaming input. However text-only adaptation of the RNN-T model is significantly more challenging due to its tight integration of acoustic and language models and end-to-end training. Existing recent approaches for text-only adaptation of RNN-Ts, either entail significant modification to the network or introduce high latency during decoding. We propose a new approach (TOLSTOI) that imputes speech representations internal to a baseline RNN-T, starting from text-only inputs, and performs in-situ adaptation that results in higher adaptation accuracy without any runtime overheads during decoding. Our imputation model is a function of the labeled data and trained parameters of the ASR model, and that we show, is more effective in controlling catastrophic forgetting compared to existing methods. We establish the effectiveness of TOLSTOI using three target domains and two ASR models of varying complexity. We yield up to 35% relative reduction in word error rate with text-only adaptation while forgetting the least compared to existing adaptation approaches. Our method is easy to implement and can be harnessed on existing RNN-T models without requiring ASR model training from scratch.",
      "tldr": "A lightweight text-only adaptation technique for end-to-end speech recognition that is both fast and accurate.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11088",
      "pdf_url": "https://openreview.net/pdf?id=T2Ncx_PN2K",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Preethi_Jyothi2",
        "name": "Preethi Jyothi",
        "name_site": null,
        "openreview_id": "~Preethi_Jyothi2",
        "position": 3,
        "gender": "F",
        "homepage_url": "http://www.cse.iitb.ac.in/~pjyothi",
        "dblp_id": "01/9014",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=QN_uhu8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mhnHqRqcjYU",
      "title": "DFPC: Data flow driven pruning of coupled channels without data.",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Modern, multi-branched neural network architectures often possess complex interconnections between layers, which we call coupled channels (CCs). Structured pruning of CCs in these multi-branch networks is an under-researched problem, as most existing works are typically designed for pruning single-branch models like VGG-nets. While these methods yield accurate subnetworks, the improvements in inference times when applied to multi-branch networks are comparatively modest, as these methods do not prune CCs, which we observe contribute significantly to inference time. For instance, layers with CCs as input or output take more than 66% of the inference time in ResNet-50. Moreover, pruning in the data-free regime, where data is not used for pruning, is gaining traction owing to privacy concerns and computational costs associated with fine-tuning. Motivated by this, we study the problem of pruning CCs in the data-free regime. To facilitate the development of algorithms to prune CCs, we define Data Flow Couplings (DFCs) to enumerate the layers that constitute coupled connections and the associated transformation. Additionally, saliencies for pruning CCs cannot be gauged in isolation, as there may be discrepancies among the layerwise importance of CCs using conventional scoring strategies. This necessitates finding grouped saliencies to gauge the importance of all corresponding coupled elements in a network. We thus propose the Backwards Graph-based Saliency Computation (BGSC) algorithm, a data-free method that computes saliencies by estimating an upper bound to the reconstruction error of intermediate layers; we call this pruning strategy Data Flow driven Pruning of Coupled channels (DFPC). Finally, we show the efficacy of DFPC for models trained on standard datasets. Since we pruned coupled channels, we achieve up to 1.66x improvements in inference time for ResNet-101 trained on CIFAR-10 with a 5% accuracy drop without fine-tuning. With access to the ImageNet training set, we achieve significant improvements over the data-free method and see an improvement of at least 47.1% in speedup for a 2.3% accuracy drop for ResNet-50 against our baselines.",
      "tldr": "We propose a novel data-free algorithm to accelerate neural networks via pruning coupled channels.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10681",
      "pdf_url": "https://openreview.net/pdf?id=mhnHqRqcjYU",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "qco4ekz2Epm",
      "title": "Online Boundary-Free Continual Learning by Scheduled Data Prior",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Typical continual learning setup assumes that the dataset is split into multiple discrete tasks. We argue that it is less realistic as the streamed data would have no notion of task boundary in real-world data. Here, we take a step forward to investigate more realistic online continual learning – learning continuously changing data distribution without explicit task boundary, which we call boundary-free setup. As there is no clear boundary of tasks, it is not obvious when and what information in the past to be preserved as a better remedy for the stability-plasticity dilemma. To this end, we propose a scheduled transfer of previously learned knowledge. We further propose a data-driven balancing between the knowledge in the past and the present in learning objective. Moreover, since it is not straight-forward to use the previously proposed forgetting measure without task boundaries, we further propose a novel forgetting measure based on information theory that can capture forgetting. We empirically evaluate our method on a Gaussian data stream, its periodic extension, which assumes periodic data distribution frequently observed in real-life data, as well as the conventional disjoint task-split. Our method outperforms prior arts by large margins in various setups, using four popular benchmark datasets – CIFAR-10, CIFAR-100, TinyImageNet and ImageNet.",
      "tldr": "We propose a new continual learning setup without explicit task boundary and a method to address it.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11136",
      "pdf_url": "https://openreview.net/pdf?id=qco4ekz2Epm",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Yonsei University (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sZI1Oj9KBKy",
      "title": "TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Achieving structured, data-free sparsity of deep neural networks (DNNs) remains an open area of research.  In this work, we address the challenge of pruning filters without access to the original training set or loss function. We propose the discriminative filters hypothesis, that well-trained models possess discriminative filters, and any non-discriminative filters can be pruned without impacting the predictive performance of the classifier. Based on this hypothesis, we propose a new paradigm for pruning neural networks: distributional pruning, wherein we only require access to the distributions that generated the original datasets. Our approach to solving the problem of formalising and quantifying the discriminating ability of filters is through the total variation (TV) distance between the class-conditional distributions of the filter outputs. We present empirical results that, using this definition of discriminability, support our hypothesis on a variety of datasets and architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to which a layer possesses a mixture of discriminative and non-discriminative filters. We empirically demonstrate that the LDIFF score is indicative of the performance of random pruning for a given layer, and thereby indicates the extent to which a layer may be pruned. Our main contribution is a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning. We extend this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the TVSPrune on a variety of datasets, and show that in some cases, we can prune up to 60% of parameters with only a 2% loss of accuracy without any fine-tuning of the model, beating the nearest baseline by almost 10%.",
      "tldr": "We use the total variation distance between the class conditional distributions of filter outputs for structured pruning of neural networks.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10682",
      "pdf_url": "https://openreview.net/pdf?id=sZI1Oj9KBKy",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "v8Mi8KU6056",
      "title": "wav2tok: Deep Sequence Tokenizer for Audio Retrieval",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Search over audio sequences is a fundamental problem. In this paper, we propose a method to extract concise discrete representations for audio that can be used for efficient retrieval. Our motivation comes from orthography which represents speech of a given language in a concise and distinct discrete form. The proposed method, wav2tok, learns such representations for any kind of audio, speech or non-speech, from pairs of similar audio. wav2tok compresses the query and target sequences into shorter sequences of tokens that are faster to match. The learning method makes use of CTC loss and expectation-maximization algorithm, which are generally used for supervised automatic speech recognition and for learning discrete latent variables, respectively. Experiments show the consistent performance of wav2tok across two audio retrieval tasks: music search (query by humming) and speech search via audio query, outperforming state-of-the-art baselines.",
      "tldr": "Represent query and target sequences as compressed token sequences for quick retrieval; similarity semantics are learned from sequence pairs",
      "site_url": "https://iclr.cc/virtual/2023/poster/11698",
      "pdf_url": "https://openreview.net/pdf?id=v8Mi8KU6056",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vipul_Arora1",
        "name": "Vipul Arora",
        "name_site": null,
        "openreview_id": "~Vipul_Arora1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~vipular",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=SC9YYPAAAAAJ",
        "orcid": "0000-0002-1207-1258",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yyLvxYBJV1B",
      "title": "AnyDA: Anytime Domain Adaptation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Unsupervised domain adaptation is an open and challenging problem in computer vision. While existing research shows encouraging results in addressing cross-domain distribution shift on common benchmarks, they are often constrained to testing under a specific target setting, limiting their impact for many real-world applications. In this paper, we introduce a simple yet effective framework for anytime domain adaptation that is executable with dynamic resource constraints to achieve accuracy-efficiency trade-offs under domain-shifts. We achieve this by training a single shared network using both labeled source and unlabeled data, with switchable depth, width and input resolutions on the fly to enable testing under a wide range of computation budgets. Starting with a teacher network trained from a label-rich source domain, we utilize bootstrapped recursive knowledge distillation as a nexus between source and target domains to jointly train the student network with switchable subnetworks. Experiments on multiple datasets well demonstrate the superiority of our approach over state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11817",
      "pdf_url": "https://openreview.net/pdf?id=yyLvxYBJV1B",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_Das4",
        "name": "Abir Das",
        "name_site": null,
        "openreview_id": "~Abir_Das4",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~adas/",
        "dblp_id": "141/1311",
        "google_scholar_url": "L4yEk2UAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sCYXJr3QJM8",
      "title": "Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.",
      "tldr": "Adapt a GAN trained on a single large-scale source dataset to multiple target domains containing very few examples without re-training the pretrained source generator.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11530",
      "pdf_url": "https://openreview.net/pdf?id=sCYXJr3QJM8",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Tiwary1",
        "name": "Piyush Tiwary",
        "name_site": null,
        "openreview_id": "~Piyush_Tiwary1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://backpropagator.github.io/",
        "dblp_id": null,
        "google_scholar_url": "tUdHYloAAAAJ",
        "orcid": "0000-0002-4499-1059",
        "linkedin_url": "thebackpropogator/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 5.000000000000001,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_BoPed4tYww",
      "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Many real-world settings involve costs for performing actions; transaction costs\nin financial systems and fuel costs being common examples. In these settings,\nperforming actions at each time step quickly accumulates costs leading to vastly\nsuboptimal outcomes. Additionally, repeatedly acting produces wear and tear and\nultimately, damage. Determining when to act is crucial for achieving successful\noutcomes and yet, the challenge of efficiently learning to behave optimally when\nactions incur minimally bounded costs remains unresolved. In this paper, we intro-\nduce a reinforcement learning (RL) framework named Learnable Impulse Control\nReinforcement Algorithm (LICRA), for learning to optimally select both when\nto act and which actions to take when actions incur costs. At the core of LICRA\nis a nested structure that combines RL and a form of policy known as impulse\ncontrol which learns to maximise objectives when actions incur costs. We prove\nthat LICRA, which seamlessly adopts any RL method, converges to policies that\noptimally select when to perform actions and their optimal magnitudes. We then\naugment LICRA to handle problems in which the agent can perform at most k < ∞\nactions and more generally, faces a budget constraint. We show LICRA learns the\noptimal value function and ensures budget constraints are satisfied almost surely.\nWe demonstrate empirically LICRA’s superior performance against benchmark\nRL methods in OpenAI gym’s Lunar Lander and in Highway environments and a\nvariant of the Merton portfolio problem within finance.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11201",
      "pdf_url": "https://openreview.net/pdf?id=_BoPed4tYww",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Byju's (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATLEl_izD87",
      "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.",
      "tldr": "Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases",
      "site_url": "https://iclr.cc/virtual/2023/poster/11168",
      "pdf_url": "https://openreview.net/pdf?id=ATLEl_izD87",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ravinder_Bhattoo1",
        "name": "Ravinder Bhattoo",
        "name_site": null,
        "openreview_id": "~Ravinder_Bhattoo1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ravinderbhattoo.github.io",
        "dblp_id": null,
        "google_scholar_url": "lPTdGRMAAAAJ",
        "orcid": "0000-0003-0323-9108",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Indore (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hY6M0JHl3uL",
      "title": "Linear Connectivity Reveals Generalization Strategies",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In the mode connectivity literature, it is widely accepted that there are common circumstances in which two neural networks, trained similarly on the same data, will maintain loss when interpolated in the weight space. In particular, transfer learning is presumed to ensure the necessary conditions for linear mode connectivity across training runs. In contrast to existing results from image classification, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster---models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we find that these clusters correspond to different generalization strategies. For example, on MNLI, one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions in standard finetuning settings.",
      "tldr": "Basins on the in-domain test loss surface predict generalization strategies for NLI, paraphrase, and CoLA tasks.",
      "site_url": "https://iclr.cc/virtual/2023/poster/12031",
      "pdf_url": "https://openreview.net/pdf?id=hY6M0JHl3uL",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rachit_Bansal1",
        "name": "Rachit Bansal",
        "name_site": null,
        "openreview_id": "~Rachit_Bansal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://rachitbansal.github.io",
        "dblp_id": "228/6038",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7-x28WYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 54,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hp_RwhKDJ5",
      "title": "Learning to Induce Causal Structure",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and evaluating them using either score-based methods (including continuous optimization) or independence tests. In our work, we instead treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. The learned model generalizes to new synthetic graphs, is robust to train-test distribution shifts, and achieves state-of-the-art performance on naturalistic graphs for low sample complexity.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/11132",
      "pdf_url": "https://openreview.net/pdf?id=hp_RwhKDJ5",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 59,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ndYXTEL6cZz",
      "title": "Extremely Simple Activation Shaping for Out-of-Distribution Detection",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model’s ability to handle unseen situations: Do models know when they don’t know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample’s activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out- of-distribution sample distinction so as to allow state-of-the-art OOD detection on ImageNet, and does not noticeably deteriorate the in-distribution accuracy. Video, animation and code can be found at: https://andrijazz.github.io/ash.",
      "tldr": "We develop an extremely simple, post hoc, on-the-fly, and plug-and-play activation shaping method for out-of-distribution detection.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11506",
      "pdf_url": "https://openreview.net/pdf?id=ndYXTEL6cZz",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nebojsa_Bozanic1",
        "name": "Nebojsa Bozanic",
        "name_site": null,
        "openreview_id": "~Arjun_Ashok1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ashok-arjun.github.io/",
        "dblp_id": "https://dblp.uni-trier.de/pid/318/2945",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "PSG College of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 190,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sCYXJr3QJM8",
      "title": "Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning",
      "status": "Top-25%",
      "normalized_status": "spotlight",
      "abstract": "In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.",
      "tldr": "Adapt a GAN trained on a single large-scale source dataset to multiple target domains containing very few examples without re-training the pretrained source generator.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11530",
      "pdf_url": "https://openreview.net/pdf?id=sCYXJr3QJM8",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5000000000000004,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3nM5uhPlfv6",
      "title": "Stochastic Differentially Private and Fair Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic\" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.",
      "tldr": "The first efficient differentially private fair learning algorithm that is guaranteed to converge, even when stochastic minibatches of data are used in each iteration of training. ",
      "site_url": "https://iclr.cc/virtual/2023/poster/12184",
      "pdf_url": "https://openreview.net/pdf?id=3nM5uhPlfv6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Devansh_Gupta1",
        "name": "Devansh Gupta",
        "name_site": null,
        "openreview_id": "~Devansh_Gupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?view_op=list_works",
        "orcid": null,
        "linkedin_url": "devansh-g-784842106/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 17,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATLEl_izD87",
      "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.",
      "tldr": "Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases",
      "site_url": "https://iclr.cc/virtual/2023/poster/11168",
      "pdf_url": "https://openreview.net/pdf?id=ATLEl_izD87",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayadeva_Jayadeva1",
        "name": "Jayadeva Jayadeva",
        "name_site": null,
        "openreview_id": "~Jayadeva_Jayadeva1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "58/4288",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "B4maZQLLW0_",
      "title": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In cooperative multi-agent reinforcement learning, a team of agents works together\nto achieve a common goal. Different environments or tasks may require varying\ndegrees of coordination among agents in order to achieve the goal in an optimal\nway. The nature of coordination will depend on properties of the environment—its\nspatial layout, distribution of obstacles, dynamics, etc. We term this variation\nof properties within an environment as heterogeneity. Existing literature has not\nsufficiently addressed the fact that different environments may have different levels\nof heterogeneity. We formalize the notions of coordination level and heterogeneity\nlevel of an environment and present HECOGrid, a suite of multi-agent RL\nenvironments that facilitates empirical evaluation of different MARL approaches\nacross different levels of coordination and environmental heterogeneity by providing\na quantitative control over coordination and heterogeneity levels of the\nenvironment. Further, we propose a Centralized Training Decentralized Execution\nlearning approach called Stateful Active Facilitator (SAF) that enables agents to\nwork efficiently in high-coordination and high-heterogeneity environments through\na differentiable and shared knowledge source used during training and dynamic\nselection from a shared pool of policies. We evaluate SAF and compare its performance\nagainst baselines IPPO and MAPPO on HECOGrid. Our results show\nthat SAF consistently outperforms the baselines across different tasks and different\nheterogeneity and coordination levels.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2023/poster/10810",
      "pdf_url": "https://openreview.net/pdf?id=B4maZQLLW0_",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "T2Ncx_PN2K",
      "title": "In-Situ Text-Only Adaptation of Speech Models with Low-Overhead Speech Imputations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Fast and accurate adaptation of automatic speech recognition (ASR) systems using only text data in the target domain is a problem of long-standing practical relevance. Text-only adaptation was easy in traditional cascaded ASR systems with completely decoupled acoustic and language models. Recently, the RNNTransducer (RNN-T) has emerged as a default ASR model because of its high accuracy, low latency, and capability of supporting streaming input. However text-only adaptation of the RNN-T model is significantly more challenging due to its tight integration of acoustic and language models and end-to-end training. Existing recent approaches for text-only adaptation of RNN-Ts, either entail significant modification to the network or introduce high latency during decoding. We propose a new approach (TOLSTOI) that imputes speech representations internal to a baseline RNN-T, starting from text-only inputs, and performs in-situ adaptation that results in higher adaptation accuracy without any runtime overheads during decoding. Our imputation model is a function of the labeled data and trained parameters of the ASR model, and that we show, is more effective in controlling catastrophic forgetting compared to existing methods. We establish the effectiveness of TOLSTOI using three target domains and two ASR models of varying complexity. We yield up to 35% relative reduction in word error rate with text-only adaptation while forgetting the least compared to existing adaptation approaches. Our method is easy to implement and can be harnessed on existing RNN-T models without requiring ASR model training from scratch.",
      "tldr": "A lightweight text-only adaptation technique for end-to-end speech recognition that is both fast and accurate.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11088",
      "pdf_url": "https://openreview.net/pdf?id=T2Ncx_PN2K",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sunita_Sarawagi1",
        "name": "Sunita Sarawagi",
        "name_site": null,
        "openreview_id": "~Sunita_Sarawagi1",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://www.cse.iitb.ac.in/~sunita/",
        "dblp_id": "s/SunitaSarawagi",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=Hg4HmTAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kDEL91Dufpa",
      "title": "On the duality between contrastive and non-contrastive self-supervised learning",
      "status": "Top-5%",
      "normalized_status": "oral",
      "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",
      "tldr": "We show that contrastive and non-contrastive self-supervised methods can be shown to be closely related, and then study how implementation details impact performance. We validate empirically our findings and significantly improve known behaviours.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10958",
      "pdf_url": "https://openreview.net/pdf?id=kDEL91Dufpa",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laurent_Najman1",
        "name": "Laurent Najman",
        "name_site": null,
        "openreview_id": "~Laurent_Najman1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://laurentnajman.org",
        "dblp_id": "68/4192",
        "google_scholar_url": "https://scholar.google.fr/citations?user=j-2_cT0AAAAJ",
        "orcid": "0000-0002-6190-0235",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ecole Supérieure d'Ingénieurs en Electronique et Electrotechnique (France)",
        "countries": [
          "France"
        ],
        "country_codes": [
          "FR"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 116,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mhnHqRqcjYU",
      "title": "DFPC: Data flow driven pruning of coupled channels without data.",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Modern, multi-branched neural network architectures often possess complex interconnections between layers, which we call coupled channels (CCs). Structured pruning of CCs in these multi-branch networks is an under-researched problem, as most existing works are typically designed for pruning single-branch models like VGG-nets. While these methods yield accurate subnetworks, the improvements in inference times when applied to multi-branch networks are comparatively modest, as these methods do not prune CCs, which we observe contribute significantly to inference time. For instance, layers with CCs as input or output take more than 66% of the inference time in ResNet-50. Moreover, pruning in the data-free regime, where data is not used for pruning, is gaining traction owing to privacy concerns and computational costs associated with fine-tuning. Motivated by this, we study the problem of pruning CCs in the data-free regime. To facilitate the development of algorithms to prune CCs, we define Data Flow Couplings (DFCs) to enumerate the layers that constitute coupled connections and the associated transformation. Additionally, saliencies for pruning CCs cannot be gauged in isolation, as there may be discrepancies among the layerwise importance of CCs using conventional scoring strategies. This necessitates finding grouped saliencies to gauge the importance of all corresponding coupled elements in a network. We thus propose the Backwards Graph-based Saliency Computation (BGSC) algorithm, a data-free method that computes saliencies by estimating an upper bound to the reconstruction error of intermediate layers; we call this pruning strategy Data Flow driven Pruning of Coupled channels (DFPC). Finally, we show the efficacy of DFPC for models trained on standard datasets. Since we pruned coupled channels, we achieve up to 1.66x improvements in inference time for ResNet-101 trained on CIFAR-10 with a 5% accuracy drop without fine-tuning. With access to the ImageNet training set, we achieve significant improvements over the data-free method and see an improvement of at least 47.1% in speedup for a 2.3% accuracy drop for ResNet-50 against our baselines.",
      "tldr": "We propose a novel data-free algorithm to accelerate neural networks via pruning coupled channels.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10681",
      "pdf_url": "https://openreview.net/pdf?id=mhnHqRqcjYU",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chaitanya_Murti1",
        "name": "Chaitanya Murti",
        "name_site": null,
        "openreview_id": "~Chaitanya_Murti1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sZI1Oj9KBKy",
      "title": "TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Achieving structured, data-free sparsity of deep neural networks (DNNs) remains an open area of research.  In this work, we address the challenge of pruning filters without access to the original training set or loss function. We propose the discriminative filters hypothesis, that well-trained models possess discriminative filters, and any non-discriminative filters can be pruned without impacting the predictive performance of the classifier. Based on this hypothesis, we propose a new paradigm for pruning neural networks: distributional pruning, wherein we only require access to the distributions that generated the original datasets. Our approach to solving the problem of formalising and quantifying the discriminating ability of filters is through the total variation (TV) distance between the class-conditional distributions of the filter outputs. We present empirical results that, using this definition of discriminability, support our hypothesis on a variety of datasets and architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to which a layer possesses a mixture of discriminative and non-discriminative filters. We empirically demonstrate that the LDIFF score is indicative of the performance of random pruning for a given layer, and thereby indicates the extent to which a layer may be pruned. Our main contribution is a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning. We extend this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the TVSPrune on a variety of datasets, and show that in some cases, we can prune up to 60% of parameters with only a 2% loss of accuracy without any fine-tuning of the model, beating the nearest baseline by almost 10%.",
      "tldr": "We use the total variation distance between the class conditional distributions of filter outputs for structured pruning of neural networks.",
      "site_url": "https://iclr.cc/virtual/2023/poster/10682",
      "pdf_url": "https://openreview.net/pdf?id=sZI1Oj9KBKy",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tanay_Narshana1",
        "name": "Tanay Narshana",
        "name_site": "Tanay Narshana, Chaitanya Murti, Chiranjib Bhattacharyya",
        "openreview_id": "~Tanay_Narshana1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "350/3876",
        "google_scholar_url": "d4YFxEcAAAAJ",
        "orcid": null,
        "linkedin_url": "tanay-narshana/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zH9GcZ3ZGXu",
      "title": "Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \\emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \\emph{Feature  Replication  Hypothesis}, coupled with the \\emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \\emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \\emph{FRR} in linear layer training (\\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \\emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\\% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme distribution shifts. Moreover, we demonstrate noteworthy gains over existing SOTA methods on the standard OOD benchmark DomainBed as well.",
      "tldr": "We propose a regularizer that enforces the reconstruction of features from the output logits of neural networks, in order to overcome Simplicity Bias and boost their OOD generalization.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11687",
      "pdf_url": "https://openreview.net/pdf?id=zH9GcZ3ZGXu",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CUOaVn6mYEj",
      "title": "Hierarchical Sliced Wasserstein Distance",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying  Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.",
      "tldr": "The paper proposes hierarchical sliced Wasserstein distance which is faster than the conventional sliced Wasserstein distance.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11704",
      "pdf_url": "https://openreview.net/pdf?id=CUOaVn6mYEj",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Litu_Rout1",
        "name": "Litu Rout",
        "name_site": "Litu Rout, Alexander Korotin, Evgeny Burnaev",
        "openreview_id": "~Litu_Rout1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://liturout.github.io/",
        "dblp_id": "206/6445",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": null,
        "linkedin_url": "litu-rout-sac-isro/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Texas at Austin (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 29,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ATLEl_izD87",
      "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.",
      "tldr": "Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases",
      "site_url": "https://iclr.cc/virtual/2023/poster/11168",
      "pdf_url": "https://openreview.net/pdf?id=ATLEl_izD87",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.25,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yf1icZHC-l9",
      "title": "Complexity-Based Prompting for Multi-step Reasoning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority\nof generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
      "tldr": "We show using prompts with more reasoning steps can improve language models multi-step reasoning ability ",
      "site_url": "https://iclr.cc/virtual/2023/poster/11280",
      "pdf_url": "https://openreview.net/pdf?id=yf1icZHC-l9",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Khot1",
        "name": "Tushar Khot",
        "name_site": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal",
        "openreview_id": "~Tushar_Khot1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://allenai.org/team/tushark/",
        "dblp_id": "83/8117",
        "google_scholar_url": "_8mkIjgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Allen Institute for Artificial Intelligence (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 1.25,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 416,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dZrQR7OR11",
      "title": "Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.",
      "tldr": "This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP) and studies algorithmic considerations to scale up classic expectation propagation to modern federated learning settings.",
      "site_url": "https://iclr.cc/virtual/2023/poster/11160",
      "pdf_url": "https://openreview.net/pdf?id=dZrQR7OR11",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 5,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 21,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "-p5ZEVGtojQ",
      "title": "Continuous Depth Recurrent Neural Differential Equations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recurrent neural networks (RNNs) have brought a lot of advancements in sequence labeling tasks and sequence data. However, their effectiveness is limited  when the observations in the sequence are irregularly sampled, where the observations arrive at irregular time intervals. To address this, continuous time variants of the RNNs  were introduced based on neural  ordinary differential equations (NODE). They  learn a better representation of the data using the continuous transformation of hidden states over time, taking into account the time interval between the observations. However, they are still limited in their capability as they use the discrete transformations and discrete number of layers (depth) over an  input in the sequence to produce the output observation. We intend to address this limitation by proposing  RNNs based on  differential equations which model continuous  transformations over depth and time to predict an output for a given input in the sequence. Specifically, we propose continuous depth recurrent neural  differential equations (CDR-NDE) which generalizes  RNN models by continuously evolving the hidden states in both  the temporal and depth dimensions. CDR-NDE considers two separate differential equations over each of these dimensions and models the evolution in  the temporal and depth directions alternatively. We also propose the CDR-NDE-heat model based on partial differential equations which treats the computation of hidden states as solving a heat equation over time.  We demonstrate the effectiveness of the proposed models by comparing against the  state-of-the-art RNN models on  real world sequence modeling problems and data sets.",
      "tldr": "Proposing novel RNN models based on differential equations that  continuously transform hidden states in both temporal and depth dimensions.",
      "site_url": "https://openreview.net/forum?id=-p5ZEVGtojQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Geetakrishnasai_Gunapati1",
        "name": "Geetakrishnasai Gunapati",
        "name_site": null,
        "openreview_id": "~Geetakrishnasai_Gunapati1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "www.linkedin.com/in/geetakrishnasai-gunapati-889882a3",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "-p5ZEVGtojQ",
      "title": "Continuous Depth Recurrent Neural Differential Equations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recurrent neural networks (RNNs) have brought a lot of advancements in sequence labeling tasks and sequence data. However, their effectiveness is limited  when the observations in the sequence are irregularly sampled, where the observations arrive at irregular time intervals. To address this, continuous time variants of the RNNs  were introduced based on neural  ordinary differential equations (NODE). They  learn a better representation of the data using the continuous transformation of hidden states over time, taking into account the time interval between the observations. However, they are still limited in their capability as they use the discrete transformations and discrete number of layers (depth) over an  input in the sequence to produce the output observation. We intend to address this limitation by proposing  RNNs based on  differential equations which model continuous  transformations over depth and time to predict an output for a given input in the sequence. Specifically, we propose continuous depth recurrent neural  differential equations (CDR-NDE) which generalizes  RNN models by continuously evolving the hidden states in both  the temporal and depth dimensions. CDR-NDE considers two separate differential equations over each of these dimensions and models the evolution in  the temporal and depth directions alternatively. We also propose the CDR-NDE-heat model based on partial differential equations which treats the computation of hidden states as solving a heat equation over time.  We demonstrate the effectiveness of the proposed models by comparing against the  state-of-the-art RNN models on  real world sequence modeling problems and data sets.",
      "tldr": "Proposing novel RNN models based on differential equations that  continuously transform hidden states in both temporal and depth dimensions.",
      "site_url": "https://openreview.net/forum?id=-p5ZEVGtojQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~P._K._Srijith1",
        "name": "P. K. Srijith",
        "name_site": null,
        "openreview_id": "~Srijith_P_K1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/pksrijith/home",
        "dblp_id": "120/8712",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=C1YpEWsAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "05ff9BRSMzE",
      "title": "Gandalf : Data Augmentation is all you need for Extreme Classification",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on the problem setting with short-text input data, and labels endowed with short textual descriptions called label features. Short-text XMC with label features has found numerous applications in areas such as prediction of related searches, title-based product recommendation, bid-phrase suggestion, amongst others. In this paper, we propose Gandalf, a graph induced data augmentation based on label features, such that the generated data-points can supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. While most recent advances (such as SiameseXML and ECLARE) in XMC have been algorithmic, mainly aimed towards developing novel deep-learning architectures, our data-centric augmentation approach is orthogonal to these methodologies. We demonstrate the generality and effectiveness of Gandalf by showing up to 30% relative improvements for 5 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3 million labels. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=05ff9BRSMzE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Devaansh_Gupta1",
        "name": "Devaansh Gupta",
        "name_site": null,
        "openreview_id": "~Devaansh_Gupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://devaansh100.github.io",
        "dblp_id": "351/9786",
        "google_scholar_url": "lSBqiz4AAAAJ",
        "orcid": null,
        "linkedin_url": "devaanshgupta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0OlEBibFa_g",
      "title": "Detecting Out-of-Distribution Data with Semi-supervised Graph “Feature\" Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Anomalous and out-of-distribution (OOD) data present a significant challenge to the robustness of decisions taken by deep neural networks, with myriad real-world consequences. State-of-the-art OOD detection techniques use embeddings learned by large pre-trained transformers. We demonstrate that graph structures and topological properties can be leveraged to detect both far-OOD and near-OOD data reliably, simply by characterising each data point (image) as a network of related features (visual concepts). Furthermore, we facilitate human-in-the-loop machine learning by expressing this data to comprise high-level domain-specific concepts. We obtained \\textit{97.95\\% AUROC} on far-OOD and \\textit{98.79\\% AUROC} on near-OOD detection tasks based on the LSUN dataset (comparable to the performance of state-of-the-art techniques).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=0OlEBibFa_g",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debayan_Gupta1",
        "name": "Debayan Gupta",
        "name_site": null,
        "openreview_id": "~Debayan_Gupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.ashoka.edu.in/profile/debayan-gupta/",
        "dblp_id": "121/4322.html",
        "google_scholar_url": "Z16kmr8AAAAJ",
        "orcid": "0000-0002-4457-1556",
        "linkedin_url": "debayang",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ashoka University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0c2SbGJ3Lt",
      "title": "Textless Phrase Structure Induction from Visually-Grounded Speech",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We study phrase structure induction from visually-grounded speech without intermediate text or text pre-trained models. The core idea is to first segment the speech waveform into sequences of word segments, then induce phrase structure based on the inferred segment-level continuous representations. To this end, we present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns non-trivial phrase structure by listening to audio and looking at images, without ever reading text. Experiments on SpokenCOCO, the spoken version of MSCOCO with paired images and spoken captions, show that AV-NSL infers meaningful phrase structures similar to those learned from naturally-supervised text parsing, quantitatively and qualitatively. The findings in this paper extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and manifest one possibility of bridging the gap between the two fields.",
      "tldr": "The first study on grammar induction from audio-visual inputs, without relying on intermediate text or ASR. ",
      "site_url": "https://openreview.net/forum?id=0c2SbGJ3Lt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 13,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 4,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0g1JdUJF7Fr",
      "title": "An Optimal Transport Perspective on Unpaired Image Super-Resolution",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Real-world image super-resolution (SR) tasks often do not have paired\ndatasets, which limits the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs), which yield complex training losses with several regularization terms, e.g., content or identity losses. We theoretically investigate optimization problems which arise in such models and find two surprizing observations. First, the learned SR map is always an optimal transport (OT) map. Second, we theoretically prove and empirically show that the learned map is biased, i.e., it does not actually transform the distribution of low-resolution images to high-resolution ones. Inspired by these findings, we propose an algorithm for unpaired SR which learns an unbiased OT map for the perceptual transport cost. Unlike the existing GAN-based alternatives, our algorithm has a simple optimization objective reducing the need for complex  hyperparameter selection and an application of additional regularizations. At the same time, it provides a nearly state-of-the-art performance on the large-scale unpaired AIM19 dataset.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=0g1JdUJF7Fr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Litu_Rout1",
        "name": "Litu Rout",
        "name_site": "Litu Rout, Alexander Korotin, Evgeny Burnaev",
        "openreview_id": "~Litu_Rout1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://liturout.github.io/",
        "dblp_id": "206/6445",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": null,
        "linkedin_url": "litu-rout-sac-isro/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Texas at Austin (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1uPo_IrEp8",
      "title": "Online Reinforcement Learning via Posterior Sampling of Policy",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We propose a Reward-Weighted Posterior Sampling of Policy (RWPSP) algorithm to tackle the classic trade-off problem between exploration and exploitation under finite Markov decision processes (MDPs). The Thompson sampling method so far has only considered posterior sampling over transition probabilities, which is hard to gain the globally sub-optimal rewards. RWPSP runs posterior sampling over stationary policy distributions instead of transition probabilities, and meanwhile keeps transition probabilities updated.  Particularly, we leverage both relevant count functions and reward-weighting to online update the policy posterior, aiming to balance between local and long-term policy distributions for a globally near-optimal game value. Theoretically, we establish a bound of $\\tilde{\\mathcal{O}}(\\Gamma\\sqrt{T}/S^{2})$\\footnote{The symbol $\\tilde{\\mathcal{O}}$ hides logarithmic factors.} on the total regret in time horizon $T$ with $\\Gamma/S^{2} < D\\sqrt{SA}$ satisfied in general, where $S$ and $A$ represents the sizes of state and action spaces, respectively, $D$ the diameter. This matches the best regret bound thus far for MDPs. Experimental results corroborate our theoretical results and show the advantage of our algorithm over the state of the art in terms of efficiency.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1uPo_IrEp8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 1.479019945774904,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1uPo_IrEp8",
      "title": "Online Reinforcement Learning via Posterior Sampling of Policy",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We propose a Reward-Weighted Posterior Sampling of Policy (RWPSP) algorithm to tackle the classic trade-off problem between exploration and exploitation under finite Markov decision processes (MDPs). The Thompson sampling method so far has only considered posterior sampling over transition probabilities, which is hard to gain the globally sub-optimal rewards. RWPSP runs posterior sampling over stationary policy distributions instead of transition probabilities, and meanwhile keeps transition probabilities updated.  Particularly, we leverage both relevant count functions and reward-weighting to online update the policy posterior, aiming to balance between local and long-term policy distributions for a globally near-optimal game value. Theoretically, we establish a bound of $\\tilde{\\mathcal{O}}(\\Gamma\\sqrt{T}/S^{2})$\\footnote{The symbol $\\tilde{\\mathcal{O}}$ hides logarithmic factors.} on the total regret in time horizon $T$ with $\\Gamma/S^{2} < D\\sqrt{SA}$ satisfied in general, where $S$ and $A$ represents the sizes of state and action spaces, respectively, $D$ the diameter. This matches the best regret bound thus far for MDPs. Experimental results corroborate our theoretical results and show the advantage of our algorithm over the state of the art in terms of efficiency.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1uPo_IrEp8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Electronic Science and Technology of China (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 1.479019945774904,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1zaoVA_z8Q",
      "title": "SemSup-XC: Semantic Supervision for Extreme Classification",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Extreme classification (XC) considers the scenario of predicting over a very large number of classes (thousands to millions), with real-world applications including serving search engine results, e-commerce product tagging, and news article  classification. The zero-shot version of this task involves the addition of new categories at test time, requiring models to generalize to novel classes without\nadditional training data (e.g. one may add a new class “fidget spinner” for ecommerce product tagging). In this paper, we develop SEMSUP-XC, a model that achieves state-of-the-art zero-shot (ZS) and few-shot (FS) performance on three extreme classification benchmarks spanning the domains of law, e-commerce, and Wikipedia. SEMSUP-XC builds upon the recently proposed framework of semantic supervision that uses semantic label descriptions to represent and generalize to classes (e.g., “fidget spinner” described as “A popular spinning toy intended as a stress reliever”). Specifically, we use a combination of contrastive learning, a hybrid lexico-semantic similarity module and automated description collection to train SEMSUP-XC efficiently over extremely large class spaces. SEMSUP-XC\nsignificantly outperforms baselines and state-of-the-art models on all three datasets, by up to 6-10 precision@1 points on zero-shot classification and >10 precision points on few-shot classification, with similar gains for recall@10 (3 for zero-shot and 2 for few-shot). Our ablation studies and qualitative analyses demonstrate the relative importance of our various improvements and show that SEMSUP-XC’s\nautomated pipeline offers a consistently efficient method for extreme classification.",
      "tldr": "We propose a new model for extreme classification over very large label spaces and achieve SOTA results on three popular benchmarks.",
      "site_url": "https://openreview.net/forum?id=1zaoVA_z8Q",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranjal_Aggarwal1",
        "name": "Pranjal Aggarwal",
        "name_site": null,
        "openreview_id": "~Pranjal_Aggarwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/Pranjal2041/",
        "dblp_id": "163/0764",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-2962-1535",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36g8Ept_CCj",
      "title": "Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a new framework of learning mixture models via data partitioning called PRESTO, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific partition. We connect PRESTO to a number of past works in data partitioning, mixture models, and clustering, and show that PRESTO generalizes several loss functions including the k-means and Bregman clustering objective, the Gaussian mixture model objective, mixtures of support vector machines, and mixtures of linear regression. We then propose a new joint discrete-continuous optimization algorithm which achieves a bounded approximation guarantee for any general loss function, thereby achieving guarantees for the afore-mentioned problems as well. We study PRESTO in the context of resource efficient deep learning, where we train smaller resource constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to PRESTO, require large initial (teacher) models.",
      "tldr": "PRESTO learns a mixture models such that each model performs well on a data partition",
      "site_url": "https://openreview.net/forum?id=36g8Ept_CCj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parth_Vipul_Sangani1",
        "name": "Parth Vipul Sangani",
        "name_site": null,
        "openreview_id": "~Parth_Vipul_Sangani1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "parth-sangani-b34b59165",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36g8Ept_CCj",
      "title": "Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a new framework of learning mixture models via data partitioning called PRESTO, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific partition. We connect PRESTO to a number of past works in data partitioning, mixture models, and clustering, and show that PRESTO generalizes several loss functions including the k-means and Bregman clustering objective, the Gaussian mixture model objective, mixtures of support vector machines, and mixtures of linear regression. We then propose a new joint discrete-continuous optimization algorithm which achieves a bounded approximation guarantee for any general loss function, thereby achieving guarantees for the afore-mentioned problems as well. We study PRESTO in the context of resource efficient deep learning, where we train smaller resource constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to PRESTO, require large initial (teacher) models.",
      "tldr": "PRESTO learns a mixture models such that each model performs well on a data partition",
      "site_url": "https://openreview.net/forum?id=36g8Ept_CCj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arjun_Shashank_Kashettiwar1",
        "name": "Arjun Shashank Kashettiwar",
        "name_site": null,
        "openreview_id": "~Arjun_Shashank_Kashettiwar1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "arjun-kashettiwar-05748a173",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36g8Ept_CCj",
      "title": "Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a new framework of learning mixture models via data partitioning called PRESTO, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific partition. We connect PRESTO to a number of past works in data partitioning, mixture models, and clustering, and show that PRESTO generalizes several loss functions including the k-means and Bregman clustering objective, the Gaussian mixture model objective, mixtures of support vector machines, and mixtures of linear regression. We then propose a new joint discrete-continuous optimization algorithm which achieves a bounded approximation guarantee for any general loss function, thereby achieving guarantees for the afore-mentioned problems as well. We study PRESTO in the context of resource efficient deep learning, where we train smaller resource constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to PRESTO, require large initial (teacher) models.",
      "tldr": "PRESTO learns a mixture models such that each model performs well on a data partition",
      "site_url": "https://openreview.net/forum?id=36g8Ept_CCj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Durga_S1",
        "name": "Durga S",
        "name_site": null,
        "openreview_id": "~Durga_S1",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "4JXFWTwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36g8Ept_CCj",
      "title": "Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a new framework of learning mixture models via data partitioning called PRESTO, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific partition. We connect PRESTO to a number of past works in data partitioning, mixture models, and clustering, and show that PRESTO generalizes several loss functions including the k-means and Bregman clustering objective, the Gaussian mixture model objective, mixtures of support vector machines, and mixtures of linear regression. We then propose a new joint discrete-continuous optimization algorithm which achieves a bounded approximation guarantee for any general loss function, thereby achieving guarantees for the afore-mentioned problems as well. We study PRESTO in the context of resource efficient deep learning, where we train smaller resource constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to PRESTO, require large initial (teacher) models.",
      "tldr": "PRESTO learns a mixture models such that each model performs well on a data partition",
      "site_url": "https://openreview.net/forum?id=36g8Ept_CCj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ganesh_Ramakrishnan1",
        "name": "Ganesh Ramakrishnan",
        "name_site": null,
        "openreview_id": "~Ganesh_Ramakrishnan1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~ganesh/",
        "dblp_id": "r/GaneshRamakrishnan",
        "google_scholar_url": "https://scholar.google.com/scholar?hl=hi",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "36g8Ept_CCj",
      "title": "Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a new framework of learning mixture models via data partitioning called PRESTO, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific partition. We connect PRESTO to a number of past works in data partitioning, mixture models, and clustering, and show that PRESTO generalizes several loss functions including the k-means and Bregman clustering objective, the Gaussian mixture model objective, mixtures of support vector machines, and mixtures of linear regression. We then propose a new joint discrete-continuous optimization algorithm which achieves a bounded approximation guarantee for any general loss function, thereby achieving guarantees for the afore-mentioned problems as well. We study PRESTO in the context of resource efficient deep learning, where we train smaller resource constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to PRESTO, require large initial (teacher) models.",
      "tldr": "PRESTO learns a mixture models such that each model performs well on a data partition",
      "site_url": "https://openreview.net/forum?id=36g8Ept_CCj",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3GDft6lexE",
      "title": "Cooperate or Compete: A New Perspective on Training of Generative Networks",
      "status": "Desk Reject",
      "normalized_status": "rejected",
      "abstract": "GANs have two competing modules: the generator module is trained to generate new examples, and the discriminator module is trained to discriminate real examples from generated examples. The training procedure of GAN is modeled as a finitely repeated simultaneous game. Each module tries to increase its performance at every repetition of the base game (at every batch of training data) in a non-cooperative manner. We observed that each module can perform better and learn faster if training is modeled as an infinitely repeated simultaneous game. At every repetition of the base game (at every batch of training data) the stronger module (whose performance is increased or remains the same compared to the previous batch of training data) cooperates with the weaker module (whose performance is decreased compared to the previous batch of training data) and only the weaker module is allowed to increase its performance. ",
      "tldr": "Generative networks can perform better and learn faster if training is modeled as an infinitely repeated simultaneous game",
      "site_url": "https://openreview.net/forum?id=3GDft6lexE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 1,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sobhan_Babu1",
        "name": "Sobhan Babu",
        "name_site": null,
        "openreview_id": "~Sobhan_Babu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.iith.ac.in/~sobhan/",
        "dblp_id": "233/3456.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=UFMtsfkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3v2DIO9oVl",
      "title": "Generalization error bounds for Neural Networks with ReLU activation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We show rigorous bounds on the generalization error for Neural Networks with ReLU activation under the condition that the network size doesn't grow with the training set size. In order to prove these bounds we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and proving that if an algorithm has low enough a.s. support stability its generalization error tends to 0 as the training set size increases. Further we show that for Stochastic Gradient Descent to be almost surely support stable we only need the loss function to be locally Lipschitz and locally smooth with probability 1, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties, thereby proving low generalization error. The caveat is that the size of NN must not grow with the size of the training set. Finally we present experimental evidence to validate our theoretical results.",
      "tldr": "We show that generalization error of Neural Netowrks with ReLU activations approaches zero with proabbility 1 as we increase the training points",
      "site_url": "https://openreview.net/forum?id=3v2DIO9oVl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Pandey1",
        "name": "Harsh Pandey",
        "name_site": null,
        "openreview_id": "~Harsh_Pandey1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "135/8401",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "harshpan/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3v2DIO9oVl",
      "title": "Generalization error bounds for Neural Networks with ReLU activation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We show rigorous bounds on the generalization error for Neural Networks with ReLU activation under the condition that the network size doesn't grow with the training set size. In order to prove these bounds we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and proving that if an algorithm has low enough a.s. support stability its generalization error tends to 0 as the training set size increases. Further we show that for Stochastic Gradient Descent to be almost surely support stable we only need the loss function to be locally Lipschitz and locally smooth with probability 1, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties, thereby proving low generalization error. The caveat is that the size of NN must not grow with the size of the training set. Finally we present experimental evidence to validate our theoretical results.",
      "tldr": "We show that generalization error of Neural Netowrks with ReLU activations approaches zero with proabbility 1 as we increase the training points",
      "site_url": "https://openreview.net/forum?id=3v2DIO9oVl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amitabha_Bagchi3",
        "name": "Amitabha Bagchi",
        "name_site": null,
        "openreview_id": "~Amitabha_Bagchi3",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~bagchi",
        "dblp_id": "77/5034",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3v2DIO9oVl",
      "title": "Generalization error bounds for Neural Networks with ReLU activation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We show rigorous bounds on the generalization error for Neural Networks with ReLU activation under the condition that the network size doesn't grow with the training set size. In order to prove these bounds we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and proving that if an algorithm has low enough a.s. support stability its generalization error tends to 0 as the training set size increases. Further we show that for Stochastic Gradient Descent to be almost surely support stable we only need the loss function to be locally Lipschitz and locally smooth with probability 1, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties, thereby proving low generalization error. The caveat is that the size of NN must not grow with the size of the training set. Finally we present experimental evidence to validate our theoretical results.",
      "tldr": "We show that generalization error of Neural Netowrks with ReLU activations approaches zero with proabbility 1 as we increase the training points",
      "site_url": "https://openreview.net/forum?id=3v2DIO9oVl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srikanta_J._Bedathur1",
        "name": "Srikanta J. Bedathur",
        "name_site": null,
        "openreview_id": "~Srikanta_J._Bedathur1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://www.cse.iitd.ac.in/~srikanta/",
        "dblp_id": "b/SrikantaJBedathur",
        "google_scholar_url": "ngfF2oAAAAAJ",
        "orcid": "0000-0002-3949-2175",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3z1Ws6GEYV4",
      "title": "Multi-Objective GFlowNets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In many applications of machine learning, like drug discovery and material design, the goal is to generate candidates that simultaneously maximize a set of objectives. As these objectives are often conflicting, there is no single candidate that simultaneously maximizes all objectives, but rather a set of Pareto-optimal candidates where one objective cannot be improved without worsening another. Moreover, these objectives, when considered in practice are often under-specified, making diversity of candidates a key consideration. The existing multi-objective optimization methods focus predominantly on covering the Pareto front, failing the capture diversity in the space of candidates. Motivated by the success of GFlowNets for generation of diverse candidates in a single objective setting, in this paper we consider Multi-Objective GFlowNets (MOGFNs). MOGFNs consist of a Conditional GFlowNet which models a family of single-objective sub-problems derived by decomposing the multi-objective optimization problem. Our work is the first to empirically demonstrate conditional GFlowNets. Through a series of experiments on synthetic tasks and real-world domains, we empirically demonstrate that MOGFNs outperform existing methods in terms of Hypervolume, R2-distance and candidate diversity. We also demonstrate the effectiveness of MOGFNs over existing methods in active learning settings. Finally, we supplement our empirical results with a careful analysis of each component of MOGFNs.",
      "tldr": "We generate diverse Pareto-optimal candidates for high-dimensional multi-objective optimization problems with GFlowNets. ",
      "site_url": "https://openreview.net/forum?id=3z1Ws6GEYV4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 84,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7pl0FRiS0Td",
      "title": "Contextual Transformer for Offline Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recently, the pretrain-tuning paradigm in large-scale sequence models has made significant progress in Natural Language Processing and Computer Vision. However, such a paradigm is still hindered by intractable challenges in Reinforcement Learning (RL), including the lack of self-supervised large-scale pretraining methods based on offline data and efficient fine-tuning/prompt-tuning over unseen downstream tasks. In this work, we explore how prompts can help sequence-modeling-based offline Reinforcement Learning (offline-RL) algorithms. Firstly, we propose prompt tuning for offline RL, where a context vector sequence is concatenated with the input to guide the conditional generation. As such, we can pretrain a model on the offline dataset with supervised loss and learn a prompt to guide the policy to play the desired actions. Secondly, we extend the framework to the Meta-RL setting and propose Contextual Meta Transformer (CMT), which leverages the context among different tasks as the prompt to improve the performance on unseen tasks. We conduct extensive experiments across three different offline-RL settings: offline single-agent RL on the D4RL dataset, offline Meta-RL on the MuJoCo benchmark, and offline MARL on the SMAC benchmark. The results validate the strong performance, and generality of our methods.",
      "tldr": "This paper explores how prompts help sequence-modeling based offline-RL algorithms",
      "site_url": "https://openreview.net/forum?id=7pl0FRiS0Td",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 8,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8zsK9lbna9L",
      "title": "RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We propose a method for the task of text-conditioned speech insertion, i.e.\\ inserting a speech sample in an input speech sample, conditioned on the corresponding complete text transcript. An example use case of the task would be to update the speech audio when corrections are done on the corresponding text transcript. The proposed method follows a transformer-based non-autoregressive approach that allows speech insertions of variable lengths, which are dynamically determined during inference, based on the text transcript and tempo of the available partial input. It is capable of maintaining the speaker's voice characteristics, prosody and other spectral properties of the available speech input. Results from our experiments and user study on LibriTTS show that our method outperforms baselines based on an existing adaptive text to speech method. We also provide numerous qualitative results to appreciate the quality of the output from the proposed method.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=8zsK9lbna9L",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Neeraj_Matiyali1",
        "name": "Neeraj Matiyali",
        "name_site": null,
        "openreview_id": "~Neeraj_Matiyali1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "230/4592",
        "google_scholar_url": "3skMlf8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BdcfKgE9dhF",
      "title": "Robust Training through Adversarially Selected Data Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Robustness to adversarial perturbations often comes at the cost of a drop in accuracy on unperturbed or clean instances. Most existing defense mechanisms attempt to defend the learner from attack on all possible instances, which often degrades the accuracy on clean instances significantly. However, in practice, an attacker might only select a small subset of instances to attack, $e.g.$, in facial recognition systems an adversary might aim to target specific faces. Moreover, the subset selection strategy of the attacker is seldom known to the defense mechanism a priori, making it challenging to attune the mechanism beforehand. This motivates designing defense mechanisms which can (i) defend against attacks on subsets instead of all instances to prevent degradation of clean accuracy and, (ii) ensure good overall performance for attacks on any selected subset. In this work, we take a step towards solving this problem. We cast the training problem as a min-max game involving worst-case subset selection along with optimization of model parameters, rendering the problem NP-hard. To tackle this, we first show that, for a given learner's model, the objective can be expressed as a difference between a $\\gamma$-weakly submodular and a modular function. We use this property to propose ROGET, an iterative algorithm, which admits approximation guarantees for a class of loss functions. Our experiments show that ROGET obtains better overall accuracy compared to several state-of-the-art defense methods for different adversarial subset selection techniques.",
      "tldr": "Develops robust learning strategy where a subset of instances are selectively chosen for perturbation and the selection strategy is never revealed to the learner.",
      "site_url": "https://openreview.net/forum?id=BdcfKgE9dhF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Hitvarth_Diwanji1",
        "name": "Hitvarth Diwanji",
        "name_site": null,
        "openreview_id": "~Hitvarth_Diwanji1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://homepages.iitb.ac.in/~190100057/HitvarthDiwanji/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BdcfKgE9dhF",
      "title": "Robust Training through Adversarially Selected Data Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Robustness to adversarial perturbations often comes at the cost of a drop in accuracy on unperturbed or clean instances. Most existing defense mechanisms attempt to defend the learner from attack on all possible instances, which often degrades the accuracy on clean instances significantly. However, in practice, an attacker might only select a small subset of instances to attack, $e.g.$, in facial recognition systems an adversary might aim to target specific faces. Moreover, the subset selection strategy of the attacker is seldom known to the defense mechanism a priori, making it challenging to attune the mechanism beforehand. This motivates designing defense mechanisms which can (i) defend against attacks on subsets instead of all instances to prevent degradation of clean accuracy and, (ii) ensure good overall performance for attacks on any selected subset. In this work, we take a step towards solving this problem. We cast the training problem as a min-max game involving worst-case subset selection along with optimization of model parameters, rendering the problem NP-hard. To tackle this, we first show that, for a given learner's model, the objective can be expressed as a difference between a $\\gamma$-weakly submodular and a modular function. We use this property to propose ROGET, an iterative algorithm, which admits approximation guarantees for a class of loss functions. Our experiments show that ROGET obtains better overall accuracy compared to several state-of-the-art defense methods for different adversarial subset selection techniques.",
      "tldr": "Develops robust learning strategy where a subset of instances are selectively chosen for perturbation and the selection strategy is never revealed to the learner.",
      "site_url": "https://openreview.net/forum?id=BdcfKgE9dhF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishi_Agarwal1",
        "name": "Rishi Agarwal",
        "name_site": null,
        "openreview_id": "~Rishi_Agarwal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://rishiagarwal2000.github.io",
        "dblp_id": null,
        "google_scholar_url": "mKJs6cAAAAAJ",
        "orcid": "0000-0002-1284-2593",
        "linkedin_url": "rishi-agarwal-a473a2202/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BdcfKgE9dhF",
      "title": "Robust Training through Adversarially Selected Data Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Robustness to adversarial perturbations often comes at the cost of a drop in accuracy on unperturbed or clean instances. Most existing defense mechanisms attempt to defend the learner from attack on all possible instances, which often degrades the accuracy on clean instances significantly. However, in practice, an attacker might only select a small subset of instances to attack, $e.g.$, in facial recognition systems an adversary might aim to target specific faces. Moreover, the subset selection strategy of the attacker is seldom known to the defense mechanism a priori, making it challenging to attune the mechanism beforehand. This motivates designing defense mechanisms which can (i) defend against attacks on subsets instead of all instances to prevent degradation of clean accuracy and, (ii) ensure good overall performance for attacks on any selected subset. In this work, we take a step towards solving this problem. We cast the training problem as a min-max game involving worst-case subset selection along with optimization of model parameters, rendering the problem NP-hard. To tackle this, we first show that, for a given learner's model, the objective can be expressed as a difference between a $\\gamma$-weakly submodular and a modular function. We use this property to propose ROGET, an iterative algorithm, which admits approximation guarantees for a class of loss functions. Our experiments show that ROGET obtains better overall accuracy compared to several state-of-the-art defense methods for different adversarial subset selection techniques.",
      "tldr": "Develops robust learning strategy where a subset of instances are selectively chosen for perturbation and the selection strategy is never revealed to the learner.",
      "site_url": "https://openreview.net/forum?id=BdcfKgE9dhF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swaprava_Nath2",
        "name": "Swaprava Nath",
        "name_site": null,
        "openreview_id": "~Swaprava_Nath2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~swaprava/",
        "dblp_id": "70/9376",
        "google_scholar_url": "TlpsH9cAAAAJ",
        "orcid": "0000-0001-8309-5006",
        "linkedin_url": "swaprava/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BdcfKgE9dhF",
      "title": "Robust Training through Adversarially Selected Data Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Robustness to adversarial perturbations often comes at the cost of a drop in accuracy on unperturbed or clean instances. Most existing defense mechanisms attempt to defend the learner from attack on all possible instances, which often degrades the accuracy on clean instances significantly. However, in practice, an attacker might only select a small subset of instances to attack, $e.g.$, in facial recognition systems an adversary might aim to target specific faces. Moreover, the subset selection strategy of the attacker is seldom known to the defense mechanism a priori, making it challenging to attune the mechanism beforehand. This motivates designing defense mechanisms which can (i) defend against attacks on subsets instead of all instances to prevent degradation of clean accuracy and, (ii) ensure good overall performance for attacks on any selected subset. In this work, we take a step towards solving this problem. We cast the training problem as a min-max game involving worst-case subset selection along with optimization of model parameters, rendering the problem NP-hard. To tackle this, we first show that, for a given learner's model, the objective can be expressed as a difference between a $\\gamma$-weakly submodular and a modular function. We use this property to propose ROGET, an iterative algorithm, which admits approximation guarantees for a class of loss functions. Our experiments show that ROGET obtains better overall accuracy compared to several state-of-the-art defense methods for different adversarial subset selection techniques.",
      "tldr": "Develops robust learning strategy where a subset of instances are selectively chosen for perturbation and the selection strategy is never revealed to the learner.",
      "site_url": "https://openreview.net/forum?id=BdcfKgE9dhF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Crw1sKsLDvl",
      "title": "COMNET : CORTICAL MODULES ARE POWERFUL",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Existing CNN architectures may achieve efficiency in either one or two dimensions: FLOPs, depth, accuracy, representation power, latency but not in all. In this work, we present a pragmatically designed novel CNN architecture “CoMNet” which offers multi-dimensional efficiency at once such as: simple yet accurate, lower latency and FLOPs, high representation power in limited parameters, low memory consumption, negligible branching, smaller depths, and only a few design hyperparameters. The key to achieve the multi-dimensional efficiency is our use of biological underpinnings into CoMNet which is primarily the organization of cortical modules in the visual cortex. To realize CoMNet, a few concepts from well understood CNN designs are directly inherited such as residual learning. Our solid experimental evaluations demonstrate superiority of CoMNet over many state-of-the-art industry and academia dominant architectures such as ResNet, RepVGG etc. For instance, CoMNet supersedes ResNet-50 on ImageNet while being 50% shallower, 22% lesser parameters, 25% lower FLOPs and latency, and in 16% lesser training epochs. Code will be opensourced post the reviews.",
      "tldr": "A novel CNN architecture leveraging biological structures in visual cortex to cater real-time applications with low latency, smaller depths",
      "site_url": "https://openreview.net/forum?id=Crw1sKsLDvl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar2",
        "name": "Ashish Kumar",
        "name_site": null,
        "openreview_id": "~Ashish_Kumar2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashishkumar822.github.io",
        "dblp_id": "34/5378-6",
        "google_scholar_url": "n-oRDEYAAAAJ",
        "orcid": null,
        "linkedin_url": "ashishkumar822/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Crw1sKsLDvl",
      "title": "COMNET : CORTICAL MODULES ARE POWERFUL",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Existing CNN architectures may achieve efficiency in either one or two dimensions: FLOPs, depth, accuracy, representation power, latency but not in all. In this work, we present a pragmatically designed novel CNN architecture “CoMNet” which offers multi-dimensional efficiency at once such as: simple yet accurate, lower latency and FLOPs, high representation power in limited parameters, low memory consumption, negligible branching, smaller depths, and only a few design hyperparameters. The key to achieve the multi-dimensional efficiency is our use of biological underpinnings into CoMNet which is primarily the organization of cortical modules in the visual cortex. To realize CoMNet, a few concepts from well understood CNN designs are directly inherited such as residual learning. Our solid experimental evaluations demonstrate superiority of CoMNet over many state-of-the-art industry and academia dominant architectures such as ResNet, RepVGG etc. For instance, CoMNet supersedes ResNet-50 on ImageNet while being 50% shallower, 22% lesser parameters, 25% lower FLOPs and latency, and in 16% lesser training epochs. Code will be opensourced post the reviews.",
      "tldr": "A novel CNN architecture leveraging biological structures in visual cortex to cater real-time applications with low latency, smaller depths",
      "site_url": "https://openreview.net/forum?id=Crw1sKsLDvl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laxmidhar_Behera1",
        "name": "Laxmidhar Behera",
        "name_site": null,
        "openreview_id": "~Laxmidhar_Behera1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~lbehera/",
        "dblp_id": "14/1412",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=QWTcyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "laxmidhar-behera-a74a5b174/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FHZUqgxIBYn",
      "title": "Opportunistic Actor-Critic (OPAC) with Clipped Triple Q-learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Despite being the most successful model-free deep reinforcement learning (RL) algorithms in recent years, Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) have their respective downsides--TD3 performs well in simple tasks, while SAC does so in relatively complicated ones. However, they also suffer from underestimation due to Clipped Double Q-learning, i.e., taking a minimum of two Q-values. This paper introduces Opportunistic Actor-Critic (OPAC), an ensemble model-free deep RL algorithm that performs well in simple and complex tasks. OPAC combines the features of TD3 and SAC under one roof to retain their respective benefits. It also employs three critics and considers taking the mean of the smallest two Q-values for updating the shared target, dubbed Clipped Triple Q-learning. Our analytical results establish that Clipped Triple Q-learning incurs less underestimation than Clipped Double Q-learning. Furthermore, we have systematically evaluated OPAC in MuJoCo environments, and the empirical results indicate that OPAC attains higher average rewards than the current baselines.",
      "tldr": "OPAC achieves higher average rewards than relevant baselines and mitigates the underestimation bias with the help of Clipped Triple Q-learning.",
      "site_url": "https://openreview.net/forum?id=FHZUqgxIBYn",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srinjoy_Roy1",
        "name": "Srinjoy Roy",
        "name_site": null,
        "openreview_id": "~Srinjoy_Roy1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ramakrishna Mission Vivekananda Educational and Research Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GIZg_kOXqyG",
      "title": "Private and Efficient Meta-Learning with Low Rank and Sparse decomposition",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Meta-learning is critical for a variety of practical ML systems -- like personalized recommendations systems -- that are  required to generalize to new tasks despite a small number of task-specific training points. Existing meta-learning techniques use two complementary approaches of either learning a low-dimensional representation of points for all tasks, or task-specific fine-tuning of a global model trained using all the tasks. In this work, we propose a novel meta-learning framework that combines both the techniques to enable handling of a large number of data-starved tasks. Our framework models network weights as a sum of low-rank  and sparse matrices. This allows us to capture information from multiple domains together in the low-rank part while still allowing task specific personalization using the sparse part.  We instantiate and study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part effectively and efficiently. For the realizable, Gaussian data setting, we show that AMHT-LRS indeed solves the problem efficiently with nearly optimal samples.  We extend  AMHT-LRS  to ensure that it preserves privacy of each individual user in the dataset, while still ensuring strong  generalization with nearly optimal number of samples. Finally, on multiple datasets, we demonstrate that the framework allows personalized models to obtain superior performance in the data-scarce regime.",
      "tldr": "Provable meta-learning via privacy preserving and optimal low-rank+sparse decomposition",
      "site_url": "https://openreview.net/forum?id=GIZg_kOXqyG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Aggarwal4",
        "name": "Gaurav Aggarwal",
        "name_site": null,
        "openreview_id": "~Gaurav_Aggarwal4",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "14/5218",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9XiIwDQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GIZg_kOXqyG",
      "title": "Private and Efficient Meta-Learning with Low Rank and Sparse decomposition",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Meta-learning is critical for a variety of practical ML systems -- like personalized recommendations systems -- that are  required to generalize to new tasks despite a small number of task-specific training points. Existing meta-learning techniques use two complementary approaches of either learning a low-dimensional representation of points for all tasks, or task-specific fine-tuning of a global model trained using all the tasks. In this work, we propose a novel meta-learning framework that combines both the techniques to enable handling of a large number of data-starved tasks. Our framework models network weights as a sum of low-rank  and sparse matrices. This allows us to capture information from multiple domains together in the low-rank part while still allowing task specific personalization using the sparse part.  We instantiate and study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part effectively and efficiently. For the realizable, Gaussian data setting, we show that AMHT-LRS indeed solves the problem efficiently with nearly optimal samples.  We extend  AMHT-LRS  to ensure that it preserves privacy of each individual user in the dataset, while still ensuring strong  generalization with nearly optimal number of samples. Finally, on multiple datasets, we demonstrate that the framework allows personalized models to obtain superior performance in the data-scarce regime.",
      "tldr": "Provable meta-learning via privacy preserving and optimal low-rank+sparse decomposition",
      "site_url": "https://openreview.net/forum?id=GIZg_kOXqyG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pradeep_Shenoy1",
        "name": "Pradeep Shenoy",
        "name_site": null,
        "openreview_id": "~Pradeep_Shenoy1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "12/771",
        "google_scholar_url": "lXbPKmkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GKpwIa9wgwR",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subset selection, in recent times, has emerged as a successful approach toward efficient training of models by significantly reducing the amount of data and computational resources required. However, existing methods employ discrete combinatorial and model-specific approaches which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for data subset selection for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose SubSelNet, a non-adaptive  subset selection framework, which tackles these problems with two main components. First, we introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This leads us to develop two variants of  SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization.  Most state-of-the-art data subset selection approaches are adaptive, in that the subset selection adapts as the training progresses, and as a result, they require access to the entire data at training time.  Our approach, in contrast, is non-adaptive and does the subset selection only once in the beginning, thereby achieving resource and memory efficiency along with compute-efficiency at training time. Our experiments show that both transductive and inductive variants of our models outperform several methods on the quality of the subset chosen and further demonstrate that our method can be used for choosing the best architecture from a set of architectures.\n",
      "tldr": "Trainable non-adaptive data subset selection that generalizes across different model training",
      "site_url": "https://openreview.net/forum?id=GKpwIa9wgwR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Eeshaan_Jain1",
        "name": "Eeshaan Jain",
        "name_site": null,
        "openreview_id": "~Eeshaan_Jain1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://eeshaanjain.github.io",
        "dblp_id": null,
        "google_scholar_url": "r5rqqJEAAAAJ",
        "orcid": null,
        "linkedin_url": "eeshaanjain/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GKpwIa9wgwR",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subset selection, in recent times, has emerged as a successful approach toward efficient training of models by significantly reducing the amount of data and computational resources required. However, existing methods employ discrete combinatorial and model-specific approaches which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for data subset selection for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose SubSelNet, a non-adaptive  subset selection framework, which tackles these problems with two main components. First, we introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This leads us to develop two variants of  SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization.  Most state-of-the-art data subset selection approaches are adaptive, in that the subset selection adapts as the training progresses, and as a result, they require access to the entire data at training time.  Our approach, in contrast, is non-adaptive and does the subset selection only once in the beginning, thereby achieving resource and memory efficiency along with compute-efficiency at training time. Our experiments show that both transductive and inductive variants of our models outperform several methods on the quality of the subset chosen and further demonstrate that our method can be used for choosing the best architecture from a set of architectures.\n",
      "tldr": "Trainable non-adaptive data subset selection that generalizes across different model training",
      "site_url": "https://openreview.net/forum?id=GKpwIa9wgwR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Nandy1",
        "name": "Tushar Nandy",
        "name_site": null,
        "openreview_id": "~Tushar_Nandy1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "tushar-nandy/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GKpwIa9wgwR",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subset selection, in recent times, has emerged as a successful approach toward efficient training of models by significantly reducing the amount of data and computational resources required. However, existing methods employ discrete combinatorial and model-specific approaches which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for data subset selection for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose SubSelNet, a non-adaptive  subset selection framework, which tackles these problems with two main components. First, we introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This leads us to develop two variants of  SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization.  Most state-of-the-art data subset selection approaches are adaptive, in that the subset selection adapts as the training progresses, and as a result, they require access to the entire data at training time.  Our approach, in contrast, is non-adaptive and does the subset selection only once in the beginning, thereby achieving resource and memory efficiency along with compute-efficiency at training time. Our experiments show that both transductive and inductive variants of our models outperform several methods on the quality of the subset chosen and further demonstrate that our method can be used for choosing the best architecture from a set of architectures.\n",
      "tldr": "Trainable non-adaptive data subset selection that generalizes across different model training",
      "site_url": "https://openreview.net/forum?id=GKpwIa9wgwR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Aggarwal4",
        "name": "Gaurav Aggarwal",
        "name_site": null,
        "openreview_id": "~Gaurav_Aggarwal4",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "14/5218",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9XiIwDQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GKpwIa9wgwR",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subset selection, in recent times, has emerged as a successful approach toward efficient training of models by significantly reducing the amount of data and computational resources required. However, existing methods employ discrete combinatorial and model-specific approaches which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for data subset selection for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose SubSelNet, a non-adaptive  subset selection framework, which tackles these problems with two main components. First, we introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This leads us to develop two variants of  SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization.  Most state-of-the-art data subset selection approaches are adaptive, in that the subset selection adapts as the training progresses, and as a result, they require access to the entire data at training time.  Our approach, in contrast, is non-adaptive and does the subset selection only once in the beginning, thereby achieving resource and memory efficiency along with compute-efficiency at training time. Our experiments show that both transductive and inductive variants of our models outperform several methods on the quality of the subset chosen and further demonstrate that our method can be used for choosing the best architecture from a set of architectures.\n",
      "tldr": "Trainable non-adaptive data subset selection that generalizes across different model training",
      "site_url": "https://openreview.net/forum?id=GKpwIa9wgwR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_V._Tendulkar1",
        "name": "Ashish V. Tendulkar",
        "name_site": null,
        "openreview_id": "~Ashish_V._Tendulkar1",
        "position": 4,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "08/1521",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GKpwIa9wgwR",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Subset selection, in recent times, has emerged as a successful approach toward efficient training of models by significantly reducing the amount of data and computational resources required. However, existing methods employ discrete combinatorial and model-specific approaches which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for data subset selection for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose SubSelNet, a non-adaptive  subset selection framework, which tackles these problems with two main components. First, we introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This leads us to develop two variants of  SubSelNet. The first variant is transductive (called as Transductive-SubSelNet) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-SubSelNet) which computes the subset using a trained subset selector, without any optimization.  Most state-of-the-art data subset selection approaches are adaptive, in that the subset selection adapts as the training progresses, and as a result, they require access to the entire data at training time.  Our approach, in contrast, is non-adaptive and does the subset selection only once in the beginning, thereby achieving resource and memory efficiency along with compute-efficiency at training time. Our experiments show that both transductive and inductive variants of our models outperform several methods on the quality of the subset chosen and further demonstrate that our method can be used for choosing the best architecture from a set of architectures.\n",
      "tldr": "Trainable non-adaptive data subset selection that generalizes across different model training",
      "site_url": "https://openreview.net/forum?id=GKpwIa9wgwR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GVWySHBD3Cl",
      "title": "Estimating Treatment Effects using Neurosymbolic Program Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Estimating treatment effects from observational data is a central problem in causal inference. Methods to solve this problem exploit inductive biases and heuristics from causal inference to design multi-head neural network architectures and regularizers. In this work, we propose to use neurosymbolic program synthesis, a data-efficient, and interpretable technique, to solve the treatment effect estimation problem. We theoretically show that neurosymbolic programming can solve the treatment effect estimation problem. By designing a Domain Specific Language (DSL) for treatment effect estimation based on the inductive biases used in literature, we argue that neurosymbolic programming is a better alternative to treatment effect estimation than traditional models. Our empirical study reveals that our model, which implicitly encodes inductive biases in a DSL, achieves better performance on benchmark datasets than the state-of-the-art models.",
      "tldr": "We estimate treatment effects/ causal effects using neurosymbolic program synthesis by designing a domain specific language ",
      "site_url": "https://openreview.net/forum?id=GVWySHBD3Cl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abbavaram_Gowtham_Reddy1",
        "name": "Abbavaram Gowtham Reddy",
        "name_site": null,
        "openreview_id": "~Abbavaram_Gowtham_Reddy1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://gautam0707.github.io",
        "dblp_id": "294/8798",
        "google_scholar_url": "Iewg-GAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "GVWySHBD3Cl",
      "title": "Estimating Treatment Effects using Neurosymbolic Program Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Estimating treatment effects from observational data is a central problem in causal inference. Methods to solve this problem exploit inductive biases and heuristics from causal inference to design multi-head neural network architectures and regularizers. In this work, we propose to use neurosymbolic program synthesis, a data-efficient, and interpretable technique, to solve the treatment effect estimation problem. We theoretically show that neurosymbolic programming can solve the treatment effect estimation problem. By designing a Domain Specific Language (DSL) for treatment effect estimation based on the inductive biases used in literature, we argue that neurosymbolic programming is a better alternative to treatment effect estimation than traditional models. Our empirical study reveals that our model, which implicitly encodes inductive biases in a DSL, achieves better performance on benchmark datasets than the state-of-the-art models.",
      "tldr": "We estimate treatment effects/ causal effects using neurosymbolic program synthesis by designing a domain specific language ",
      "site_url": "https://openreview.net/forum?id=GVWySHBD3Cl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaji_Krishnamurthy1_1",
        "name": "Balaji Krishnamurthy",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Gp91Et4LeRf",
      "title": "Auditing Fairness Online through Interactive Refinement",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Machine learning algorithms are increasingly being deployed for high-stakes scenarios. A sizeable proportion of currently deployed models make their decisions in a black box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we focus on user-specified accountability of decision-making processes of black box systems. Previous work has formulated this problem as run time fairness monitoring over decision functions. However, formulating appropriate specifications for situation-appropriate fairness metrics is challenging. We construct AVOIR, an automated inference-based optimization system that improves bounds for and generalizes prior work across a wide range of fairness metrics. AVOIR offers an interactive and iterative process for exploring fairness violations aligned with governance and regulatory requirements. Our bounds improve over previous probabilistic guarantees for such fairness grammars in online settings. We also construct a novel visualization mechanism that can be used to investigate the context of reported fairness violations and guide users towards meaningful and compliant fairness specifications. We then conduct case studies with fairness metrics on three different datasets and demonstrate how the visualization and improved optimization can detect fairness violations more efficiently and ameliorate the issues with faulty fairness metric design. ",
      "tldr": "A visual inference-based optimization framework that facilitates the specification and auditing of fairness on blackbox ML models efficiently.",
      "site_url": "https://openreview.net/forum?id=Gp91Et4LeRf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~srinivasan_parthasarathy1",
        "name": "srinivasan parthasarathy",
        "name_site": null,
        "openreview_id": "~srinivasan_parthasarathy1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://web.cse.ohio-state.edu/~parthasarathy.2/",
        "dblp_id": "p/SParathasarathy.html",
        "google_scholar_url": "2mjUsP8AAAAJ",
        "orcid": "0000-0002-6062-6449",
        "linkedin_url": "srinivasan-parthasarathy-5703761/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.8,
        "confidence_std": 0.9797958971132712,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IHGnybgLo1Z",
      "title": "A Critical Analysis of Out-of-Distribution Detection for Document Understanding",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Large-scale pretraining is widely used in recent document understanding models. During deployment, one may expect that large-scale pretrained models should trigger a conservative fallback policy when encountering out-of-distribution (OOD) samples, which suggests the importance of OOD detection. However, most existing OOD detection methods focus on single-modal inputs such as images or texts. While documents are multi-modal in nature, it is underexplored if and how multi-modal information in documents can be exploited for OOD detection. In this work, we first provide a systematic and in-depth analysis on OOD detection for document understanding models. We study the effects of model modality, pretraining, and finetuning across various types of OOD inputs. In particular, we find that spatial information is critical for document OOD detection. To better exploit spatial information, we propose a simple yet effective special-aware adapter, which serves as an add-on module to adapt transformer-based language models to document domain. Extensive experiments show that our method consistently improves ID accuracy and OOD detection performance compared to baselines. We hope our findings can help inspire future works on understanding OOD robustness for documents.",
      "tldr": "This work investigates the OOD robustness of pretrained models and presents a benchmark for various document understanding tasks.",
      "site_url": "https://openreview.net/forum?id=IHGnybgLo1Z",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 12,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 9,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IM4Iwo58T4M",
      "title": "TOWARDS AN OBJECTIVE EVALUATION OF THE TRUSTWORTHINESS OF CLASSIFIERS",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the widespread deployment of AI models in applications that impact human lives, research on model trustworthiness has become increasingly important, as a result of which model effectiveness alone (measured, e.g., with accuracy, F1, etc.) should not be the only criteria to evaluate predictive models; additionally the trustworthiness of these models should also be factored in. It has been argued that the features deemed important by a black-box model should be aligned with the human perception of the data, which in turn, should contribute to increasing the trustworthiness of a model. Existing research in XAI evaluates such alignments with user studies - the limitations being that these studies are subjective, difficult to reproduce, and consumes a large amount of time to conduct. We propose an evaluation framework, which provides a quantitative measure for trustworthiness of a black-box model, and hence, we are able to provide a fair comparison between a number of different black-box models. Our framework is applicable to both text and images, and our experiment results show that a model with a higher accuracy does not necessarily exhibit better trustworthiness.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=IM4Iwo58T4M",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debasis_Ganguly2",
        "name": "Debasis Ganguly",
        "name_site": null,
        "openreview_id": "~Debasis_Ganguly2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://gdebasis.github.io/",
        "dblp_id": "41/7272",
        "google_scholar_url": "FhQENQgAAAAJ",
        "orcid": "0000-0003-0050-7138",
        "linkedin_url": "deb4it/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Glasgow (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IqN5SgOmxp",
      "title": "Reinforcement Learning using a Molecular Fragment Based Approach for Reaction Discovery",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Deep learning methods have recently been applied to both predictive and generative tasks in the molecular space. While molecular generation and prediction of an associated property are now reasonably common, studies on reaction outcome due to the generated molecules remain less explored. Chemical reactions present a complex scenario as they involve multiple molecules and the breaking/forming of bonds. In reaction discovery, one aims to maximise yield and/or selectivity, which depends on a multitude of factors, including partner reactants and reaction conditions. We propose a multi-pronged approach that combines policy gradient reinforcement learning with a recurrent neural network-based deep generative model to identify prospective new reactants, whose yield/selectivity is estimated by a pre-trained regressor. Using SMILES (simplified molecular-input line-entry system) as the raw representation, our approach involves attaching a user-defined core fragment to the generated molecules for reaction-specific learning. On three distinct reaction types (alcohol deoxyflourination, imine-thiol coupling, asymmetric hydrogenation of imines and alkenes), we obtain notable improvements in yield and enantioselectivity. The generated molecules are diverse, while remaining synthetically accessible.",
      "tldr": "A multi-pronged deep learning approach using a fragment based method is applied to chemical reaction discovery",
      "site_url": "https://openreview.net/forum?id=IqN5SgOmxp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ajnabiul_Hoque1",
        "name": "Ajnabiul Hoque",
        "name_site": null,
        "openreview_id": "~Ajnabiul_Hoque1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0000-0001-9807-3061",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IqN5SgOmxp",
      "title": "Reinforcement Learning using a Molecular Fragment Based Approach for Reaction Discovery",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Deep learning methods have recently been applied to both predictive and generative tasks in the molecular space. While molecular generation and prediction of an associated property are now reasonably common, studies on reaction outcome due to the generated molecules remain less explored. Chemical reactions present a complex scenario as they involve multiple molecules and the breaking/forming of bonds. In reaction discovery, one aims to maximise yield and/or selectivity, which depends on a multitude of factors, including partner reactants and reaction conditions. We propose a multi-pronged approach that combines policy gradient reinforcement learning with a recurrent neural network-based deep generative model to identify prospective new reactants, whose yield/selectivity is estimated by a pre-trained regressor. Using SMILES (simplified molecular-input line-entry system) as the raw representation, our approach involves attaching a user-defined core fragment to the generated molecules for reaction-specific learning. On three distinct reaction types (alcohol deoxyflourination, imine-thiol coupling, asymmetric hydrogenation of imines and alkenes), we obtain notable improvements in yield and enantioselectivity. The generated molecules are diverse, while remaining synthetically accessible.",
      "tldr": "A multi-pronged deep learning approach using a fragment based method is applied to chemical reaction discovery",
      "site_url": "https://openreview.net/forum?id=IqN5SgOmxp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mihir_Jitendra_Surve1",
        "name": "Mihir Jitendra Surve",
        "name_site": null,
        "openreview_id": "~Mihir_Jitendra_Surve1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mihir-surve-3a9840171/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IqN5SgOmxp",
      "title": "Reinforcement Learning using a Molecular Fragment Based Approach for Reaction Discovery",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Deep learning methods have recently been applied to both predictive and generative tasks in the molecular space. While molecular generation and prediction of an associated property are now reasonably common, studies on reaction outcome due to the generated molecules remain less explored. Chemical reactions present a complex scenario as they involve multiple molecules and the breaking/forming of bonds. In reaction discovery, one aims to maximise yield and/or selectivity, which depends on a multitude of factors, including partner reactants and reaction conditions. We propose a multi-pronged approach that combines policy gradient reinforcement learning with a recurrent neural network-based deep generative model to identify prospective new reactants, whose yield/selectivity is estimated by a pre-trained regressor. Using SMILES (simplified molecular-input line-entry system) as the raw representation, our approach involves attaching a user-defined core fragment to the generated molecules for reaction-specific learning. On three distinct reaction types (alcohol deoxyflourination, imine-thiol coupling, asymmetric hydrogenation of imines and alkenes), we obtain notable improvements in yield and enantioselectivity. The generated molecules are diverse, while remaining synthetically accessible.",
      "tldr": "A multi-pronged deep learning approach using a fragment based method is applied to chemical reaction discovery",
      "site_url": "https://openreview.net/forum?id=IqN5SgOmxp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shivaram_Kalyanakrishnan1",
        "name": "Shivaram Kalyanakrishnan",
        "name_site": null,
        "openreview_id": "~Shivaram_Kalyanakrishnan1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~shivaram/",
        "dblp_id": "16/4410",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=YZkeEqAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IqN5SgOmxp",
      "title": "Reinforcement Learning using a Molecular Fragment Based Approach for Reaction Discovery",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Deep learning methods have recently been applied to both predictive and generative tasks in the molecular space. While molecular generation and prediction of an associated property are now reasonably common, studies on reaction outcome due to the generated molecules remain less explored. Chemical reactions present a complex scenario as they involve multiple molecules and the breaking/forming of bonds. In reaction discovery, one aims to maximise yield and/or selectivity, which depends on a multitude of factors, including partner reactants and reaction conditions. We propose a multi-pronged approach that combines policy gradient reinforcement learning with a recurrent neural network-based deep generative model to identify prospective new reactants, whose yield/selectivity is estimated by a pre-trained regressor. Using SMILES (simplified molecular-input line-entry system) as the raw representation, our approach involves attaching a user-defined core fragment to the generated molecules for reaction-specific learning. On three distinct reaction types (alcohol deoxyflourination, imine-thiol coupling, asymmetric hydrogenation of imines and alkenes), we obtain notable improvements in yield and enantioselectivity. The generated molecules are diverse, while remaining synthetically accessible.",
      "tldr": "A multi-pronged deep learning approach using a fragment based method is applied to chemical reaction discovery",
      "site_url": "https://openreview.net/forum?id=IqN5SgOmxp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Raghavan_B_Sunoj1",
        "name": "Raghavan B Sunoj",
        "name_site": null,
        "openreview_id": "~Raghavan_B_Sunoj1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.chem.iitb.ac.in/facultyuserview/r-b-sunoj",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-6484-2878",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J0IhgZ8ziv-",
      "title": "ENHANCING THE PRIVACY OF FEDERATED LEARNING THROUGH DATA SYNTHESIS",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated Learning (FL) is a distributed machine learning architecture where edge devices collaboratively learn the shared model, while the training data is securely held at the edge devices. FL promises a way forward to preserving data privacy by sending model updates in the form of gradients or the weights themselves. However, these updates still contain the essence of the original training data and can be\nreconstructed using gradient-based attacks. To overcome this, we propose a novel Privacy-Preserving Federated Learning algorithm (PPFed) wherein we generate a condensed dataset from the original training data at each edge device. The client’s then train their local models on the condensed dataset which is then broadcasted to the server, followed by regular federated averaging. Our method provides privacy by being robust against gradient-based attacks, which holds across different benchmark datasets and CNN based architectures",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=J0IhgZ8ziv-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~M_Yashwanth2",
        "name": "M Yashwanth",
        "name_site": null,
        "openreview_id": "~M_Yashwanth2",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "348/7177.html",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "yashwanth-mandula-aba700a5/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J0IhgZ8ziv-",
      "title": "ENHANCING THE PRIVACY OF FEDERATED LEARNING THROUGH DATA SYNTHESIS",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated Learning (FL) is a distributed machine learning architecture where edge devices collaboratively learn the shared model, while the training data is securely held at the edge devices. FL promises a way forward to preserving data privacy by sending model updates in the form of gradients or the weights themselves. However, these updates still contain the essence of the original training data and can be\nreconstructed using gradient-based attacks. To overcome this, we propose a novel Privacy-Preserving Federated Learning algorithm (PPFed) wherein we generate a condensed dataset from the original training data at each edge device. The client’s then train their local models on the condensed dataset which is then broadcasted to the server, followed by regular federated averaging. Our method provides privacy by being robust against gradient-based attacks, which holds across different benchmark datasets and CNN based architectures",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=J0IhgZ8ziv-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Kumar_Nayak2",
        "name": "Gaurav Kumar Nayak",
        "name_site": null,
        "openreview_id": "~Gaurav_Kumar_Nayak2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/gauravnayak",
        "dblp_id": "241/6244",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=cLCeKTkAAAAJ",
        "orcid": "0000-0002-6406-6178",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Central Florida (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J0IhgZ8ziv-",
      "title": "ENHANCING THE PRIVACY OF FEDERATED LEARNING THROUGH DATA SYNTHESIS",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated Learning (FL) is a distributed machine learning architecture where edge devices collaboratively learn the shared model, while the training data is securely held at the edge devices. FL promises a way forward to preserving data privacy by sending model updates in the form of gradients or the weights themselves. However, these updates still contain the essence of the original training data and can be\nreconstructed using gradient-based attacks. To overcome this, we propose a novel Privacy-Preserving Federated Learning algorithm (PPFed) wherein we generate a condensed dataset from the original training data at each edge device. The client’s then train their local models on the condensed dataset which is then broadcasted to the server, followed by regular federated averaging. Our method provides privacy by being robust against gradient-based attacks, which holds across different benchmark datasets and CNN based architectures",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=J0IhgZ8ziv-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Chakraborty1",
        "name": "Anirban Chakraborty",
        "name_site": null,
        "openreview_id": "~Anirban_Chakraborty1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://anirbanchakraborty.github.io/",
        "dblp_id": "73/2286-1",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=NtAsZK-2HjcC",
        "orcid": "0000-0002-6946-9152",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J0IhgZ8ziv-",
      "title": "ENHANCING THE PRIVACY OF FEDERATED LEARNING THROUGH DATA SYNTHESIS",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated Learning (FL) is a distributed machine learning architecture where edge devices collaboratively learn the shared model, while the training data is securely held at the edge devices. FL promises a way forward to preserving data privacy by sending model updates in the form of gradients or the weights themselves. However, these updates still contain the essence of the original training data and can be\nreconstructed using gradient-based attacks. To overcome this, we propose a novel Privacy-Preserving Federated Learning algorithm (PPFed) wherein we generate a condensed dataset from the original training data at each edge device. The client’s then train their local models on the condensed dataset which is then broadcasted to the server, followed by regular federated averaging. Our method provides privacy by being robust against gradient-based attacks, which holds across different benchmark datasets and CNN based architectures",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=J0IhgZ8ziv-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yogesh_Simmhan2",
        "name": "Yogesh Simmhan",
        "name_site": null,
        "openreview_id": "~Yogesh_Simmhan2",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/simmhan/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=HIOx9E0AAAAJ",
        "orcid": "0000-0003-4140-7774",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JKFSUPa70W6M",
      "title": "Don’t Bet on Sparsity: Designing Brain-inspired Distance-preserving Encoder",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Multi-headed self-attention-based Transformers have been a central area of research for quite some time. Albeit showing a significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformer and its variants fail to preserve layer-wise contextual information. Further, text representations learned by Transformer-based encoders are usually of low entropy with low variance, which contradicts typical human brain functions. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between any pair of tokens. We propose a simple alternative to dot product attention to ensure Lipschitz continuity that allows TransJect to learn injective mappings to transform token representations to different manifolds and preserve Euclidean distance between every pair of tokens in subsequent layers. Our evaluation on several benchmark short- and long-sequence classification tasks shows a remarkable improvement of 3.1% and 11%, on average, respectively. Furthermore, empirical results suggest that TransJect is layer-agnostic; in fact, it prefers shallower architectures than deeper ones and prevents layer-wise incremental learning beyond a threshold. Our empirical analyses also show the generalization capabilities of TransJect and the robustness under different hyperparameter configurations. We conduct detailed statistical analysis to confirm the necessity of high-entropic representations to achieve human-like cognition. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JKFSUPa70W6M",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ayan_Sengupta1",
        "name": "Ayan Sengupta",
        "name_site": null,
        "openreview_id": "~Ayan_Sengupta1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://victor7246.github.io/",
        "dblp_id": null,
        "google_scholar_url": "90EGfboAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JKFSUPa70W6M",
      "title": "Don’t Bet on Sparsity: Designing Brain-inspired Distance-preserving Encoder",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Multi-headed self-attention-based Transformers have been a central area of research for quite some time. Albeit showing a significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformer and its variants fail to preserve layer-wise contextual information. Further, text representations learned by Transformer-based encoders are usually of low entropy with low variance, which contradicts typical human brain functions. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between any pair of tokens. We propose a simple alternative to dot product attention to ensure Lipschitz continuity that allows TransJect to learn injective mappings to transform token representations to different manifolds and preserve Euclidean distance between every pair of tokens in subsequent layers. Our evaluation on several benchmark short- and long-sequence classification tasks shows a remarkable improvement of 3.1% and 11%, on average, respectively. Furthermore, empirical results suggest that TransJect is layer-agnostic; in fact, it prefers shallower architectures than deeper ones and prevents layer-wise incremental learning beyond a threshold. Our empirical analyses also show the generalization capabilities of TransJect and the robustness under different hyperparameter configurations. We conduct detailed statistical analysis to confirm the necessity of high-entropic representations to achieve human-like cognition. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JKFSUPa70W6M",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Md_Shad_Akhtar1",
        "name": "Md Shad Akhtar",
        "name_site": null,
        "openreview_id": "~Md_Shad_Akhtar1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "184/8579.html",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JKFSUPa70W6M",
      "title": "Don’t Bet on Sparsity: Designing Brain-inspired Distance-preserving Encoder",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Multi-headed self-attention-based Transformers have been a central area of research for quite some time. Albeit showing a significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformer and its variants fail to preserve layer-wise contextual information. Further, text representations learned by Transformer-based encoders are usually of low entropy with low variance, which contradicts typical human brain functions. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between any pair of tokens. We propose a simple alternative to dot product attention to ensure Lipschitz continuity that allows TransJect to learn injective mappings to transform token representations to different manifolds and preserve Euclidean distance between every pair of tokens in subsequent layers. Our evaluation on several benchmark short- and long-sequence classification tasks shows a remarkable improvement of 3.1% and 11%, on average, respectively. Furthermore, empirical results suggest that TransJect is layer-agnostic; in fact, it prefers shallower architectures than deeper ones and prevents layer-wise incremental learning beyond a threshold. Our empirical analyses also show the generalization capabilities of TransJect and the robustness under different hyperparameter configurations. We conduct detailed statistical analysis to confirm the necessity of high-entropic representations to achieve human-like cognition. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JKFSUPa70W6M",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tanmoy_Chakraborty2",
        "name": "Tanmoy Chakraborty",
        "name_site": null,
        "openreview_id": "~Tanmoy_Chakraborty2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://tanmoychak.com",
        "dblp_id": "65/2136-2.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=C5S9JnIAAAAJ",
        "orcid": "0000-0002-0210-0369",
        "linkedin_url": "tanmoy-chakraborty-89553324/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JyD-NobfNL_",
      "title": "Distinguishing Feature Model for Ranking From Pairwise Comparisons",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We consider the problem of ranking a set of items from pairwise comparisons among them when the underlying preferences are intransitive in nature. Intransitivity is a common occurrence in real world data sets and we introduce a flexible and natural parametric model for pairwise comparisons that we call the \\emph{Distinguishing Feature Model} (DF) to capture this. Under this model, the items have an unknown but fixed embedding and the pairwise comparison between a pair of items depends probabilisitically on the feature in the embedding that can best distinguish the items. We study several theoretical properties including how it generalizes the popular transitive Bradley-Terry-Luce model. With just an embedding dimension  $d = 3$, we show that the proposed model can capture  arbitrarily long cyclic dependencies. Furthermore, we explicitly show the type of preference relations that cannot be modelled under the DF model for $d=3$. On the algorithmic side, we propose a Siamese type neural network based algorithm which can learn to predict well under the DF model while at the same time being interpretable in the sense that the embeddings learnt can be extracted directly from the learnt model. Our experimental results show that the model outperforms  standard baselines in both synthetic and real world ranking datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JyD-NobfNL_",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Elisha_Parhi1",
        "name": "Elisha Parhi",
        "name_site": null,
        "openreview_id": "~Elisha_Parhi1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JyD-NobfNL_",
      "title": "Distinguishing Feature Model for Ranking From Pairwise Comparisons",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We consider the problem of ranking a set of items from pairwise comparisons among them when the underlying preferences are intransitive in nature. Intransitivity is a common occurrence in real world data sets and we introduce a flexible and natural parametric model for pairwise comparisons that we call the \\emph{Distinguishing Feature Model} (DF) to capture this. Under this model, the items have an unknown but fixed embedding and the pairwise comparison between a pair of items depends probabilisitically on the feature in the embedding that can best distinguish the items. We study several theoretical properties including how it generalizes the popular transitive Bradley-Terry-Luce model. With just an embedding dimension  $d = 3$, we show that the proposed model can capture  arbitrarily long cyclic dependencies. Furthermore, we explicitly show the type of preference relations that cannot be modelled under the DF model for $d=3$. On the algorithmic side, we propose a Siamese type neural network based algorithm which can learn to predict well under the DF model while at the same time being interpretable in the sense that the embeddings learnt can be extracted directly from the learnt model. Our experimental results show that the model outperforms  standard baselines in both synthetic and real world ranking datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JyD-NobfNL_",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arun_Rajkumar4",
        "name": "Arun Rajkumar",
        "name_site": "Arun Rajkumar, Vishnu Veerathu, Abdul Mir",
        "openreview_id": "~Arun_Rajkumar4",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "32/11350",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Lq2rsxfNt8",
      "title": "Fusion of Deep Transfer Learning with Mixed convolution network",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Global thirst in computer vision for image classification is performance improvement and parameter optimization. The evolution of deep learning raised the boundaries of the model size to hundreds of millions of parameters. This obliquely influences the training time of the model. Thus, contemporary research has diverted to parameter optimization par with performance. In this paper, a fusion-based deep transfer learning approach has been furbished with a mixed convolution block. The proposed mixed convolution block has been designed using two convolution paths including residual and separable convolutions. The residual convolution avoids vanishing gradient while separable convolution includes depthwise features. The experiments on the popular Fashion-MNIST bench mark dataset have proved that the proposed mixed convolution enticed the pre-trained models. It has been observed that there is a clear improvement of 1\\%  than the base models. Further, the proposed fusion model exhibits a competing performance of 96.04\\% with existing models.",
      "tldr": "Fusion of Deep Transfer Learning",
      "site_url": "https://openreview.net/forum?id=Lq2rsxfNt8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 1,
      "track_name": "Main Conference",
      "author": {
        "id": "~Isunuri_BV1",
        "name": "Isunuri BV",
        "name_site": null,
        "openreview_id": "~Isunuri_BV1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "2nux5iMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Information Technology Design and Manufacturing Kancheepuram (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ms1Zs8s7rg",
      "title": "Demystifying Approximate RL with $\\epsilon$-greedy Exploration: A Differential Inclusion View",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Q-learning and SARSA(0) with $\\epsilon$-greedy exploration are leading reinforcement learning methods, and their tabular forms converge to the optimal Q-function under reasonable conditions. However, with function approximation, they exhibit unexpected behaviors, such as i.) policy oscillation and chattering, and ii.) convergence to different attractors (possibly even the worst policy) on different runs, ii.) multiple attractors, and iii.) worst policy convergence, apart from the textbook instability. Accordingly, a theory to explain these phenomena has been a long-standing open problem, even for basic linear function approximation (Sutton, 1999). Our work uses differential inclusion theory to provide the first framework for resolving this problem. We further illustrate via numerical examples how this framework helps explain these algorithms' asymptotic behaviors.",
      "tldr": "We provide the first framework  for analyzing value-based RL methods with function approximation and $\\epsilon$-greedy exploration, answering a long standing open question.",
      "site_url": "https://openreview.net/forum?id=Ms1Zs8s7rg",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Gopalan1",
        "name": "Aditya Gopalan",
        "name_site": null,
        "openreview_id": "~Aditya_Gopalan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~aditya/",
        "dblp_id": "90/9826",
        "google_scholar_url": "dM5_1NsAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ms1Zs8s7rg",
      "title": "Demystifying Approximate RL with $\\epsilon$-greedy Exploration: A Differential Inclusion View",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Q-learning and SARSA(0) with $\\epsilon$-greedy exploration are leading reinforcement learning methods, and their tabular forms converge to the optimal Q-function under reasonable conditions. However, with function approximation, they exhibit unexpected behaviors, such as i.) policy oscillation and chattering, and ii.) convergence to different attractors (possibly even the worst policy) on different runs, ii.) multiple attractors, and iii.) worst policy convergence, apart from the textbook instability. Accordingly, a theory to explain these phenomena has been a long-standing open problem, even for basic linear function approximation (Sutton, 1999). Our work uses differential inclusion theory to provide the first framework for resolving this problem. We further illustrate via numerical examples how this framework helps explain these algorithms' asymptotic behaviors.",
      "tldr": "We provide the first framework  for analyzing value-based RL methods with function approximation and $\\epsilon$-greedy exploration, answering a long standing open question.",
      "site_url": "https://openreview.net/forum?id=Ms1Zs8s7rg",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gugan_Thoppe1",
        "name": "Gugan Thoppe",
        "name_site": null,
        "openreview_id": "~Gugan_Thoppe1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "117/3710",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=X5zV3s8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "O7x_ldrlaO7",
      "title": "Structural Privacy in Graphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) gained popularity to address the tasks over the graph-structured data that best represent many real-world systems. The privacy of the participants of these systems is at risk if the GNNs are not carefully designed. Existing works in privacy-preserving GNNs primarily ensure the privacy of features and labels of a node. In order to ensure complete privacy related to graph data, its structure also needs to be privatized. We provide a method SPGraph to privatize the graph structure by adding noise to the neighborhood data of the node. Our method addresses two challenges in introducing structural privacy in graphs. Applying randomization on the set of actual neighbors to introduce noise leads to a reduction in the degree of a node, which is undesirable. To overcome this first challenge, we introduce $\\lambda$-selector that samples nodes to be added to the set of neighbors. The second challenge is to denoise the neighborhood so that the noise added in the neighborhood does not significantly impact the accuracy. In this view, we use $p$-hop neighborhood to compensate for the loss of actual neighbors in the randomization. We continue to use the node and label privacy as implemented in the previous methods for privacy in GNNs. We conduct extensive experiments over real-world datasets to show the impact of perturbation in the graph structure. ",
      "tldr": "Make the structure of the graph private in addition to the privacy of node features and labels",
      "site_url": "https://openreview.net/forum?id=O7x_ldrlaO7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rucha_Bhalchandra_Joshi1",
        "name": "Rucha Bhalchandra Joshi",
        "name_site": null,
        "openreview_id": "~Rucha_Bhalchandra_Joshi1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://ruchajoshi.github.io/",
        "dblp_id": "266/5932",
        "google_scholar_url": "Xfxu5jQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "O7x_ldrlaO7",
      "title": "Structural Privacy in Graphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) gained popularity to address the tasks over the graph-structured data that best represent many real-world systems. The privacy of the participants of these systems is at risk if the GNNs are not carefully designed. Existing works in privacy-preserving GNNs primarily ensure the privacy of features and labels of a node. In order to ensure complete privacy related to graph data, its structure also needs to be privatized. We provide a method SPGraph to privatize the graph structure by adding noise to the neighborhood data of the node. Our method addresses two challenges in introducing structural privacy in graphs. Applying randomization on the set of actual neighbors to introduce noise leads to a reduction in the degree of a node, which is undesirable. To overcome this first challenge, we introduce $\\lambda$-selector that samples nodes to be added to the set of neighbors. The second challenge is to denoise the neighborhood so that the noise added in the neighborhood does not significantly impact the accuracy. In this view, we use $p$-hop neighborhood to compensate for the loss of actual neighbors in the randomization. We continue to use the node and label privacy as implemented in the previous methods for privacy in GNNs. We conduct extensive experiments over real-world datasets to show the impact of perturbation in the graph structure. ",
      "tldr": "Make the structure of the graph private in addition to the privacy of node features and labels",
      "site_url": "https://openreview.net/forum?id=O7x_ldrlaO7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhankar_Mishra1",
        "name": "Subhankar Mishra",
        "name_site": null,
        "openreview_id": "~Subhankar_Mishra1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.niser.ac.in/~smishra/",
        "dblp_id": "147/8391",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "P5ZTXA7zy6",
      "title": "When Neural ODEs meet Neural Operators",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Differential equation-based neural networks perform well in a variety of deep learning fields. Among those many methods, neural ordinary differential equations (NODEs) are one of the most fundamental work. NODEs have been applied to general downstream tasks such as image classification, time series classification, and image generation. The ODE function of NODEs can be understood as a special type of differential operators, which had been overlooked before.  In this paper, therefore, we study the feasibility of modeling NODEs (or the ODE function of NODEs) as neural operators. Our neural operator-based methods are more rigorous than existing approaches when it comes to learning the differential operator (or the ODE function). To this end, we design a new neural operator structure called branched Fourier neural operator (BFNO), which is suitable for modeling the ODE function. It shows improved performance for several general machine learning tasks, as compared to existing various NODE models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=P5ZTXA7zy6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Pqi9ZxxdjM",
      "title": "Leveraging the Third Dimension in Contrastive Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Self-Supervised Learning (SSL) methods operate on unlabeled data to learn robust representations useful for downstream tasks. Most SSL methods rely on augmentations obtained by transforming the 2D image pixel map.  These augmentations ignore the fact that  biological vision takes place in an immersive  three-dimensional, temporally contiguous environment, and that low-level biological vision relies heavily on depth cues. Using a signal provided by a pretrained state-of-the-art RGB-to-depth model (the Depth Prediction Transformer, Ranftl et al., 2021), we explore two distinct approaches to incorporating depth signals into the SSL framework. First, we evaluate contrastive learning using an RGB+depth input representation. Second, we use the depth signal to generate novel views from slightly different camera positions, thereby producing a 3D augmentation for contrastive learning. We evaluate these two approaches on three different SSL methods---BYOL, SimSiam, and SwAV---using ImageNette (10 class subset of ImageNet) and ImageNet-100. We find that both approaches to incorporating depth signals improve the robustness and generalization of the baseline SSL methods, though the first approach (with depth-channel concatenation) is superior.",
      "tldr": "Depth signal improves contrastive learning",
      "site_url": "https://openreview.net/forum?id=Pqi9ZxxdjM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PxohstFQm9q",
      "title": "Simplicity bias in $1$-hidden layer neural networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works \\citep{shah2020pitfalls,chen2021intriguing} have demonstrated that neural networks exhibit extreme \\emph{simplicity bias} (SB). That is,  they learn \\emph{only the simplest} features  to solve a task at hand, even in the presence of other, more robust but more complex features. Due to lack of a general and rigorous definition of \\emph{features}, these works showcase SB on \\emph{semi-synthetic} datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier. \n\nIn this work, we rigorously define as well as thoroughly establish SB for \\emph{one hidden layer} neural networks. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable ($1$-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier,  (iii) empirically, we show that models trained on \\emph{real} datasets such as Imagenette and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in  models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.",
      "tldr": "Gradient Descent on 1-hidden-layer neural network learns a function of essentially a lower dimensional projection of the input.",
      "site_url": "https://openreview.net/forum?id=PxohstFQm9q",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Depen_Morwani1",
        "name": "Depen Morwani",
        "name_site": null,
        "openreview_id": "~Depen_Morwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "277/5200",
        "google_scholar_url": "vOngxFUAAAAJ",
        "orcid": null,
        "linkedin_url": "depen-morwani-070298122/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Harvard University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PxohstFQm9q",
      "title": "Simplicity bias in $1$-hidden layer neural networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent works \\citep{shah2020pitfalls,chen2021intriguing} have demonstrated that neural networks exhibit extreme \\emph{simplicity bias} (SB). That is,  they learn \\emph{only the simplest} features  to solve a task at hand, even in the presence of other, more robust but more complex features. Due to lack of a general and rigorous definition of \\emph{features}, these works showcase SB on \\emph{semi-synthetic} datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier. \n\nIn this work, we rigorously define as well as thoroughly establish SB for \\emph{one hidden layer} neural networks. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable ($1$-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier,  (iii) empirically, we show that models trained on \\emph{real} datasets such as Imagenette and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in  models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.",
      "tldr": "Gradient Descent on 1-hidden-layer neural network learns a function of essentially a lower dimensional projection of the input.",
      "site_url": "https://openreview.net/forum?id=PxohstFQm9q",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~jatin_batra1",
        "name": "jatin batra",
        "name_site": null,
        "openreview_id": "~jatin_batra1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "157/6041",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Institute of Fundamental Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "QrdSiDAv5ek",
      "title": "FACS: FAST ADAPTIVE CHANNEL SQUEEZING",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Channel squeezing is one of the central operations performed in CNN bottlenecks to reduce the number of channels in a feature map. This operation is carried out by using a 1 × 1 pointwise convolution which constitutes a significant amount of computations and parameters in a given network. ResNet-50 for instance, consists of 16 such layers which form 33% of total layers and 25% (1.05B/4.12B) of total FLOPs or computations. In the light of their predominance, we propose a novel “Fast Adaptive Channel Squeezing” module which carries out the squeezing operation in a computationally efficient manner. The key benefit of FACS is that it neither alters the number of parameters nor affects the accuracy of a given network. When plugged into diverse CNNs architectures, namely ResNet, VGG, and MobileNet-v2, FACS achieves state-of-the-art performance on ImageNet and CIFAR datasets at dramatically reduced FLOPs. FACS also cuts the training time significantly, and lowers the latency which is particularly advantageous for fast inference on edge devices. The source-code will be made publicly available.",
      "tldr": "Computationally Efficient Channel Squeezing in CNNs  with high representation power",
      "site_url": "https://openreview.net/forum?id=QrdSiDAv5ek",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar2",
        "name": "Ashish Kumar",
        "name_site": null,
        "openreview_id": "~Ashish_Kumar2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashishkumar822.github.io",
        "dblp_id": "34/5378-6",
        "google_scholar_url": "n-oRDEYAAAAJ",
        "orcid": null,
        "linkedin_url": "ashishkumar822/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "QrdSiDAv5ek",
      "title": "FACS: FAST ADAPTIVE CHANNEL SQUEEZING",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Channel squeezing is one of the central operations performed in CNN bottlenecks to reduce the number of channels in a feature map. This operation is carried out by using a 1 × 1 pointwise convolution which constitutes a significant amount of computations and parameters in a given network. ResNet-50 for instance, consists of 16 such layers which form 33% of total layers and 25% (1.05B/4.12B) of total FLOPs or computations. In the light of their predominance, we propose a novel “Fast Adaptive Channel Squeezing” module which carries out the squeezing operation in a computationally efficient manner. The key benefit of FACS is that it neither alters the number of parameters nor affects the accuracy of a given network. When plugged into diverse CNNs architectures, namely ResNet, VGG, and MobileNet-v2, FACS achieves state-of-the-art performance on ImageNet and CIFAR datasets at dramatically reduced FLOPs. FACS also cuts the training time significantly, and lowers the latency which is particularly advantageous for fast inference on edge devices. The source-code will be made publicly available.",
      "tldr": "Computationally Efficient Channel Squeezing in CNNs  with high representation power",
      "site_url": "https://openreview.net/forum?id=QrdSiDAv5ek",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laxmidhar_Behera1",
        "name": "Laxmidhar Behera",
        "name_site": null,
        "openreview_id": "~Laxmidhar_Behera1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~lbehera/",
        "dblp_id": "14/1412",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=QWTcyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "laxmidhar-behera-a74a5b174/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RIJM-pJF_3K",
      "title": "Causally Constrained Data Synthesis For Private Data Release",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Data privacy is critical in many decision-making contexts, such as healthcare and finance. A common mechanism is to create differentially private synthetic data using generative models. Such data generation reflects certain statistical properties of the original data, but often has an unacceptable privacy vs. utility trade-off. Since natural data inherently exhibits causal structure, we propose incorporating \\emph{causal information} into the training process to favorably navigate the aforementioned trade-off. Under certain assumptions for linear gaussian models and a broader class of models, we theoretically prove that causally informed generative models provide better differential privacy guarantees than their non-causal counterparts. We evaluate our proposal using variational autoencoders, and demonstrate that the trade-off is mitigated through better utility for comparable privacy.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RIJM-pJF_3K",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RePt5K6wPux",
      "title": "Code Means More Than Plain Language: Bringing Syntax Structure Awareness To Algorithmic Problem Solution Generation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Program Synthesis (PS) is the task of building computer programs that satisfy problem specifications. Large-scale pre-trained language models treat the PS as a sequence prediction task, which has gained vivid popularity recently. However, these methods heavily rely on the conventional Natural Language Processing (NLP) tokenizers, which overlooks the rich structural/syntax information in the code. In this work, we posit that the syntax structures help generate syntax error-free and algorithmically correct programs. If the program syntax structures can be integrated into the tokenizer, the program representation space could be significantly simplified. To this end, we propose a new end-to-end framework named ASTer, coupled with our novel syntax-aware tokenization design toolkit. More specifically, our tokenizer encodes and decodes the program by its syntax roles and contents, not by what is superficially shown on the strings. The ASTer encompasses a novel sample-wise and token-wise attention mechanism, and avails the benefits of training with the syntactically aligned samples from our tokenization toolkit. Extensive evaluations show superior performance against state-of-the-arts, which confirms that bringing syntax knowledge into the language model can help better capture the data structure and simplify the search space. All of our codes will be publicly available upon acceptance. ",
      "tldr": "The first work to introduce syntax tree structure in programming synthesis",
      "site_url": "https://openreview.net/forum?id=RePt5K6wPux",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~S_P_Sharan1",
        "name": "S P Sharan",
        "name_site": null,
        "openreview_id": "~S_P_Sharan1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://spsharan.com/",
        "dblp_id": "324/6204",
        "google_scholar_url": "1NtGcNIAAAAJ",
        "orcid": "0000-0002-6298-6464",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "NEC Laboratories (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RnH_0iL4xao",
      "title": "Towards Conditionally Dependent Masked Language Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Masked language modeling has proven to be an effective paradigm for learning representations of language. However, when multiple tokens are masked out, the masked language model's (MLM) distribution over the masked positions assumes that the masked tokens are conditionally independent given the unmasked tokens---an assumption that does not hold in practice. Existing work addresses this limitation by interpreting the sum of unary scores (i.e., the logits or the log probabilities of  single tokens when conditioned on all others) as the log potential a Markov random field (MRF). While this new model no longer makes any independence assumptions, it remains unclear whether this approach (i) results in a good probabilistic model of language and further (ii) derives a model that is faithful (i.e., has matching unary distributions) to the original model. This paper studies MRFs derived this way in a controlled setting where only two tokens are masked out at a time, which makes it possible to compute exact distributional properties. We find that such pairwise MRFs are often worse probabilistic models of language from a perplexity standpoint, and moreover have unary distributions that do not match the unary distributions of the original MLM. We then study a statistically-motivated iterative optimization algorithm for deriving joint pairwise distributions that are more compatible with the original unary distributions. While this iterative approach outperforms the MRF approach, the algorithm itself is too expensive to be practical. We thus amortize this optimization process through a parameterized feed-forward layer that learns to modify the original MLM's pairwise distributions to be both non-independent and faithful, and find that this approach outperforms the MLM for scoring pairwise tokens.",
      "tldr": "We study the limitations of MRFs defined from MLMs' unary conditionals, and propose alternatives that are either better (from a probabilistic modeling standpoint) or faster to run",
      "site_url": "https://openreview.net/forum?id=RnH_0iL4xao",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 2,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RqJZTlQMph",
      "title": "Weakly Supervised Neuro-Symbolic Image Manipulation via Multi-Hop Complex Instructions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We are interested in image manipulation via natural language text – a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches (Mao et al., 2019) (NSCL) has been quite effective for solving VQA as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NeuroSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NeuroSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation.",
      "tldr": "We propose a weakly supervised neuro-symbolic approach for the problem of image manipulation using text instructions.",
      "site_url": "https://openreview.net/forum?id=RqJZTlQMph",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Poorva_Garg1",
        "name": "Poorva Garg",
        "name_site": null,
        "openreview_id": "~Poorva_Garg1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RqJZTlQMph",
      "title": "Weakly Supervised Neuro-Symbolic Image Manipulation via Multi-Hop Complex Instructions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We are interested in image manipulation via natural language text – a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches (Mao et al., 2019) (NSCL) has been quite effective for solving VQA as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NeuroSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NeuroSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation.",
      "tldr": "We propose a weakly supervised neuro-symbolic approach for the problem of image manipulation using text instructions.",
      "site_url": "https://openreview.net/forum?id=RqJZTlQMph",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal2",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal2",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "MZ8N49AAAAAJ",
        "orcid": "0000-0001-7297-374X",
        "linkedin_url": "arnab-mondal-a4448a18/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RqJZTlQMph",
      "title": "Weakly Supervised Neuro-Symbolic Image Manipulation via Multi-Hop Complex Instructions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We are interested in image manipulation via natural language text – a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches (Mao et al., 2019) (NSCL) has been quite effective for solving VQA as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NeuroSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NeuroSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation.",
      "tldr": "We propose a weakly supervised neuro-symbolic approach for the problem of image manipulation using text instructions.",
      "site_url": "https://openreview.net/forum?id=RqJZTlQMph",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dinesh_Khandelwal2",
        "name": "Dinesh Khandelwal",
        "name_site": null,
        "openreview_id": "~Dinesh_Khandelwal2",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://research.ibm.com/people/dinesh-khandelwal",
        "dblp_id": "177/0164",
        "google_scholar_url": "Pi-SqXwAAAAJ",
        "orcid": null,
        "linkedin_url": "dinesh-khandelwal-68689420/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RqJZTlQMph",
      "title": "Weakly Supervised Neuro-Symbolic Image Manipulation via Multi-Hop Complex Instructions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We are interested in image manipulation via natural language text – a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches (Mao et al., 2019) (NSCL) has been quite effective for solving VQA as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NeuroSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NeuroSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation.",
      "tldr": "We propose a weakly supervised neuro-symbolic approach for the problem of image manipulation using text instructions.",
      "site_url": "https://openreview.net/forum?id=RqJZTlQMph",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 7,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TDUMUFa5zz",
      "title": "Divide-and-Cluster: Spatial Decomposition Based Hierarchical Clustering",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "This paper is about increasing the computational efficiency of clustering algorithms. Many clustering algorithms are based on properties of relative locations of points, globally or locally, e.g., interpoint distances and nearest neighbor distances. This amounts to using a lower dimensional space than the full dimensionality $D$ of the space in which the points are embedded.  We present a clustering algorithm, Divide-and-Cluster (DAC), which detects local clusters in small neighborhoods obtained by recursive tessellation of space, and then merges them hierarchically, following the Divide-and-Conquer paradigm. This significantly reduces computation time which may otherwise grow nonlinearly number $n$ of points. We define locality as hypercubical neighborhoods in a recursive hypercubical decomposition of space, represented by a tree. Clusters are detected within each hypercube, and merged with those from neighboring hypercubes while traversing up the tree. We expect DAC to perform better than many other algorithms because (a) as clusters merge into larger clusters (components), their number steadily decreases vs the number of points, and (b) we cluster only neighboring components. The ordering of component appearances also simultaneously yields a cluster hierarchy (tree). Further, our use of small neighborhoods allows piecewise uniform approximation of large, nonuniform, arbitrary shaped clusters, thus avoiding the need for global cluster models. We experimentally verify the correctness of detected clusters on a variety of datasets, posing a variety of challenges, as well as show that DAC’s runtime is significantly better than representative algorithms of other types, particularly for increasing values of $n$.\n",
      "tldr": "This paper clusters n points located in a D-dimensional space by detecting their mutual clustering affinity within local neighborhoods, using more efficient local computations, and then hierarchically growing the local clusters outward.",
      "site_url": "https://openreview.net/forum?id=TDUMUFa5zz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Akshat_Sharma1",
        "name": "Akshat Sharma",
        "name_site": null,
        "openreview_id": "~Akshat_Sharma1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "351/4945",
        "google_scholar_url": "xRRx-NQAAAAJ",
        "orcid": null,
        "linkedin_url": "akshat-sharma-0b19881b2/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TQZkycVeMIy",
      "title": "Test-time Adaptation for Segmentation via Image Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We consider the problem of segmenting scenes into constituent objects and their parts. Current supervised visual detectors, though impressive within their training distribution, often fail to segment out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses can be insufficient for instance segmentation tasks, without also considering architectural inductive biases. For image segmentation, recent slot-centric generative models break such dependence on supervision by attempting to segment scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Generating Fast and Slow Networks (GFS-Nets), a semi-supervised instance segmentation model equipped with a slot-centric image or point-cloud rendering component that is adapted per scene at test time through gradient descent on reconstruction or novel view synthesis objectives. We show that test-time adaptation greatly improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in several 3D and 2D scene segmentation benchmarks and show substantial out-of-distribution performance improvements against state-of-the-art supervised feed forward detectors and self-supervised domain adaptation models.",
      "tldr": "We propose a test-time adaptation framework that optimizes image synthesis loss to improve image segmentation.",
      "site_url": "https://openreview.net/forum?id=TQZkycVeMIy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.8,
        "confidence_std": 1.16619037896906,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TQZkycVeMIy",
      "title": "Test-time Adaptation for Segmentation via Image Synthesis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We consider the problem of segmenting scenes into constituent objects and their parts. Current supervised visual detectors, though impressive within their training distribution, often fail to segment out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses can be insufficient for instance segmentation tasks, without also considering architectural inductive biases. For image segmentation, recent slot-centric generative models break such dependence on supervision by attempting to segment scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Generating Fast and Slow Networks (GFS-Nets), a semi-supervised instance segmentation model equipped with a slot-centric image or point-cloud rendering component that is adapted per scene at test time through gradient descent on reconstruction or novel view synthesis objectives. We show that test-time adaptation greatly improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in several 3D and 2D scene segmentation benchmarks and show substantial out-of-distribution performance improvements against state-of-the-art supervised feed forward detectors and self-supervised domain adaptation models.",
      "tldr": "We propose a test-time adaptation framework that optimizes image synthesis loss to improve image segmentation.",
      "site_url": "https://openreview.net/forum?id=TQZkycVeMIy",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Aggarwal4",
        "name": "Gaurav Aggarwal",
        "name_site": null,
        "openreview_id": "~Gaurav_Aggarwal4",
        "position": 6,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "14/5218",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9XiIwDQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 2.8,
        "confidence_std": 1.16619037896906,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "U3_J25hAoRQ",
      "title": "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional Reinforcement Learning (RL) algorithms assume the distribution of the data to be uniform or mostly uniform. However, this is not the case with most real-world applications like autonomous driving or in nature, where animals roam. Some objects are encountered frequently, and most of the remaining experiences occur rarely; the resulting distribution is called \\emph{Zipfian}. Taking inspiration from the theory of \\emph{complementary learning systems}, an architecture for learning from Zipfian distributions is proposed where long tail states are discovered in an unsupervised manner and states along with their recurrent activation are kept longer in episodic memory. The recurrent activations are then reinstated from episodic memory using a similarity search, giving weighted importance. The proposed architecture yields improved performance in a Zipfian task over conventional architectures. Our method outperforms IMPALA by a significant margin of 20.3\\% when maps/objects occur with a uniform distribution and by 50.2\\% on the rarest 20\\% of the distribution.",
      "tldr": "Improving learning in long-tailed RL environments using momentum boosted episodic memory.",
      "site_url": "https://openreview.net/forum?id=U3_J25hAoRQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dolton_Milagres_Fernandes1",
        "name": "Dolton Milagres Fernandes",
        "name_site": null,
        "openreview_id": "~Dolton_Milagres_Fernandes1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "j20j_ggAAAAJ",
        "orcid": null,
        "linkedin_url": "dolton-fernandes/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UFaOH39SZ4y",
      "title": "Monkeypox with Cross Infection Hypothesis via Epidemiological Mode",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "A new re-emerging infectious disease of monkeypox 2022 is structurally related to smallpox that is induced by the monkeypox viruses and has caused 59,606 active cases with 18 deaths up to September 15, 2022. To end this ongoing epidemic, there is a need for population-wide control policies like reducing social interaction by keeping social distance, treatment of infected individuals, and restriction on animals, etc. We forecast the progression of the epidemic and come up with an efficient control mechanism by formulating a mathematical model. The biological feasibility and dynamical behavior of the proposed model are then investigated together with sensitivity analysis to obtain the effect of various epidemic parameters mitigating the spread of the disease. Subsequently, by taking non-pharmaceutical and pharmaceutical intervention strategies as control measures, an optimal control theory is applied to mitigate the fatality of the disease to minimize the infectious population and reduce the cost of controls, we construct an objective functional and solve it by using Pontryagin’s maximum principle. Finally, extensive numerical simulations are performed to show the impact of the application of intervention mechanisms in controlling the transmission of the monkeypox epidemic.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=UFaOH39SZ4y",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Vf2DK1Ol0ed",
      "title": "A Benchmark Dataset for Learning from Label Proportions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning from label proportions (LLP) has recently emerged as an important technique of weakly supervised learning on aggregated labels. In LLP, a model is trained on groups (a.k.a bags) of feature-vectors and their corresponding label proportions to predict labels for individual feature-vectors. While previous works have developed a variety of techniques for LLP, including novel loss functions, model architectures and their optimization, they typically evaluated their methods on pseudo-synthetically generated LLP training data using common small scale supervised learning datasets by randomly sampling or partitioning their instances into bags.  Despite growing interest in this important task there are no large scale open source LLP benchmarks to compare various approaches. Construction of such a benchmark is hurdled by two challenges a) lack of natural large scale LLP like data, b) large number of mostly artificial methods of forming bags from instance level datasets. \nIn this paper we propose LLP-Bench: a large scale LLP benchmark constructed from   the Criteo Kaggle CTR dataset. We do an in-depth, systematic study of the Criteo dataset and propose a methodology to create a  benchmark as a collection of diverse and large scale LLP datasets. We choose the Criteo dataset since it admits multiple natural collections of bags formed by grouping  subsets of its 26 categorical features. We analyze all bag collections obtained through grouping by one or two categorical features, in terms of their bag-level statistics as well as embedding based distance metrics quantifying the geometric separation of bags. We then propose to include in LLP-Bench a few groupings to fairly represent real world bag distributions.\nWe also measure the performance of state of the art models, loss functions (adapted to LLP) and optimizers on LLP-Bench. We perform a series of ablations and explain the performance of various techniques  on LLP-Bench. To the best of our knowledge LLP-Bench is the first open source benchmark for the LLP task. We hope that the proposed benchmark and the evaluation methodology will be used by ML researchers and practitioners to better understand and hence devise state of art LLP algorithms. ",
      "tldr": "A Benchmark based on Criteo Kaggle CTR dataset for Learning from Label Proportions",
      "site_url": "https://openreview.net/forum?id=Vf2DK1Ol0ed",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohith_Pokala1",
        "name": "Mohith Pokala",
        "name_site": null,
        "openreview_id": "~Mohith_Pokala1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "www.linkedin.com/in/mohith-pokala-35385a194",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "XG_LmeoU8Xq",
      "title": "Graduated Non-Convexity for Robust Self-Trained Language Understanding",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Self-training has been proved an efficient strategy for unsupervised fine-tuning of language models using unlabeled data and model-generated pseudo-labels. However, the performance of self-trained models is unstable under different settings of training and evaluation data, influenced by both data distribution and pseudo-label accuracy. In this work, we propose an outlier robust self-training method based on graduated non-convexity (GNC) to mitigate the problem. We construct self-training as a non-convex optimization problem with outlier training examples. The models are self-trained with robust cost functions based according to Black-Rangarajan Duality. The algorithm learns slack variables as the loss weights for all training samples. The slack variables are used to calibrate the loss items during training to update the model parameters. The calibrated loss items lead to more robust self-trained models against different training and evaluation data and tasks. We conducted experiments on few-shot natural language understanding tasks with labeled and unlabeled data examples. Experiment results show that the proposed loss calibration method improves the performance and stability of self-training under different training tasks and data examples, and also benefits the robustness against adversarial evaluation corpora. ",
      "tldr": "Robust self-trained language understanding against pseudo labeling noises, data imbalance, overfitting, and adversarial evaluation data.",
      "site_url": "https://openreview.net/forum?id=XG_LmeoU8Xq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 2,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xj1orI5p6Sv",
      "title": "Corruption Depth: Analysis of DNN depth for Misclassification",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Many large and complex deep neural networks have been shown to provide higher accuracy. However, very little is known about the relationship between the complexity of the input data along with the type of noise and the depth needed for correct classification.  Existing studies do not address the issue of common corruption adequately, especially in understanding what impact these corruptions leave on the individual part of a deep neural network. Therefore, we can safely assume that the classification (or misclassification) might be happening at a particular layer(s) of a network that accumulates to draw a final correct or incorrect prediction. In this paper, we introduce a novel concept called {\\bf corruption depth}, which identifies the location of the network layer/depth until the misclassification persists. We assert that the identification of such layers will help in better design of the network by pruning certain layers in comparison to the purification of the entire network which is computationally heavy to do. Through our extensive experiments, we present a coherent study in comparison to the existing studies which are diverse in understanding the processing of examples through the network. Our approach also illustrates different philosophies of example memorization and a one-dimensional view of sample or query difficulty. We believe that the understanding of the corruption depth can \\textbf{open a new dimension of model explainability}, where in place of just visualizing the attention map, the classification progress can be seen throughout the network.",
      "tldr": "Identify the layers lead to robust image classification ",
      "site_url": "https://openreview.net/forum?id=Xj1orI5p6Sv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Akshay_Agarwal1",
        "name": "Akshay Agarwal",
        "name_site": null,
        "openreview_id": "~Akshay_Agarwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/iiitd.ac.in/agarwalakshay/home",
        "dblp_id": "152/3672-1",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=LrW6Hb4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xj1orI5p6Sv",
      "title": "Corruption Depth: Analysis of DNN depth for Misclassification",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Many large and complex deep neural networks have been shown to provide higher accuracy. However, very little is known about the relationship between the complexity of the input data along with the type of noise and the depth needed for correct classification.  Existing studies do not address the issue of common corruption adequately, especially in understanding what impact these corruptions leave on the individual part of a deep neural network. Therefore, we can safely assume that the classification (or misclassification) might be happening at a particular layer(s) of a network that accumulates to draw a final correct or incorrect prediction. In this paper, we introduce a novel concept called {\\bf corruption depth}, which identifies the location of the network layer/depth until the misclassification persists. We assert that the identification of such layers will help in better design of the network by pruning certain layers in comparison to the purification of the entire network which is computationally heavy to do. Through our extensive experiments, we present a coherent study in comparison to the existing studies which are diverse in understanding the processing of examples through the network. Our approach also illustrates different philosophies of example memorization and a one-dimensional view of sample or query difficulty. We believe that the understanding of the corruption depth can \\textbf{open a new dimension of model explainability}, where in place of just visualizing the attention map, the classification progress can be seen throughout the network.",
      "tldr": "Identify the layers lead to robust image classification ",
      "site_url": "https://openreview.net/forum?id=Xj1orI5p6Sv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mayank_Vatsa1",
        "name": "Mayank Vatsa",
        "name_site": null,
        "openreview_id": "~Mayank_Vatsa1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://home.iitj.ac.in/~mvatsa/",
        "dblp_id": "58/323",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=-DAxp-cAAAAJ",
        "orcid": "0000-0001-5952-2274",
        "linkedin_url": "mayankvatsa/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xj1orI5p6Sv",
      "title": "Corruption Depth: Analysis of DNN depth for Misclassification",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Many large and complex deep neural networks have been shown to provide higher accuracy. However, very little is known about the relationship between the complexity of the input data along with the type of noise and the depth needed for correct classification.  Existing studies do not address the issue of common corruption adequately, especially in understanding what impact these corruptions leave on the individual part of a deep neural network. Therefore, we can safely assume that the classification (or misclassification) might be happening at a particular layer(s) of a network that accumulates to draw a final correct or incorrect prediction. In this paper, we introduce a novel concept called {\\bf corruption depth}, which identifies the location of the network layer/depth until the misclassification persists. We assert that the identification of such layers will help in better design of the network by pruning certain layers in comparison to the purification of the entire network which is computationally heavy to do. Through our extensive experiments, we present a coherent study in comparison to the existing studies which are diverse in understanding the processing of examples through the network. Our approach also illustrates different philosophies of example memorization and a one-dimensional view of sample or query difficulty. We believe that the understanding of the corruption depth can \\textbf{open a new dimension of model explainability}, where in place of just visualizing the attention map, the classification progress can be seen throughout the network.",
      "tldr": "Identify the layers lead to robust image classification ",
      "site_url": "https://openreview.net/forum?id=Xj1orI5p6Sv",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Richa_Singh1",
        "name": "Richa Singh",
        "name_site": null,
        "openreview_id": "~Richa_Singh1",
        "position": 4,
        "gender": "F",
        "homepage_url": "http://home.iitj.ac.in/~richa/",
        "dblp_id": "75/3512",
        "google_scholar_url": "okqK5UAAAAAJ",
        "orcid": "0000-0003-4060-4573",
        "linkedin_url": "richa-singh-40ba237/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "YmVcNC2oCzq",
      "title": "Transmission Dynamics of Hepatitis B: Analysis and Control",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The infection of hepatitis B attacks the liver and can produce acute and chronic diseases, while it is a major health problem and life-threatening around the globe. The control of this infection is a difficult task due to several reasons such as variation of human behavior, proper medication, vaccination, and existence of a large number of carries, etc., but understanding the dynamics of the infection helps to design appropriate control strategies. Thus, a proper complex dynamical system is needed to find the stability conditions and propose intervention strategies for forecasting the control of hepatitis B virus transmission. We formulate a model that will be helpful to investigate the temporal dynamics and suggest control strategies for hepatitis B infection. The well-posedness of the proposed model will be shown, and used to find the threshold parameter to analyze the model equilibria and its stability. We also perform the sensitive analysis of the threshold quantity to quantify the most sensitive epidemic parameters. Based on the temporal dynamics and sensitivity, we investigate effective methods to minimize the infection of hepatitis B, and develop the algorithms to support the theoretical results with the help of numerical simulations.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=YmVcNC2oCzq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "YzOEjv-7nP",
      "title": "PALM: Preference-based Adversarial Manipulation against Deep Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "To improve the robustness of DRL agents, it is important to study their vulnerability under  adversarial attacks that would lead to extreme behaviors desired by adversaries. Preference-based RL (PbRL) aims for learning desired behaviors with human preferences. In this paper, we propose PALM, a preference-based adversarial manipulation method against DRL agents  which adopts human preferences to perform targeted attacks with the assistance of an intention policy and a weighting function. The intention policy is trained based on the PbRL framework to guide the adversarial policy  to mitigate restrictions of the victim policy during exploration, and the weighting function learns weight assignment to improve the performance of the adversarial policy. Theoretical analysis demonstrates that PALM converges to critical points under some mild conditions. Empirical results on a few manipulation tasks of Meta-world show that PALM exceeds the performance of state-of-the-art adversarial attack methods under the targeted setting. Additionally, we show the vulnerability of the offline RL agents by fooling them into behaving as human desires on several Mujoco tasks. Our code and videos are available in https://sites.google.com/view/palm-adversarial-attack.",
      "tldr": "A preference-based adversarial attack method that manipulates the victim policy to perform human desired behaviors.",
      "site_url": "https://openreview.net/forum?id=YzOEjv-7nP",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 4,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ZEXh0XyO2hh",
      "title": "Learning Binary Networks on Long-Tailed Distributions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In deploying deep models to real world scenarios, there are a number of issues including computational resource constraints and long-tailed data distributions. For the first time in the literature, we address the combined challenge of learning long-tailed distributions under the extreme resource constraints of using binary networks as backbones. Specifically, we propose a framework of calibrating off-the-shelf pretrained full precision weights that are learned on $\\textit{non-long-tailed}$ distributions when training binary networks on long-tailed datasets. In the framework, we additionally propose a novel adversarial balancing and a multi-resolution learning method for better generalization to diverse semantic domains and input resolutions. We conduct extensive empirical evaluations on 15 datasets including newly derived long-tailed datasets from existing balanced datasets, which is the largest benchmark in the literature. Our empirical studies show that our proposed method outperforms prior arts by large margins, $\\textit{e.g.}$, at least $+14.33\\%$ on average.",
      "tldr": "We propose the first method to learn binary networks on long-tailed distributions in the literature.",
      "site_url": "https://openreview.net/forum?id=ZEXh0XyO2hh",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 1.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ZLv-8v0Sp_H",
      "title": "Communication Efficient Fair Federated Recommender System",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Federated Recommender Systems (FRSs) aim to provide recommendations to clients in a distributed manner with privacy preservation. FRSs suffer from high communication costs due to the communication between the server and many clients. Some past literature on federated supervised learning shows that sampling clients randomly improve communication efficiency without jeopardizing accuracy. However, each user is considered a separate client in FRS and clients communicate only item gradients. Thus, incorporating random sampling and determining the number of clients to be sampled in each communication round to retain the model's accuracy in FRS becomes challenging. This paper provides sample complexity bounds on the number of clients that must be sampled in an FRS to preserve accuracy. Next, we consider the issue of demographic bias in FRS, quantified as the difference in the average error rates across different groups. Supervised learning algorithms mitigate the group bias by adding the fairness constraint in the training loss, which requires sharing protected attributes with the server. This is prohibited in a federated setting to ensure clients' privacy. We design \\ouralgo, a Random Sampling based Fair Federated Recommender System, which trains to achieve a fair global model. In addition, it also trains local clients towards a fair global model to reduce demographic bias at the client level without the need to share their protected attributes. We empirically demonstrate all our results across the two most popular real-world datasets (ML1M, ML100k) and different sensitive features (age and gender) to prove that RS-FairFRS helps reduce communication cost and demographic bias with improved model accuracy.\n",
      "tldr": "A random sampling based Fair Federated Recommender System.",
      "site_url": "https://openreview.net/forum?id=ZLv-8v0Sp_H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sujit_Gujar1",
        "name": "Sujit Gujar",
        "name_site": null,
        "openreview_id": "~Sujit_Gujar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/sujitgujar/",
        "dblp_id": "https://www.linkedin.com/in/sujitgujar/",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9yxKajsAAAAJ",
        "orcid": null,
        "linkedin_url": "sujitgujar/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ZLv-8v0Sp_H",
      "title": "Communication Efficient Fair Federated Recommender System",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Federated Recommender Systems (FRSs) aim to provide recommendations to clients in a distributed manner with privacy preservation. FRSs suffer from high communication costs due to the communication between the server and many clients. Some past literature on federated supervised learning shows that sampling clients randomly improve communication efficiency without jeopardizing accuracy. However, each user is considered a separate client in FRS and clients communicate only item gradients. Thus, incorporating random sampling and determining the number of clients to be sampled in each communication round to retain the model's accuracy in FRS becomes challenging. This paper provides sample complexity bounds on the number of clients that must be sampled in an FRS to preserve accuracy. Next, we consider the issue of demographic bias in FRS, quantified as the difference in the average error rates across different groups. Supervised learning algorithms mitigate the group bias by adding the fairness constraint in the training loss, which requires sharing protected attributes with the server. This is prohibited in a federated setting to ensure clients' privacy. We design \\ouralgo, a Random Sampling based Fair Federated Recommender System, which trains to achieve a fair global model. In addition, it also trains local clients towards a fair global model to reduce demographic bias at the client level without the need to share their protected attributes. We empirically demonstrate all our results across the two most popular real-world datasets (ML1M, ML100k) and different sensitive features (age and gender) to prove that RS-FairFRS helps reduce communication cost and demographic bias with improved model accuracy.\n",
      "tldr": "A random sampling based Fair Federated Recommender System.",
      "site_url": "https://openreview.net/forum?id=ZLv-8v0Sp_H",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shweta_Jain5",
        "name": "Shweta Jain",
        "name_site": null,
        "openreview_id": "~Shweta_Jain1",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://sites.google.com/site/shwetajainiisc/home",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Ropar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_3Lk3cUWSI",
      "title": "Off Policy Average Reward Actor Critic with Deterministic Policy Search",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The average reward criterion is relatively less explored as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this paper, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We show a finite time analysis of the resulting three-timescale stochastic approximation scheme and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed algorithm and observe better empirical performance compared to state-of-the-art on-policy average reward actor-critic algorithms over MuJoCo based environments. ",
      "tldr": "This paper proposes actor critic algorithm with deterministic policy for the average reward criterion",
      "site_url": "https://openreview.net/forum?id=_3Lk3cUWSI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Naman_Saxena1",
        "name": "Naman Saxena",
        "name_site": null,
        "openreview_id": "~Naman_Saxena1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "naman-saxena-718b7b175/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_3Lk3cUWSI",
      "title": "Off Policy Average Reward Actor Critic with Deterministic Policy Search",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The average reward criterion is relatively less explored as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this paper, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We show a finite time analysis of the resulting three-timescale stochastic approximation scheme and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed algorithm and observe better empirical performance compared to state-of-the-art on-policy average reward actor-critic algorithms over MuJoCo based environments. ",
      "tldr": "This paper proposes actor critic algorithm with deterministic policy for the average reward criterion",
      "site_url": "https://openreview.net/forum?id=_3Lk3cUWSI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhojyoti_Khastagir1",
        "name": "Subhojyoti Khastagir",
        "name_site": null,
        "openreview_id": "~Subhojyoti_Khastagir1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "271/3391",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "subhojyoti-khastagir-2a4716152/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_3Lk3cUWSI",
      "title": "Off Policy Average Reward Actor Critic with Deterministic Policy Search",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The average reward criterion is relatively less explored as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this paper, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We show a finite time analysis of the resulting three-timescale stochastic approximation scheme and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed algorithm and observe better empirical performance compared to state-of-the-art on-policy average reward actor-critic algorithms over MuJoCo based environments. ",
      "tldr": "This paper proposes actor critic algorithm with deterministic policy for the average reward criterion",
      "site_url": "https://openreview.net/forum?id=_3Lk3cUWSI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shishir_N_Y1",
        "name": "Shishir N Y",
        "name_site": null,
        "openreview_id": "~Shishir_N_Y1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.shishirny.com",
        "dblp_id": "144/4182",
        "google_scholar_url": "is0x16gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_3Lk3cUWSI",
      "title": "Off Policy Average Reward Actor Critic with Deterministic Policy Search",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The average reward criterion is relatively less explored as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this paper, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We show a finite time analysis of the resulting three-timescale stochastic approximation scheme and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed algorithm and observe better empirical performance compared to state-of-the-art on-policy average reward actor-critic algorithms over MuJoCo based environments. ",
      "tldr": "This paper proposes actor critic algorithm with deterministic policy for the average reward criterion",
      "site_url": "https://openreview.net/forum?id=_3Lk3cUWSI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shalabh_Bhatnagar1",
        "name": "Shalabh Bhatnagar",
        "name_site": null,
        "openreview_id": "~Shalabh_Bhatnagar1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~shalabh/",
        "dblp_id": "71/2542",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=cj3fJJsbjAoC",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_NlE9YiyXKb",
      "title": "Tessellated Neural Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": " Data-driven deep learning approaches for image classification are prone to adversarial attacks. An adversarial image which is sufficiently close (visually indistinguishable) from a true image of its representative class can often be misclassified to be a member of a different class. It is possible for attackers to exploit the high dimensionality of image representations, as learned by the neural models, to identify adversarial perturbations. To mitigate this problem, we propose a novel divide-and-conquer based approach of tessellating a base network architecture (e.g., a ResNet used in our experiments). The tessellated network learns the parameterized representations of each non-overlapping sub-region or tiles within an image, independently, and then learns how to combine these representations to finally estimate the class of the input image. We investigate two different modes of tessellation, namely periodic, comprised of regular square-shaped tiles, and aperiodic, comprised of rectangles of different dimensions. Experiments demonstrate that the tessellated extension of two standard deep neural models leads to a better defence against a number of standard adversarial attacks. We observed that the decrease in post-attack accuracy values relative to the accuracy of the uncompromised networks is smaller for our proposed tessellated approach. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_NlE9YiyXKb",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chinmay_Jain1",
        "name": "Chinmay Jain",
        "name_site": null,
        "openreview_id": "~Chinmay_Jain1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "chinmay-jain-a833aa193/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_NlE9YiyXKb",
      "title": "Tessellated Neural Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": " Data-driven deep learning approaches for image classification are prone to adversarial attacks. An adversarial image which is sufficiently close (visually indistinguishable) from a true image of its representative class can often be misclassified to be a member of a different class. It is possible for attackers to exploit the high dimensionality of image representations, as learned by the neural models, to identify adversarial perturbations. To mitigate this problem, we propose a novel divide-and-conquer based approach of tessellating a base network architecture (e.g., a ResNet used in our experiments). The tessellated network learns the parameterized representations of each non-overlapping sub-region or tiles within an image, independently, and then learns how to combine these representations to finally estimate the class of the input image. We investigate two different modes of tessellation, namely periodic, comprised of regular square-shaped tiles, and aperiodic, comprised of rectangles of different dimensions. Experiments demonstrate that the tessellated extension of two standard deep neural models leads to a better defence against a number of standard adversarial attacks. We observed that the decrease in post-attack accuracy values relative to the accuracy of the uncompromised networks is smaller for our proposed tessellated approach. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_NlE9YiyXKb",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pabitra_Mitra1",
        "name": "Pabitra Mitra",
        "name_site": null,
        "openreview_id": "~Pabitra_Mitra1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~pabitra/",
        "dblp_id": "m/PabitraMitra",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=5bXSZPYAAAAJ",
        "orcid": "0000-0002-1908-9813",
        "linkedin_url": "pabitra-mitra-8028235/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_NlE9YiyXKb",
      "title": "Tessellated Neural Networks: A Robust Defence against Adversarial Attacks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": " Data-driven deep learning approaches for image classification are prone to adversarial attacks. An adversarial image which is sufficiently close (visually indistinguishable) from a true image of its representative class can often be misclassified to be a member of a different class. It is possible for attackers to exploit the high dimensionality of image representations, as learned by the neural models, to identify adversarial perturbations. To mitigate this problem, we propose a novel divide-and-conquer based approach of tessellating a base network architecture (e.g., a ResNet used in our experiments). The tessellated network learns the parameterized representations of each non-overlapping sub-region or tiles within an image, independently, and then learns how to combine these representations to finally estimate the class of the input image. We investigate two different modes of tessellation, namely periodic, comprised of regular square-shaped tiles, and aperiodic, comprised of rectangles of different dimensions. Experiments demonstrate that the tessellated extension of two standard deep neural models leads to a better defence against a number of standard adversarial attacks. We observed that the decrease in post-attack accuracy values relative to the accuracy of the uncompromised networks is smaller for our proposed tessellated approach. ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=_NlE9YiyXKb",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debasis_Ganguly2",
        "name": "Debasis Ganguly",
        "name_site": null,
        "openreview_id": "~Debasis_Ganguly2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://gdebasis.github.io/",
        "dblp_id": "41/7272",
        "google_scholar_url": "FhQENQgAAAAJ",
        "orcid": "0000-0003-0050-7138",
        "linkedin_url": "deb4it/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Glasgow (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ar_09k_Gsos",
      "title": "Learning 3D Point Cloud Embeddings using Optimal Transport",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Learning embeddings of any data largely depends on the ability of the target space to capture semantic relations. The widely used Euclidean space, where embeddings are represented as point vectors, is known to be lacking in its potential to exploit complex structures and relations. Contrary to standard Euclidean embeddings, in this work, we embed point clouds as discrete probability distributions in Wasserstein space. We build a contrastive learning setup to learn Wasserstein embeddings that can be used as a pre-training method with or without supervision for any downstream task. We show that the features captured by Wasserstein embeddings are better in preserving the point cloud geometry, including both global and local information, thus resulting in improved quality embeddings. We perform exhaustive experiments and demonstrate the effectiveness of our method for point cloud classification, transfer learning, segmentation and interpolation tasks over multiple datasets including synthetic and real-world objects in both supervised and self-supervised settings. We also compare against other existing methods and show that our method outperforms them in all downstream tasks. Additionally, our study reveals a promising interpretation of capturing critical points of point clouds that makes our proposed method self-explainable.",
      "tldr": "We introduce a novel method to learn Wasserstein embeddings for 3D point clouds endowed by contrastive learning setup.",
      "site_url": "https://openreview.net/forum?id=ar_09k_Gsos",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddharth_Katageri1",
        "name": "Siddharth Katageri",
        "name_site": null,
        "openreview_id": "~Siddharth_Katageri1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://siddharthkatageri.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ar_09k_Gsos",
      "title": "Learning 3D Point Cloud Embeddings using Optimal Transport",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Learning embeddings of any data largely depends on the ability of the target space to capture semantic relations. The widely used Euclidean space, where embeddings are represented as point vectors, is known to be lacking in its potential to exploit complex structures and relations. Contrary to standard Euclidean embeddings, in this work, we embed point clouds as discrete probability distributions in Wasserstein space. We build a contrastive learning setup to learn Wasserstein embeddings that can be used as a pre-training method with or without supervision for any downstream task. We show that the features captured by Wasserstein embeddings are better in preserving the point cloud geometry, including both global and local information, thus resulting in improved quality embeddings. We perform exhaustive experiments and demonstrate the effectiveness of our method for point cloud classification, transfer learning, segmentation and interpolation tasks over multiple datasets including synthetic and real-world objects in both supervised and self-supervised settings. We also compare against other existing methods and show that our method outperforms them in all downstream tasks. Additionally, our study reveals a promising interpretation of capturing critical points of point clouds that makes our proposed method self-explainable.",
      "tldr": "We introduce a novel method to learn Wasserstein embeddings for 3D point clouds endowed by contrastive learning setup.",
      "site_url": "https://openreview.net/forum?id=ar_09k_Gsos",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srinjay_Sarkar1",
        "name": "Srinjay Sarkar",
        "name_site": null,
        "openreview_id": "~Srinjay_Sarkar1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "244/7955",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "srinjay-sarkar-1501b9112/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ar_09k_Gsos",
      "title": "Learning 3D Point Cloud Embeddings using Optimal Transport",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Learning embeddings of any data largely depends on the ability of the target space to capture semantic relations. The widely used Euclidean space, where embeddings are represented as point vectors, is known to be lacking in its potential to exploit complex structures and relations. Contrary to standard Euclidean embeddings, in this work, we embed point clouds as discrete probability distributions in Wasserstein space. We build a contrastive learning setup to learn Wasserstein embeddings that can be used as a pre-training method with or without supervision for any downstream task. We show that the features captured by Wasserstein embeddings are better in preserving the point cloud geometry, including both global and local information, thus resulting in improved quality embeddings. We perform exhaustive experiments and demonstrate the effectiveness of our method for point cloud classification, transfer learning, segmentation and interpolation tasks over multiple datasets including synthetic and real-world objects in both supervised and self-supervised settings. We also compare against other existing methods and show that our method outperforms them in all downstream tasks. Additionally, our study reveals a promising interpretation of capturing critical points of point clouds that makes our proposed method self-explainable.",
      "tldr": "We introduce a novel method to learn Wasserstein embeddings for 3D point clouds endowed by contrastive learning setup.",
      "site_url": "https://openreview.net/forum?id=ar_09k_Gsos",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Charu_Sharma2",
        "name": "Charu Sharma",
        "name_site": null,
        "openreview_id": "~Charu_Sharma2",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://charusharma.org/",
        "dblp_id": "202/8640",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=bftN0M0AAAAJ",
        "orcid": "0000-0003-2518-5008",
        "linkedin_url": "shcharu/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ciZrud3kf3",
      "title": "Wasserstein Generalization Bound for Few-Shot Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "In the absence of large quantities of annotated data, few shot learning is used to train neural\nnetworks that make predictions based on similarities between datapoints. To better under-\nstand how models would behave when presented with unfamiliar data, research on gen-\neralization bounds have revealed some important properties about deep neural networks.\nHowever, when extended to the domain of few shot learning it often yields loose bounds\nsince it does not take into the account the nature and methodology of few shot learning.\nWe propose a novel stochastic generalization bound for prototypical neural networks by\nconstructing a Wasserstein sphere centered around the distribution of weight matrices. We\nshow that by applying concentration inequalities on the distribution of weight matrices in\nthe Wasserstein sphere stricter generalization bounds can be obtained. Comparison with\nprevious generalization bounds shows the efficacy of our approach and to our knowledge\nthis is the first bound that makes use of Wasserstein distance to give a measure of general-\nizability of deep neural networks",
      "tldr": "We use properties of wassertein distance to give a tight bound for few shot learning, specifically prototypical networks.",
      "site_url": "https://openreview.net/forum?id=ciZrud3kf3",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Phani_raj_Chinnalingu1",
        "name": "Phani raj Chinnalingu",
        "name_site": null,
        "openreview_id": "~Phani_raj_Chinnalingu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~nextgenwrl/Members.html",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dO4aZ9-CsTn",
      "title": "Hierarchical Prototypes for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "By incorporating the environment-specific factor into the dynamics prediction, model-based reinforcement learning (MBRL) is able to generalise to environments with diverse dynamics.In the majority of real-world scenarios, the environment-specific factor is not observable, so existing methods attempt to estimate it from historical transition segments. Nevertheless,earlier research was unable to identify distinct clusters for environment-specific factors learned from different environments, resulting in poor performance.\nTo address this issue,\nWe introduce a set of environmental prototypes to represent the environmental-specified representation for each environment. By encouraging learned environment-specific factors to resemble their assigned environmental prototypes more closely, the discrimination between factors estimated from distinct environments will be enhanced. To learn such prototypes, we first construct prototypes for each sampled trajectory and then hierarchically combine trajectory prototypes with similar semantics into one environmental prototype. Experiments demonstrate that environment-specific factors estimated by our method have superior clustering performance and can consistently improve MBRL's generalisation performance in six environments consistently.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=dO4aZ9-CsTn",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "diOVflNRZnG",
      "title": "Curvature Informed Furthest Point Sampling",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Point cloud representation is becoming increasingly popular due to its low memory footprint, ease of creation, collection and modification. As the size of the point cloud increases, we need to incorporate a down-sampling operation to meet the computational demands of the tasks. Classical approaches such as farthest point sampling perform exceedingly well over downstream tasks. The major drawback is that farthest point sampling is a mere heuristic and does not take geometric priors such as curvature into consideration. We propose a novel sampling procedure that conditions the output of farthest point sampling with curvature information. We create a joint rank by multiplying the soft furthest point rank with corresponding curvature scores obtained via a deep neural network and exchange a percentage of low-ranking points in the furthest set with the high-ranking points in the left-out set. Previous differentiable sampling approaches have failed to conform to the end-to-end learning paradigm due to instability while training. We demonstrate that our algorithm is compatible with end-to-end learning. Our sampling scheme consistently outperforms previous baselines on various downstream geometry processing tasks. Finally, we show detailed ablation studies regarding the qualitative and quantitative analysis of the role of different features used in the proposed algorithm.",
      "tldr": "An extension of furthest point sampling algorithm that takes curvature information into consideration",
      "site_url": "https://openreview.net/forum?id=diOVflNRZnG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubham_Bhardwaj1",
        "name": "Shubham Bhardwaj",
        "name_site": null,
        "openreview_id": "~Shubham_Bhardwaj1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.linkedin.com/in/shubham-bhardwaj/",
        "dblp_id": null,
        "google_scholar_url": "50Ue3d4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Jio Platforms (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 5.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "g4JB0ksCrKe",
      "title": "Contrastive Prompt Tuning Improves Generalization in Vision-Language Models",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Prompt tuning, which focuses on learning continuous text prompts for adapting large vision-language models, has attracted much attention in recent years. While prior works show promising performance over the hand-crafted prompts, they typically use cross-entropy loss for learning prompts, which limits their generalization capability in many real-world scenarios. Motivated by the effectiveness of contrastive learning for improved generalization, we introduce Contrastive Prompt Tuning (CPT), an incredibly simple yet highly efficient framework that explicitly optimizes for the learned prompts to be consistent with the image space. In particular, combined with cross-entropy loss, our contrastive losses help learning prompts so that the model has consistent predictions across different views of an image while also maintaining the consistency of pairwise similarities among different images. Extensive experiments on a battery of datasets demonstrate that our proposed method significantly outperforms the existing methods in improving model's generalization, while also achieving consistent improvements in few-shot in-domain performance for a wide variety of vision-language models.",
      "tldr": "We introduce contrastive prompt tuning for improved generalization in vision-language models by optimizing for the learned prompts to be consistent with the image space.",
      "site_url": "https://openreview.net/forum?id=g4JB0ksCrKe",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_Das4",
        "name": "Abir Das",
        "name_site": null,
        "openreview_id": "~Abir_Das4",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~adas/",
        "dblp_id": "141/1311",
        "google_scholar_url": "L4yEk2UAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "g4JB0ksCrKe",
      "title": "Contrastive Prompt Tuning Improves Generalization in Vision-Language Models",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Prompt tuning, which focuses on learning continuous text prompts for adapting large vision-language models, has attracted much attention in recent years. While prior works show promising performance over the hand-crafted prompts, they typically use cross-entropy loss for learning prompts, which limits their generalization capability in many real-world scenarios. Motivated by the effectiveness of contrastive learning for improved generalization, we introduce Contrastive Prompt Tuning (CPT), an incredibly simple yet highly efficient framework that explicitly optimizes for the learned prompts to be consistent with the image space. In particular, combined with cross-entropy loss, our contrastive losses help learning prompts so that the model has consistent predictions across different views of an image while also maintaining the consistency of pairwise similarities among different images. Extensive experiments on a battery of datasets demonstrate that our proposed method significantly outperforms the existing methods in improving model's generalization, while also achieving consistent improvements in few-shot in-domain performance for a wide variety of vision-language models.",
      "tldr": "We introduce contrastive prompt tuning for improved generalization in vision-language models by optimizing for the learned prompts to be consistent with the image space.",
      "site_url": "https://openreview.net/forum?id=g4JB0ksCrKe",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 5,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gwTP_sA-aj-",
      "title": "Interval Bound Interpolation for Few-shot Learning with Few Tasks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution, with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between the available tasks and their respective interval bounds. We apply our framework to both model-agnostic meta-learning as well as prototype-based metric-learning paradigms. The efficacy of our proposed approach is evident from the improved performance on several datasets from diverse domains in comparison to recent methods.",
      "tldr": "A method to densify the task distribution for few-task few-shot learning using task interpolation within interval-arithmetic-based bounds",
      "site_url": "https://openreview.net/forum?id=gwTP_sA-aj-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shounak_Datta1",
        "name": "Shounak Datta",
        "name_site": null,
        "openreview_id": "~Shounak_Datta1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=qtW4ugoAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gwTP_sA-aj-",
      "title": "Interval Bound Interpolation for Few-shot Learning with Few Tasks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution, with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between the available tasks and their respective interval bounds. We apply our framework to both model-agnostic meta-learning as well as prototype-based metric-learning paradigms. The efficacy of our proposed approach is evident from the improved performance on several datasets from diverse domains in comparison to recent methods.",
      "tldr": "A method to densify the task distribution for few-task few-shot learning using task interpolation within interval-arithmetic-based bounds",
      "site_url": "https://openreview.net/forum?id=gwTP_sA-aj-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anish_Chakrabarty1",
        "name": "Anish Chakrabarty",
        "name_site": null,
        "openreview_id": "~Anish_Chakrabarty1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "304/5289",
        "google_scholar_url": "KfCQY5oAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gwTP_sA-aj-",
      "title": "Interval Bound Interpolation for Few-shot Learning with Few Tasks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution, with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between the available tasks and their respective interval bounds. We apply our framework to both model-agnostic meta-learning as well as prototype-based metric-learning paradigms. The efficacy of our proposed approach is evident from the improved performance on several datasets from diverse domains in comparison to recent methods.",
      "tldr": "A method to densify the task distribution for few-task few-shot learning using task interpolation within interval-arithmetic-based bounds",
      "site_url": "https://openreview.net/forum?id=gwTP_sA-aj-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swagatam_Das2_1",
        "name": "Swagatam Das",
        "name_site": null,
        "openreview_id": "~Swagatam_Das1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.isical.ac.in/~swagatam.das/",
        "dblp_id": "00/3298.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=L8XYpAwAAAAJ",
        "orcid": "0000-0001-6843-4508",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hRfJzvTYvD-",
      "title": "Towards Estimating Transferability using Hard Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "As transfer learning techniques are increasingly used to transfer knowledge from the source model to the target task, it becomes important to quantify which source models are suitable for a given target task without performing computationally expensive fine-tuning. In this work, we propose HASTE (HArd Subset TransfErability), a new strategy to estimate the transferability of a source model to a particular target task using only a harder subset of target data. By leveraging the model’s internal and output representations, we introduce two techniques – one class-agnostic and another class-specific – to identify harder subsets and show that HASTE can be used with any existing transferability metric to improve their reliability. We further analyze the relation between HASTE and the optimal average log-likelihood as well as negative conditional entropy and empirically validate our theoretical bounds. Our experimental results across multiple source model architectures, target datasets, and transfer learning tasks show that HASTE-modified metrics are consistently better or on par with the state-of-the-art transferability metrics.",
      "tldr": "We propose HASTE, a strategy that ensures better transferability estimation using just a hard subset of target data.",
      "site_url": "https://openreview.net/forum?id=hRfJzvTYvD-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tarun_Ram_Menta1",
        "name": "Tarun Ram Menta",
        "name_site": null,
        "openreview_id": "~Tarun_Ram_Menta1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "290/7394",
        "google_scholar_url": "8HC8W9EAAAAJ",
        "orcid": null,
        "linkedin_url": "tarun-ram-menta-50b4121b9",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hRfJzvTYvD-",
      "title": "Towards Estimating Transferability using Hard Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "As transfer learning techniques are increasingly used to transfer knowledge from the source model to the target task, it becomes important to quantify which source models are suitable for a given target task without performing computationally expensive fine-tuning. In this work, we propose HASTE (HArd Subset TransfErability), a new strategy to estimate the transferability of a source model to a particular target task using only a harder subset of target data. By leveraging the model’s internal and output representations, we introduce two techniques – one class-agnostic and another class-specific – to identify harder subsets and show that HASTE can be used with any existing transferability metric to improve their reliability. We further analyze the relation between HASTE and the optimal average log-likelihood as well as negative conditional entropy and empirically validate our theoretical bounds. Our experimental results across multiple source model architectures, target datasets, and transfer learning tasks show that HASTE-modified metrics are consistently better or on par with the state-of-the-art transferability metrics.",
      "tldr": "We propose HASTE, a strategy that ensures better transferability estimation using just a hard subset of target data.",
      "site_url": "https://openreview.net/forum?id=hRfJzvTYvD-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Akash_Patil1",
        "name": "Akash Patil",
        "name_site": null,
        "openreview_id": "~Akash_Patil1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hRfJzvTYvD-",
      "title": "Towards Estimating Transferability using Hard Subsets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "As transfer learning techniques are increasingly used to transfer knowledge from the source model to the target task, it becomes important to quantify which source models are suitable for a given target task without performing computationally expensive fine-tuning. In this work, we propose HASTE (HArd Subset TransfErability), a new strategy to estimate the transferability of a source model to a particular target task using only a harder subset of target data. By leveraging the model’s internal and output representations, we introduce two techniques – one class-agnostic and another class-specific – to identify harder subsets and show that HASTE can be used with any existing transferability metric to improve their reliability. We further analyze the relation between HASTE and the optimal average log-likelihood as well as negative conditional entropy and empirically validate our theoretical bounds. Our experimental results across multiple source model architectures, target datasets, and transfer learning tasks show that HASTE-modified metrics are consistently better or on par with the state-of-the-art transferability metrics.",
      "tldr": "We propose HASTE, a strategy that ensures better transferability estimation using just a hard subset of target data.",
      "site_url": "https://openreview.net/forum?id=hRfJzvTYvD-",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vimal_K_B1",
        "name": "Vimal K B",
        "name_site": null,
        "openreview_id": "~Vimal_K_B1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "vimalkb07/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iQYFdEL6KeS",
      "title": "Enhancement and Numerical Assessment of Novel SARS-CoV-2 Virus Transmission Model",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent pandemic of the coronavirus started in December 2019, which has affected almost all groups of humankind. In this regard, accurate epidemic models are not only crucial for demonstrating the mitigation of the current pandemic but also helpful for forecasting their future dynamics. In this work, we propose a model for SARS-CoV-2 virus transmission to forecast the temporal dynamics of the novel coronavirus disease by considering the characteristics of the disease and the recent literature. Due to the nondeterministic and stochastic nature of the novel-coronavirus disease, we present the model with the aid of stochastic differential equations by considering two infectious phases: pre-symptomatic and symptomatic, because both are significant in the spread of SARS-CoV-2 virus transmission. We ensure that the model is well-posed and identify the necessary conditions for disease eradication by proving the existence, uniqueness, and extinction analysis. The efficacy of the model and the importance of the current study are demonstrated using the actual data. Finally, the model will be simulated using Euler-Maruyama and Milstein’s numerical schemes to support the theoretical findings and show the significance of the results obtained.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=iQYFdEL6KeS",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 1.299038105676658,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iaCzfh6vtwQ",
      "title": "FUN: Filter-based Unlearnable Datasets",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large-scale training of modern deep learning models heavily relies on publicly available data on the web. This potentially unauthorized usage of online data leads to concerns regarding data privacy. Recent works aim to make unlearnable data for deep learning models by adding small, specially designed noises to tackle this issue. However, these methods are vulnerable to adversarial training (AT) and/or are computationally heavy. In this work, we propose a novel, model-free convolutional Filter-based UNlearnable (FUN) dataset generation technique. FUN performs controlled class-wise convolutions using filters that are randomly generated via a private key. FUN encourages the network to learn the relation between filters and labels rather than informative features for classifying the clean data. We develop some theoretical analysis demonstrating that FUN can successfully poison Gaussian mixture data by reducing the clean data performance of the optimal Bayes classifier. We also empirically demonstrate the effectiveness of FUN with various datasets (CIFAR-10, CIFAR-100, and ImageNet-100), and architectures (ResNet-18, VGG-16, Wide ResNet-34-10, and DenseNet-121). Our experiments show that FUN is robust to various data augmentations and training approaches such as smoothing, AT with different budgets, transfer learning, and fine-tuning. For instance, training a ResNet-18 on FUN ImageNet-100 data achieves only 8.96$\\%$, 40.08$\\%$, and 20.58$\\%$ clean test accuracies with empirical risk minimization (ERM), $L_{\\infty}$ AT, and $L_{2}$ AT, respectively. Here, ERM on the clean training data achieves a clean test accuracy of 80.66$\\%$. Furthermore, we also show that FUN is robust to adaptive defenses designed specifically to break it.",
      "tldr": "We propose a novel, model-free convolutional filter-based unlearnable dataset (FUN) generation technique that protects data from empirical risk minimization and adversarial training with various budgets.",
      "site_url": "https://openreview.net/forum?id=iaCzfh6vtwQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinu_Sankar_Sadasivan1",
        "name": "Vinu Sankar Sadasivan",
        "name_site": null,
        "openreview_id": "~Vinu_Sankar_Sadasivan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vinusankars.github.io/",
        "dblp_id": "244/8052",
        "google_scholar_url": "y1IKIw0AAAAJ",
        "orcid": null,
        "linkedin_url": "vinusankars/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jAD0chIdt_",
      "title": "Impulse Control Arbitration for A Dual System of Exploitation and Exploration",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Efficient reinforcement learning (RL) involves a trade-off between \"exploitative\" actions that maximise expected reward and ``explorative\" ones that lead to the visitation of \"novel\" states. To encourage exploration, existing methods proposed methods such as injecting stochasticity into action selection, implicit regularisation, and additive synthetic reward. However, these techniques do not necessarily offer entirely systematic approaches making this trade-off. Here we introduce SElective Reinforcement EXploration (SEREX), a plug-and-play framework that casts the exploration-exploitation trade-off as a game between an RL agent--- exploiter, which purely exploits task-dependent rewards, and another RL agent--- switcher, which chooses at which states to activate a pure exploration policy that is trained to minimise system uncertainty and override Exploiter. Using a form of policies known as impulse control, switcher is able to determine the best set of states to switch to the exploration policy while Exploiter is free to execute its actions everywhere else. We prove that SEREX converges quickly and induces a natural schedule towards pure exploitation. Through extensive empirical studies in both discrete and continuous control benchmarks, we show that with minimal modification, SEREX can be readily combined with existing RL algorithms and yields significant improvement in performance.",
      "tldr": "We propose a plug-and-play framework with a learned impulse control switching mechanism for targeted arbitration between exploration and exploitation behaviour.",
      "site_url": "https://openreview.net/forum?id=jAD0chIdt_",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Byju's (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jK02XX9ZpJkt",
      "title": "CAMA: A New Framework for Safe Multi-Agent Reinforcement Learning Using Constraint Augmentation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the widespread application of multi-agent reinforcement learning (MARL) in real-life settings, the ability to meet safety constraints has become an urgent problem to solve. For example, it is necessary to avoid collisions to reach a common goal in controlling multiple drones. We address this problem by introducing the Constraint Augmented Multi-Agent framework --- CAMA. CAMA can serve as a plug-and-play module to the popular MARL algorithms, including centralized training, decentralized execution and independent learning frameworks. In our approach, we represent the safety constraint as the sum of discounted safety costs bounded by the predefined value, which we call the safety budget. Experiments demonstrate that CAMA can converge quickly to a high degree of constraint satisfaction and surpasses other state-of-the-art safety counterpart algorithms in both cooperative and competitive settings. ",
      "tldr": "CAMA can combine any SOTA non-safe MARL algorithms to ensure they satisfied added constraints without strong assumptions and complex implementations.",
      "site_url": "https://openreview.net/forum?id=jK02XX9ZpJkt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Byju's (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jK02XX9ZpJkt",
      "title": "CAMA: A New Framework for Safe Multi-Agent Reinforcement Learning Using Constraint Augmentation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the widespread application of multi-agent reinforcement learning (MARL) in real-life settings, the ability to meet safety constraints has become an urgent problem to solve. For example, it is necessary to avoid collisions to reach a common goal in controlling multiple drones. We address this problem by introducing the Constraint Augmented Multi-Agent framework --- CAMA. CAMA can serve as a plug-and-play module to the popular MARL algorithms, including centralized training, decentralized execution and independent learning frameworks. In our approach, we represent the safety constraint as the sum of discounted safety costs bounded by the predefined value, which we call the safety budget. Experiments demonstrate that CAMA can converge quickly to a high degree of constraint satisfaction and surpasses other state-of-the-art safety counterpart algorithms in both cooperative and competitive settings. ",
      "tldr": "CAMA can combine any SOTA non-safe MARL algorithms to ensure they satisfied added constraints without strong assumptions and complex implementations.",
      "site_url": "https://openreview.net/forum?id=jK02XX9ZpJkt",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aivar_Sootla1",
        "name": "Aivar Sootla",
        "name_site": null,
        "openreview_id": "~Aivar_Sootla1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "66/9184",
        "google_scholar_url": "https://scholar.google.co.uk/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Huawei (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "k5e6oQP2zHx",
      "title": "QUANTILE-LSTM: A ROBUST LSTM FOR ANOMALY DETECTION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Anomalies refer to departure of systems and devices from their normal behaviour in standard operating conditions. An anomaly in an industrial device can indicate an upcoming failure, often in the temporal direction. In this paper, we make two contributions: 1) we estimate conditional quantiles, and consider three different ways to define anomalies based on the estimated quantiles and 2) use a new\nlearnable activation function in the popular Long Short Term Memory (LSTM) architecture to model temporal long-range dependency. In particular, we propose Parametrized Elliot Function (Parametric Elliot Function (PEF)) as an activation function inside LSTM, which saturates lately compared to sigmoid and tanh. The proposed algorithms are compared with other well known anomaly detection algorithms, such as Isolation Forest (iForest), Elliptic Envelope, Autoencoder,and modern Deep Learning models such as Deep Autoencoding Gaussian Mixture Model (DAGMM), Generative Adversarial Networks (GAN) etc. The algorithms are evaluated in terms of various performance metrics, such as precision and recall. The algorithms are experimented on multiple industrial timeseries datasets such as Yahoo, AWS, GE, and machine sensor. We have found the LSTM based quantile algorithms are very effective and outperformed the existing algorithms in identifying the anomalies.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=k5e6oQP2zHx",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Snehanshu_Saha1",
        "name": "Snehanshu Saha",
        "name_site": null,
        "openreview_id": "~Snehanshu_Saha1",
        "position": 1,
        "gender": "Not Specified",
        "homepage_url": "https://www.bits-pilani.ac.in/goa/snehanshus/profile",
        "dblp_id": "130/3938",
        "google_scholar_url": "C-Qm2LcAAAAJ",
        "orcid": "0000-0002-8458-604X",
        "linkedin_url": "snehanshusaha/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mcJvCys7DX7",
      "title": "Counterfactual Generation Under Confounding",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "A machine learning model, under the influence of observed or unobserved confounders in the training data, can learn spurious correlations and fail to generalize when deployed. For image classifiers, augmenting a training dataset using counterfactual examples has been empirically shown to break spurious correlations.  However, the counterfactual generation task itself becomes more difficult as the level of confounding increases. Existing methods for counterfactual generation under confounding consider a fixed set of interventions (e.g., texture, rotation) and are not flexible enough to capture diverse data-generating processes. Given a causal generative process, we formally characterize the adverse effects of confounding on any downstream tasks and show that the correlation between generative factors (attributes) can be used to quantitatively measure confounding between generative factors. To minimize such correlation, we propose a counterfactual generation method that learns to modify the value of any attribute in an image and generate new images given a set of observed attributes, even when the dataset is highly confounded. These counterfactual images are then used to regularize the downstream classifier such that the learned representations are the same across various generative factors conditioned on the class label. Our method is computationally efficient, simple to implement, and works well for any number of generative factors and confounding variables. Our experimental results on both synthetic (MNIST variants) and real-world (CelebA) datasets show the usefulness of our approach.",
      "tldr": "We propose a counterfactual data augmentation to improve the performance of a classifier when the data is spuriously correlated",
      "site_url": "https://openreview.net/forum?id=mcJvCys7DX7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abbavaram_Gowtham_Reddy1",
        "name": "Abbavaram Gowtham Reddy",
        "name_site": null,
        "openreview_id": "~Abbavaram_Gowtham_Reddy1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://gautam0707.github.io",
        "dblp_id": "294/8798",
        "google_scholar_url": "Iewg-GAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mcJvCys7DX7",
      "title": "Counterfactual Generation Under Confounding",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "A machine learning model, under the influence of observed or unobserved confounders in the training data, can learn spurious correlations and fail to generalize when deployed. For image classifiers, augmenting a training dataset using counterfactual examples has been empirically shown to break spurious correlations.  However, the counterfactual generation task itself becomes more difficult as the level of confounding increases. Existing methods for counterfactual generation under confounding consider a fixed set of interventions (e.g., texture, rotation) and are not flexible enough to capture diverse data-generating processes. Given a causal generative process, we formally characterize the adverse effects of confounding on any downstream tasks and show that the correlation between generative factors (attributes) can be used to quantitatively measure confounding between generative factors. To minimize such correlation, we propose a counterfactual generation method that learns to modify the value of any attribute in an image and generate new images given a set of observed attributes, even when the dataset is highly confounded. These counterfactual images are then used to regularize the downstream classifier such that the learned representations are the same across various generative factors conditioned on the class label. Our method is computationally efficient, simple to implement, and works well for any number of generative factors and confounding variables. Our experimental results on both synthetic (MNIST variants) and real-world (CelebA) datasets show the usefulness of our approach.",
      "tldr": "We propose a counterfactual data augmentation to improve the performance of a classifier when the data is spuriously correlated",
      "site_url": "https://openreview.net/forum?id=mcJvCys7DX7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ngCT1EelZk",
      "title": "Aging with GRACE: Lifelong Model Editing with Key-Value Adaptors",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Large language models often err during deployment, either due to non-representative training data or distribution shift in the test set. Recently, model editors have been proposed to fix errors by adjusting a pre-trained model's weights. So far, however, existing model editors fail when making sequential edits by quickly decaying a model's performance on its upstream data. Further, when editing deployed online models, they quickly forget how to fix previously-seen mistakes. We advance beyond these existing methods by proposing and studying a novel Lifelong Model Editing setting, where errors stream into a deployed model and we update the model to correct its predictions without influencing its predictions for unrelated inputs. Towards effective methods in this challenging setting, we propose with General Retrieval Adaptors for Continual Editing, or GRACE. GRACE is a new Key-Value framework that casts model editing as a codebook update problem. The proposed approach edits selected model layers by caching activations that are queried using embeddings from the previous layer. The cached activations are trained to correct a model's predictions, treating future layers as a decoder. As edits stream in, the keys and values of a GRACE layer are updated while the model weights remain frozen, ensuring similar edits are treated similarly without altering the model's performance on unrelated instances. Experimentally, we show that \\method substantially improves over recent model editors.",
      "tldr": "We continually fix large models' mistakes by caching learned activations in a codebook for a selected layer. The cached activations can be re-used to influence the model's behavior for future instances that are similar to previously-fixed errors.",
      "site_url": "https://openreview.net/forum?id=ngCT1EelZk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 4,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pKu077C57fH",
      "title": "Towards a Mathematics Formalisation Assistant using Large Language Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful input-dependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\\% accuracy for $120$ theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of $13$ theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation. ",
      "tldr": "Large language models have the potential to be useful for mathematics formalization",
      "site_url": "https://openreview.net/forum?id=pKu077C57fH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddhartha_Gadgil1",
        "name": "Siddhartha Gadgil",
        "name_site": null,
        "openreview_id": "~Siddhartha_Gadgil1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://math.iisc.ac.in/~gadgil/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pRUW8BuTEFI",
      "title": "MixBin: Towards Budgeted Binarization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Binarization has proven to be amongst the most effective ways of neural network compression, reducing the FLOPs of the original model by a large extent. However, such levels of compression are often accompanied by a significant drop in the performance of the model. There exist some approaches that reduce this performance drop by facilitating partial binarization of the network, however, a systematic approach to mix binary and full-precision parameters in a single network is still missing. In this paper, we propose a paradigm to perform partial binarization of neural networks in a controlled sense, thereby constructing budgeted binary neural network (B2NN). We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components. $\\texttt{MixBin}$ allows to explicitly choose the approximate fraction of the network to be kept as binary, thereby presenting the flexibility to adapt the inference cost at a prescribed budget. We demonstrate through numerical experiments that B2NNs obtained from our $\\texttt{MixBin}$ strategy are significantly better than those obtained from random selection of the network layers. To perform partial binarization in an effective manner, it is important that both the full-precision as well as the binary components of the B2NN are appropriately optimized. We also demonstrate that the choice of the activation function can have a significant effect on this process, and to circumvent this issue, we present BinReLU, an integral component of $\\texttt{MixBin}$, that can be used as an effective activation function for the full-precision as well as the binary components of any B2NN. Experimental investigations reveal that BinReLU outperforms the other activation functions in all possible scenarios of B2NN: zero-, partial- as well as full binarization. Finally, we demonstrate the efficacy of $\\texttt{MixBin}$ on the tasks of classification and object tracking using benchmark datasets.",
      "tldr": "We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components.",
      "site_url": "https://openreview.net/forum?id=pRUW8BuTEFI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Udbhav_Bamba1",
        "name": "Udbhav Bamba",
        "name_site": null,
        "openreview_id": "~Udbhav_Bamba1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://ubamba98.github.com",
        "dblp_id": "261/3097",
        "google_scholar_url": "PgnnH78AAAAJ",
        "orcid": null,
        "linkedin_url": "ubamba98",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Transmute AI Lab (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pRUW8BuTEFI",
      "title": "MixBin: Towards Budgeted Binarization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Binarization has proven to be amongst the most effective ways of neural network compression, reducing the FLOPs of the original model by a large extent. However, such levels of compression are often accompanied by a significant drop in the performance of the model. There exist some approaches that reduce this performance drop by facilitating partial binarization of the network, however, a systematic approach to mix binary and full-precision parameters in a single network is still missing. In this paper, we propose a paradigm to perform partial binarization of neural networks in a controlled sense, thereby constructing budgeted binary neural network (B2NN). We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components. $\\texttt{MixBin}$ allows to explicitly choose the approximate fraction of the network to be kept as binary, thereby presenting the flexibility to adapt the inference cost at a prescribed budget. We demonstrate through numerical experiments that B2NNs obtained from our $\\texttt{MixBin}$ strategy are significantly better than those obtained from random selection of the network layers. To perform partial binarization in an effective manner, it is important that both the full-precision as well as the binary components of the B2NN are appropriately optimized. We also demonstrate that the choice of the activation function can have a significant effect on this process, and to circumvent this issue, we present BinReLU, an integral component of $\\texttt{MixBin}$, that can be used as an effective activation function for the full-precision as well as the binary components of any B2NN. Experimental investigations reveal that BinReLU outperforms the other activation functions in all possible scenarios of B2NN: zero-, partial- as well as full binarization. Finally, we demonstrate the efficacy of $\\texttt{MixBin}$ on the tasks of classification and object tracking using benchmark datasets.",
      "tldr": "We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components.",
      "site_url": "https://openreview.net/forum?id=pRUW8BuTEFI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Neeraj_Anand1",
        "name": "Neeraj Anand",
        "name_site": null,
        "openreview_id": "~Neeraj_Anand1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "bGbMOkMAAAAJ",
        "orcid": null,
        "linkedin_url": "neeraj-anand-092353204/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian School of Mines (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pRUW8BuTEFI",
      "title": "MixBin: Towards Budgeted Binarization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Binarization has proven to be amongst the most effective ways of neural network compression, reducing the FLOPs of the original model by a large extent. However, such levels of compression are often accompanied by a significant drop in the performance of the model. There exist some approaches that reduce this performance drop by facilitating partial binarization of the network, however, a systematic approach to mix binary and full-precision parameters in a single network is still missing. In this paper, we propose a paradigm to perform partial binarization of neural networks in a controlled sense, thereby constructing budgeted binary neural network (B2NN). We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components. $\\texttt{MixBin}$ allows to explicitly choose the approximate fraction of the network to be kept as binary, thereby presenting the flexibility to adapt the inference cost at a prescribed budget. We demonstrate through numerical experiments that B2NNs obtained from our $\\texttt{MixBin}$ strategy are significantly better than those obtained from random selection of the network layers. To perform partial binarization in an effective manner, it is important that both the full-precision as well as the binary components of the B2NN are appropriately optimized. We also demonstrate that the choice of the activation function can have a significant effect on this process, and to circumvent this issue, we present BinReLU, an integral component of $\\texttt{MixBin}$, that can be used as an effective activation function for the full-precision as well as the binary components of any B2NN. Experimental investigations reveal that BinReLU outperforms the other activation functions in all possible scenarios of B2NN: zero-, partial- as well as full binarization. Finally, we demonstrate the efficacy of $\\texttt{MixBin}$ on the tasks of classification and object tracking using benchmark datasets.",
      "tldr": "We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components.",
      "site_url": "https://openreview.net/forum?id=pRUW8BuTEFI",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Arctic University of Norway (Norway)",
        "countries": [
          "Norway"
        ],
        "country_codes": [
          "NO"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sn8w7P9TrYf",
      "title": "Linear Convergence of Decentralized FedAvg for Non-Convex Objectives: The Interpolation Regime",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In the age of Bigdata, Federated Learning (FL) provides machine learning (ML) practitioners with an indispensable tool for solving large-scale learning problems. FL is a distributed optimization paradigm where multiple nodes each having access to a local dataset collaborate (with or without a server) to solve a joint problem. Federated Averaging (FedAvg) although the algorithm of choice for many FL applications is not very well understood especially in the interpolation regime, a common phenomenon observed in modern overparameterized neural networks. In this work, we address this challenge and perform a thorough theoretical performance analysis of FedAvg in the interpolation regime for training of overparameterized neural networks. Specifically, we analyze the performance of FedAvg in two settings: (i) {\\em[Server]}: When the network has access to a server that coordinates the information sharing among nodes, and (ii) {\\em[Decentralized]:} The serverless setting, where the local nodes communicate over an undirected graph. We consider a class of non-convex functions satisfying the Polyak-Lojasiewicz (PL) condition, a condition that is satisfied by overparameterized neural networks. For the first time, we establish that FedAvg under both {\\em Server} and {\\em Decentralized} settings achieve linear convergence rates of $\\mathcal{O}(T^{3/2} \\log  (1/{\\epsilon} ) )$ and $\\mathcal{O}({T^2} \\log ({1}/{\\epsilon}))$, respectively, where $\\epsilon$ is the desired solution accuracy, and $T$ is the number of local updates at each node. In contrast to the standard FedAvg analysis, our work does not require bounded heterogeneity, variance, and gradient assumptions. Instead, we show that sample-wise (and local) smoothness of the local loss functions suffice to capture the effect of heterogeneity in FL training. We use a novel application of induction to prove the linear convergence in the {\\em Decentralized} setting, which can be of independent interest. Finally, we conduct experiments on multiple real datasets to corroborate our theoretical findings.",
      "tldr": "Our work shows linear convergence for Federated Averaging algorithm in {\\em Server} and {\\em Decentralized} settings.",
      "site_url": "https://openreview.net/forum?id=sn8w7P9TrYf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shruti_P_Maralappanavar1",
        "name": "Shruti P Maralappanavar",
        "name_site": null,
        "openreview_id": "~Shruti_P_Maralappanavar1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Wayne State University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uGEBxC8dnEh",
      "title": "RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations and few principled guidelines that would help practitioners to successfully deploy those methods. The main reason for that pitfall actually comes from JE-SSL's core principle of not employing any input reconstruction. Without any visual clue, it becomes extremely cryptic to judge the quality of a learned representation without having access to a labelled dataset. We hope to correct those limitations by providing a single --theoretically motivated-- criterion that reflects the quality of learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method ---coined {\\em RankMe}--- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels, training or parameters to tune. Through thorough empirical experiments involving hundreds of repeated training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no loss in final performance compared to the current selection method that involve dataset labels. We hope that RankMe will facilitate the use of JE-SSL in domains with little or no labeled data.",
      "tldr": "We show how the rank of representations can be used to measure their dowstream performance, even on unseen datasets. We validate this simple metric with thorough experiments and show its power for hyperparameter selection.",
      "site_url": "https://openreview.net/forum?id=uGEBxC8dnEh",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laurent_Najman1",
        "name": "Laurent Najman",
        "name_site": null,
        "openreview_id": "~Laurent_Najman1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://laurentnajman.org",
        "dblp_id": "68/4192",
        "google_scholar_url": "https://scholar.google.fr/citations?user=j-2_cT0AAAAJ",
        "orcid": "0000-0002-6190-0235",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ecole Supérieure d'Ingénieurs en Electronique et Electrotechnique (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 90,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "w2mDq-p9EEf",
      "title": "Learning Latent Structural Causal Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Causal learning has long concerned itself with the accurate recovery of underlying causal mechanisms. Such causal modelling enables better explanations of out-of-distribution data. Prior works on causal learning assume that the high-level causal variables are given. However, in machine learning tasks, one often operates on low-level data like image pixels or high-dimensional vectors. In such settings, the entire Structural Causal Model (SCM) -- structure, parameters, \\textit{and} high-level causal variables -- is unobserved and needs to be learnt from low-level data. We treat this problem as Bayesian inference of the latent SCM, given low-level data. For linear Gaussian additive noise SCMs, we present a tractable approximate inference method which performs joint inference over the causal variables, structure and parameters of the latent SCM from random, known interventions. Experiments are performed on synthetic datasets and a causally generated image dataset to demonstrate the efficacy of our approach. We also perform image generation from unseen interventions, thereby verifying out of distribution generalization for the proposed causal model.",
      "tldr": "bayesian inference over latent structural causal models from low level data and random, known interventions for linear Gaussian additive noise SCMs. Such a model also performs image generation from unseen interventions,",
      "site_url": "https://openreview.net/forum?id=w2mDq-p9EEf",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jithendaraa_Subramanian1",
        "name": "Jithendaraa Subramanian",
        "name_site": null,
        "openreview_id": "~Jithendaraa_Subramanian1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://jithendaraa.github.io/",
        "dblp_id": "281/6755",
        "google_scholar_url": "s0BzYvYAAAAJ",
        "orcid": null,
        "linkedin_url": "jithendaraa-subramanian-85a22b176/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "McGill University (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.8,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wNnaozRwl5O",
      "title": "Speech denoising by listening to noise",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Speech denoising is the task of obtaining clean speech from the speech signal corrupted by background noise. Except in high end recording studios, we do not get clean speech signal as some background noise, or noise due to the recording device is always present. We propose an approach to denoise noisy speech signal by modeling the noise explicitly. Existing approaches model speech, potentially of multiple speakers, for denoising. Such approaches have an inherent drawback as a separate model is required for each speaker. We show that instead of modeling speaker(s), modelling the noise helps obtain a unified speaker independent denoiser, cf.\\ speaker dependent ones in existing popular approaches. In addition to a novel speech denoising network, we also propose a large scale noise dataset, \\texttt{AudioNoiseSet}, derived from Audioset dataset, to train our model. We show that our model outperforms prior approaches by significant margin in a large scale, in the wild speech datasets, \\ie AVspeech, with standard quantitative metrics. In addition we show with multiple human ratings that the method is preferred over state-of-the-art approaches. The user study also points towards limitations of the metrics used, which we discuss. We also provide many qualitative results to demonstrate our better results.\n",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=wNnaozRwl5O",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kranti_Kumar_Parida1",
        "name": "Kranti Kumar Parida",
        "name_site": null,
        "openreview_id": "~Kranti_Kumar_Parida1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://krantiparida.github.io/",
        "dblp_id": "251/3070",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=TbkQaFgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.4,
        "confidence_std": 0.48989794855663565,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wSysC6I_S0z",
      "title": "A Hybrid Framework for Generating A Country-scale Synthetic Population",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Population censuses are vital to public policy decision-making. They provide insights into human resources, demography, culture, and economic structure at local, regional, and national levels. However, such surveys are very expensive (especially for low and middle income countries with high populations, such as India), and may also raise privacy concerns, depending upon the kinds of data collected.\nWe introduce a novel hybrid framework which can combine data from multiple real-world surveys (with different, partially overlapping sets of attributes) to produce a real-scale synthetic population of humans. Critically, our population maintains family structures comprising individuals with demographic, socioeconomic, health, and geolocation attributes: this means that our \"fake\" people live in realistic locations, have realistic families, etc. Such data can be used for a variety of purposes: we explore one such use case, agent-based modelling of infectious disease in India.\nWe use both machine learning and statistical metrics to gauge the quality of our synthetic population. Our experimental results show that synthetic data can realistically simulate the population for various administrative units of India, producing real-scale, detailed data at the desired level of zoom -- from cities, to districts, to states, eventually combining to form a country-scale synthetic population.",
      "tldr": "This paper provides a hybrid framework to generate country-scale synthetic population and also provides metrics to assess the quality of our population.",
      "site_url": "https://openreview.net/forum?id=wSysC6I_S0z",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Bhavesh_Neekhra1",
        "name": "Bhavesh Neekhra",
        "name_site": null,
        "openreview_id": "~Bhavesh_Neekhra1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://bhaveshneekhra.github.io/",
        "dblp_id": "329/6208",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-5468-0812",
        "linkedin_url": "bhaveshneekhra/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ashoka University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wSysC6I_S0z",
      "title": "A Hybrid Framework for Generating A Country-scale Synthetic Population",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Population censuses are vital to public policy decision-making. They provide insights into human resources, demography, culture, and economic structure at local, regional, and national levels. However, such surveys are very expensive (especially for low and middle income countries with high populations, such as India), and may also raise privacy concerns, depending upon the kinds of data collected.\nWe introduce a novel hybrid framework which can combine data from multiple real-world surveys (with different, partially overlapping sets of attributes) to produce a real-scale synthetic population of humans. Critically, our population maintains family structures comprising individuals with demographic, socioeconomic, health, and geolocation attributes: this means that our \"fake\" people live in realistic locations, have realistic families, etc. Such data can be used for a variety of purposes: we explore one such use case, agent-based modelling of infectious disease in India.\nWe use both machine learning and statistical metrics to gauge the quality of our synthetic population. Our experimental results show that synthetic data can realistically simulate the population for various administrative units of India, producing real-scale, detailed data at the desired level of zoom -- from cities, to districts, to states, eventually combining to form a country-scale synthetic population.",
      "tldr": "This paper provides a hybrid framework to generate country-scale synthetic population and also provides metrics to assess the quality of our population.",
      "site_url": "https://openreview.net/forum?id=wSysC6I_S0z",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kshitij_Kapoor1",
        "name": "Kshitij Kapoor",
        "name_site": null,
        "openreview_id": "~Kshitij_Kapoor1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "kapoorkshitij/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Ashoka University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wSysC6I_S0z",
      "title": "A Hybrid Framework for Generating A Country-scale Synthetic Population",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Population censuses are vital to public policy decision-making. They provide insights into human resources, demography, culture, and economic structure at local, regional, and national levels. However, such surveys are very expensive (especially for low and middle income countries with high populations, such as India), and may also raise privacy concerns, depending upon the kinds of data collected.\nWe introduce a novel hybrid framework which can combine data from multiple real-world surveys (with different, partially overlapping sets of attributes) to produce a real-scale synthetic population of humans. Critically, our population maintains family structures comprising individuals with demographic, socioeconomic, health, and geolocation attributes: this means that our \"fake\" people live in realistic locations, have realistic families, etc. Such data can be used for a variety of purposes: we explore one such use case, agent-based modelling of infectious disease in India.\nWe use both machine learning and statistical metrics to gauge the quality of our synthetic population. Our experimental results show that synthetic data can realistically simulate the population for various administrative units of India, producing real-scale, detailed data at the desired level of zoom -- from cities, to districts, to states, eventually combining to form a country-scale synthetic population.",
      "tldr": "This paper provides a hybrid framework to generate country-scale synthetic population and also provides metrics to assess the quality of our population.",
      "site_url": "https://openreview.net/forum?id=wSysC6I_S0z",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debayan_Gupta1",
        "name": "Debayan Gupta",
        "name_site": null,
        "openreview_id": "~Debayan_Gupta1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.ashoka.edu.in/profile/debayan-gupta/",
        "dblp_id": "121/4322.html",
        "google_scholar_url": "Z16kmr8AAAAJ",
        "orcid": "0000-0002-4457-1556",
        "linkedin_url": "debayang",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wYZeJ3BsXl6",
      "title": "PMI-guided Masking Strategy to Enable Few-shot Learning for Genomic Applications",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Learning effective gene representations is of great research interest. Lately, large-scale language models based on the \"transformer\" architecture, such as DNABert and LOGO, have been proposed to learn gene representations from the Human Reference Genome. Although these large language models outperform previous approaches, currently, no study empirically determined the best strategy for representing gene sequences as tokens. Therefore, the uniform random masking strategy, which is the default during the pretraining of such masked language models, might lead to pretraining inefficiency, resulting in suboptimal downstream task performance in the few-shot setting. However, good few-shot performance is critical, with dataset sizes in (personalized) medicine often not exceeding a couple of hundred data points.  \n\nIn this paper, we develop a novel strategy to adapt \"Pointwise Mutual Information (PMI) masking\" used previously in the NLP setting to the domain of gene sequence modeling. PMI-masking masks spans of tokens that are more likely to co-occur, forming a statistically relevant span. First, we learn a vocabulary of tokens with a high PMI score from our pretraining corpus (the \"Human Reference Genome\"). Next, we utilize this side information in the following step and train our model by masking tokens based on PMI scores. In extensive experiments, we evaluate the effectiveness of the PMI-masking strategy on two baseline models of DNABert and LOGO, over three benchmark datasets (two on promoters and one on enhancer), and on a variety of few-shot settings. We observe that our PMI-masking-guided baseline models substantially outperform the baseline models. We further observe that almost all the top-ranked DNA tokens in terms of PMI score are closely associated with existing \"conserved DNA sequence motifs\".",
      "tldr": "PMI-masking in MLMs helps to achieve better few-shot classification performance in gene sequence modeling applications.",
      "site_url": "https://openreview.net/forum?id=wYZeJ3BsXl6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumyadeep_Roy1",
        "name": "Soumyadeep Roy",
        "name_site": null,
        "openreview_id": "~Soumyadeep_Roy1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "244/0004",
        "google_scholar_url": "t3EF4i0AAAAJ",
        "orcid": "0000-0001-7269-2163",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wYZeJ3BsXl6",
      "title": "PMI-guided Masking Strategy to Enable Few-shot Learning for Genomic Applications",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Learning effective gene representations is of great research interest. Lately, large-scale language models based on the \"transformer\" architecture, such as DNABert and LOGO, have been proposed to learn gene representations from the Human Reference Genome. Although these large language models outperform previous approaches, currently, no study empirically determined the best strategy for representing gene sequences as tokens. Therefore, the uniform random masking strategy, which is the default during the pretraining of such masked language models, might lead to pretraining inefficiency, resulting in suboptimal downstream task performance in the few-shot setting. However, good few-shot performance is critical, with dataset sizes in (personalized) medicine often not exceeding a couple of hundred data points.  \n\nIn this paper, we develop a novel strategy to adapt \"Pointwise Mutual Information (PMI) masking\" used previously in the NLP setting to the domain of gene sequence modeling. PMI-masking masks spans of tokens that are more likely to co-occur, forming a statistically relevant span. First, we learn a vocabulary of tokens with a high PMI score from our pretraining corpus (the \"Human Reference Genome\"). Next, we utilize this side information in the following step and train our model by masking tokens based on PMI scores. In extensive experiments, we evaluate the effectiveness of the PMI-masking strategy on two baseline models of DNABert and LOGO, over three benchmark datasets (two on promoters and one on enhancer), and on a variety of few-shot settings. We observe that our PMI-masking-guided baseline models substantially outperform the baseline models. We further observe that almost all the top-ranked DNA tokens in terms of PMI score are closely associated with existing \"conserved DNA sequence motifs\".",
      "tldr": "PMI-masking in MLMs helps to achieve better few-shot classification performance in gene sequence modeling applications.",
      "site_url": "https://openreview.net/forum?id=wYZeJ3BsXl6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Niloy_Ganguly1",
        "name": "Niloy Ganguly",
        "name_site": null,
        "openreview_id": "~Niloy_Ganguly1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.facweb.iitkgp.ac.in/~niloy/",
        "dblp_id": "https://dblp.org/pers/hd/g/Ganguly:Niloy",
        "google_scholar_url": "hCbFmUUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xdVNc95sY1l",
      "title": "SYNC: Efficient Neural Code Search Through Structurally Guided Hard Negative Curricula",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Efficient code snippet search using natural language queries can be a great productivity tool for developers (beginners and professionals alike). Recently neural code search has been popular, where a neural method is used to embed both the query (NL) and the code snippet (PL) into a common representation space; which is further used to obtain the most relevant PL satisfying the intent in the query. Transformers-based pre-trained language models (such as CodeBERT, GraphCodeBERT, UniXCoder) have been especially effective to learn such representation. These models often make mistakes such as retrieving snippets with incorrect data types, and incorrect method names or signatures; even when exposed to the underlying structural information of the code (such as Abstract Syntax Tree and other static analysis outputs) during pre-training. The generalization ability beyond the training data is also limited (as the code retrieval datasets vary in the ways NL-PL pairs are collected). In this work, we propose a structure-aware hard negative sampling method and a mastering-rate based curriculum learning technique (SYNC) that enhances the pre-trained representation using both soft (random) and the (synthesized) hard negative samples. Our experiments on three state-of-the-art pre-trained language models for programming languages, over four Python code retrieval datasets, show the efficacy of the approach (under both in-distribution and out-of-distribution settings).",
      "tldr": "This paper proposes an AST-guided hard negative sampling method for training efficient neural code search models using curriculum learning.",
      "site_url": "https://openreview.net/forum?id=xdVNc95sY1l",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumitra_Das1",
        "name": "Soumitra Das",
        "name_site": null,
        "openreview_id": "~Soumitra_Das1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "soumitra-das-043bb8184/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xdVNc95sY1l",
      "title": "SYNC: Efficient Neural Code Search Through Structurally Guided Hard Negative Curricula",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Efficient code snippet search using natural language queries can be a great productivity tool for developers (beginners and professionals alike). Recently neural code search has been popular, where a neural method is used to embed both the query (NL) and the code snippet (PL) into a common representation space; which is further used to obtain the most relevant PL satisfying the intent in the query. Transformers-based pre-trained language models (such as CodeBERT, GraphCodeBERT, UniXCoder) have been especially effective to learn such representation. These models often make mistakes such as retrieving snippets with incorrect data types, and incorrect method names or signatures; even when exposed to the underlying structural information of the code (such as Abstract Syntax Tree and other static analysis outputs) during pre-training. The generalization ability beyond the training data is also limited (as the code retrieval datasets vary in the ways NL-PL pairs are collected). In this work, we propose a structure-aware hard negative sampling method and a mastering-rate based curriculum learning technique (SYNC) that enhances the pre-trained representation using both soft (random) and the (synthesized) hard negative samples. Our experiments on three state-of-the-art pre-trained language models for programming languages, over four Python code retrieval datasets, show the efficacy of the approach (under both in-distribution and out-of-distribution settings).",
      "tldr": "This paper proposes an AST-guided hard negative sampling method for training efficient neural code search models using curriculum learning.",
      "site_url": "https://openreview.net/forum?id=xdVNc95sY1l",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jyothi_Vedurada1",
        "name": "Jyothi Vedurada",
        "name_site": null,
        "openreview_id": "~Jyothi_Vedurada1",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://jyothivedurada.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "y_icnxeeUcl",
      "title": "WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Gains in the ability to generalize on image analysis tasks for neural networks have come at the cost of increased number of parameters and layers, dataset sizes, training and test computations, and GPU RAM. We introduce a new architecture -- WaveMix-Lite -- that can generalize on par with contemporary transformers and convolutional neural networks (CNNs) while needing fewer resources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix spatial information from pixels. WaveMix-Lite seems to be a versatile and scalable architectural framework that can be used for multiple vision tasks, such as image classification and semantic segmentation, without requiring significant architectural changes, unlike transformers and CNNs. It is able to meet or exceed several accuracy benchmarks while training on a single GPU. For instance, it achieves state-of-the-art accuracy on five EMNIST datasets, outperforms CNNs and transformers in ImageNet-1K and Places-365, and achieves an mIoU of 77\\% on Cityscapes validation set, while using less than one-fifth the number parameters and half the GPU RAM of comparable CNNs or transformers. Our experiments show that while the convolutional elements of neural architectures exploit the shift-invariance property of images, new types of layers (e.g., wavelet transform) can exploit additional properties of images, such as scale-invariance and finite spatial extents of objects.",
      "tldr": "WaveMix-Lite uses 2D discrete Wavelet transform for resource-efficient token-mixing and performs better than CNNs and transformers in image classification and segmentation tasks while requiring fewer GPU RAM and parameters.",
      "site_url": "https://openreview.net/forum?id=y_icnxeeUcl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranav_Jeevan_P1",
        "name": "Pranav Jeevan P",
        "name_site": null,
        "openreview_id": "~Pranav_Jeevan_P1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://pranavphoenix.github.io/",
        "dblp_id": "296/3727",
        "google_scholar_url": "3GlJQ24AAAAJ",
        "orcid": "0000-0003-4110-9638",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Samsung (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "y_icnxeeUcl",
      "title": "WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Gains in the ability to generalize on image analysis tasks for neural networks have come at the cost of increased number of parameters and layers, dataset sizes, training and test computations, and GPU RAM. We introduce a new architecture -- WaveMix-Lite -- that can generalize on par with contemporary transformers and convolutional neural networks (CNNs) while needing fewer resources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix spatial information from pixels. WaveMix-Lite seems to be a versatile and scalable architectural framework that can be used for multiple vision tasks, such as image classification and semantic segmentation, without requiring significant architectural changes, unlike transformers and CNNs. It is able to meet or exceed several accuracy benchmarks while training on a single GPU. For instance, it achieves state-of-the-art accuracy on five EMNIST datasets, outperforms CNNs and transformers in ImageNet-1K and Places-365, and achieves an mIoU of 77\\% on Cityscapes validation set, while using less than one-fifth the number parameters and half the GPU RAM of comparable CNNs or transformers. Our experiments show that while the convolutional elements of neural architectures exploit the shift-invariance property of images, new types of layers (e.g., wavelet transform) can exploit additional properties of images, such as scale-invariance and finite spatial extents of objects.",
      "tldr": "WaveMix-Lite uses 2D discrete Wavelet transform for resource-efficient token-mixing and performs better than CNNs and transformers in image classification and segmentation tasks while requiring fewer GPU RAM and parameters.",
      "site_url": "https://openreview.net/forum?id=y_icnxeeUcl",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kavitha_Viswanathan1",
        "name": "Kavitha Viswanathan",
        "name_site": null,
        "openreview_id": "~Kavitha_Viswanathan1",
        "position": 2,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "321/3456",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0003-3475-5076",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ydv0gtW4WLU",
      "title": "CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Hierarchical reinforcement learning is a promising approach that uses temporal abstraction to solve complex long horizon problems. However, simultaneously learning a hierarchy of policies is unstable as it is challenging to train higher-level policy when the lower-level primitive is non-stationary. In this paper, we propose to generate a curriculum of achievable subgoals for evolving lower-level primitives using reinforcement learning and imitation learning. The lower level primitive periodically performs data relabeling on a handful of expert demonstrations using our primitive informed parsing method. We derive expressions to bound the sub-optimality of our method and develop a practical algorithm for hierarchical reinforcement learning. Since our approach uses a handful of expert demonstrations, it is suitable for most real world robotic control tasks. Experimental results on complex maze navigation and robotic manipulation environments show that inducing hierarchical curriculum learning significantly improves sample efficiency, and results in better learning of goal conditioned policies in complex temporally extended tasks. ",
      "tldr": "We effectively leverage expert demonstrations using our curriculum learning based approach to deal with non-stationarity in the context of hierarchical reinforcement learning.",
      "site_url": "https://openreview.net/forum?id=ydv0gtW4WLU",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Utsav_Singh1",
        "name": "Utsav Singh",
        "name_site": null,
        "openreview_id": "~Utsav_Singh1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.cse.iitk.ac.in/users/utsavz/",
        "dblp_id": "241/9336",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zMVCSe945x",
      "title": "Taming Policy Constrained Offline Reinforcement Learning for Non-expert Demonstrations",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "A promising paradigm for offline reinforcement learning (RL) is to constrain the learned policy to stay close to the dataset behaviors, known as policy constraint offline RL. However, existing works heavily rely on the purity of the data, exhibiting performance degradation or even catastrophic failure when learning from contaminated datasets containing impure trajectories of diverse levels. e.g., expert level, medium level, etc., while offline contaminated data logs exist commonly in the real world.  To mitigate this, we first introduce gradient penalty over the learned value function to tackle the exploding Q-function gradients. We then relax the closeness constraints towards non-optimal actions with critic weighted constraint relaxation. Experimental results show that the proposed techniques effectively tame the non-optimal trajectories for policy constraint offline RL methods, evaluated on a set of contaminated D4RL Mujoco and Adroit datasets.",
      "tldr": "The performance losses of policy constraint-based offline RL algorithms on contaminated datasets can be alleviated by gradient penalty and constraint relaxation.",
      "site_url": "https://openreview.net/forum?id=zMVCSe945x",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": null,
        "rating_std": null,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    }
  ]
}