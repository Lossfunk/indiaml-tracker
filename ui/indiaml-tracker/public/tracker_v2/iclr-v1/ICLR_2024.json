{
  "conference": "ICLR 2024",
  "focus_country": "India",
  "total_papers": 302,
  "generated_at": "2025-07-06T10:37:20.825485",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "SQrHpTllXa",
      "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18603",
      "pdf_url": "https://openreview.net/pdf?id=SQrHpTllXa",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sohan_Patnaik1",
        "name": "Sohan Patnaik",
        "name_site": "Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Singla, Balaji Krishnamurthy",
        "openreview_id": "~Sohan_Patnaik1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "sohan-patnaik-29061a1a4/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 8.0,
        "rating_std": 0.0,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 18,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashmit_Khandelwal1",
        "name": "Ashmit Khandelwal",
        "name_site": "Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman Singla, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy",
        "openreview_id": "~Ashmit_Khandelwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashmitkx.github.io",
        "dblp_id": "356/2261",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0007-0533-9176",
        "linkedin_url": "ashmitkx/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eUgS9Ig8JG",
      "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18219",
      "pdf_url": "https://openreview.net/pdf?id=eUgS9Ig8JG",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sravanthi_Gurugubelli1",
        "name": "Sravanthi Gurugubelli",
        "name_site": "Sravanthi Gurugubelli, Sundeep Prabhakar Chepuri",
        "openreview_id": "~Sravanthi_Gurugubelli1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "DXyvmJsAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "i9wDX850jR",
      "title": "Feature emergence via margin maximization: case studies in algebraic tasks",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding *how* neural networks implement specific target functions, this paper explores a complementary question -- *why* do networks arrive at particular computational strategies? \nOur inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. \nSpecifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compositions in general groups, aligning closely with the empirical observations of Nanda et al. (2023) and Chughtai et al. (2023). More generally, we hope our techniques can help to foster a deeper understanding of why neural networks adopt specific computational strategies.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18073",
      "pdf_url": "https://openreview.net/pdf?id=i9wDX850jR",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Depen_Morwani1",
        "name": "Depen Morwani",
        "name_site": null,
        "openreview_id": "~Depen_Morwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "277/5200",
        "google_scholar_url": "vOngxFUAAAAJ",
        "orcid": null,
        "linkedin_url": "depen-morwani-070298122/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Harvard University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hnrB5YHoYu",
      "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a 75% young and 25% old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18085",
      "pdf_url": "https://openreview.net/pdf?id=hnrB5YHoYu",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 20.0,
      "reviews": {
        "rating_mean": 7.333333333333333,
        "rating_std": 1.8856180831641267,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 45,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2iGiSHmeAN",
      "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19540",
      "pdf_url": "https://openreview.net/pdf?id=2iGiSHmeAN",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suresh_Bishnoi1",
        "name": "Suresh Bishnoi",
        "name_site": "Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, N. M. Anoop Krishnan",
        "openreview_id": "~Suresh_Bishnoi1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~srz208500/",
        "dblp_id": "329/6194",
        "google_scholar_url": "Wy6q2QwAAAAJ",
        "orcid": null,
        "linkedin_url": "sureshb1999/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "78iGZdqxYY",
      "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19373",
      "pdf_url": "https://openreview.net/pdf?id=78iGZdqxYY",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mridul_Gupta2",
        "name": "Mridul Gupta",
        "name_site": "Mridul Gupta, Sahil Manchanda, HARIPRASAD KODAMANA, Sayan Ranu",
        "openreview_id": "~Mridul_Gupta2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~aiz218322",
        "dblp_id": null,
        "google_scholar_url": "g_cTs3YAAAAJ",
        "orcid": "0009-0003-4343-4263",
        "linkedin_url": "mridul1618/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 16,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A0HKeKl4Nl",
      "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19277",
      "pdf_url": "https://openreview.net/pdf?id=A0HKeKl4Nl",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samyak_Jain1",
        "name": "Samyak Jain",
        "name_site": null,
        "openreview_id": "~Samyak_Jain1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://samyakjain0112.github.io/",
        "dblp_id": "249/4464.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-3785-4782",
        "linkedin_url": "samyak-jain-276738178/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Five AI (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 62,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EDPxCjXzSb",
      "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we proposeto tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19114",
      "pdf_url": "https://openreview.net/pdf?id=EDPxCjXzSb",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shyamgopal_Karthik1",
        "name": "Shyamgopal Karthik",
        "name_site": "Shyamgopal Karthik, Ameya Prabhu, Puneet Dokania, Vineet Gandhi",
        "openreview_id": "~Shyamgopal_Karthik1",
        "position": 1,
        "gender": null,
        "homepage_url": "https://sgk98.github.io/",
        "dblp_id": "251/8983",
        "google_scholar_url": "MofhemMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Snap Inc. (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 64,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ixi4j6LtdX",
      "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18953",
      "pdf_url": "https://openreview.net/pdf?id=Ixi4j6LtdX",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ayan_Sengupta1",
        "name": "Ayan Sengupta",
        "name_site": null,
        "openreview_id": "~Ayan_Sengupta1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://victor7246.github.io/",
        "dblp_id": null,
        "google_scholar_url": "90EGfboAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eUgS9Ig8JG",
      "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18219",
      "pdf_url": "https://openreview.net/pdf?id=eUgS9Ig8JG",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sundeep_Prabhakar_Chepuri1",
        "name": "Sundeep Prabhakar Chepuri",
        "name_site": null,
        "openreview_id": "~Sundeep_Prabhakar_Chepuri1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~spchepuri/",
        "dblp_id": "72/10237.html",
        "google_scholar_url": "Gu8FjdwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "f1xnBr4WD6",
      "title": "Cycle Consistency Driven Object Discovery",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing  consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18200",
      "pdf_url": "https://openreview.net/pdf?id=f1xnBr4WD6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Rajiv_Didolkar1",
        "name": "Aniket Rajiv Didolkar",
        "name_site": null,
        "openreview_id": "~Aniket_Rajiv_Didolkar1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/dido1998/",
        "dblp_id": "245/8589",
        "google_scholar_url": "https://scholar.google.ca/citations?user=ekvl5o0AAAAJ",
        "orcid": null,
        "linkedin_url": "aniket-didolkar-7a9b8912a",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Manipal Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fjpfCOV4ru",
      "title": "Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In this work, we consider rather general and broad class of Markov chains, Ito chains, that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent  noise instead of normal and state-independent one as in most related papers. Moreover, in our chain the drift and diffusion coefficient can be inexact in order to cover wide range of applications as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent or Stochastic Gradient Boosting. We prove the bound in $\\mathcal{W}_{2}$-distance between the laws of our Ito chain and corresponding differential equation. These results improve or cover most of the known estimates. And for some particular cases, our analysis is the first.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18173",
      "pdf_url": "https://openreview.net/pdf?id=fjpfCOV4ru",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aleksei_Ustimenko1",
        "name": "Aleksei Ustimenko",
        "name_site": "Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova",
        "openreview_id": "~Aleksei_Ustimenko1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "242/3873",
        "google_scholar_url": "OES5pK4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ShareChat (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.666666666666667,
        "rating_std": 2.0548046676563256,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fkrYDQaHOJ",
      "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons.\nWe approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent's action at every time step.\nOur approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18172",
      "pdf_url": "https://openreview.net/pdf?id=fkrYDQaHOJ",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal1",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://arnab39.github.io",
        "dblp_id": null,
        "google_scholar_url": "NhWR4yIAAAAJ",
        "orcid": null,
        "linkedin_url": "arnab-mondal-01b522a9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "McGill University (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jJvXNpvOdM",
      "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper presents a novel hierarchical task planner under partial observability\nthat empowers an embodied agent to use visual input to efficiently plan a sequence\nof actions for simultaneous object search and rearrangement in an untidy room,\nto achieve a desired tidy state. The paper introduces (i) a novel Search Network\nthat utilizes commonsense knowledge from large language models to find unseen\nobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel\ngraph-based state representation to produce a scalable and effective planner that\ninterleaves object search and rearrangement to minimize the number of steps taken\nand overall traversal of the agent, as well as to resolve blocked goal and swap\ncases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training\nof the proxy reward network along with the Deep RL network. Furthermore,\nthe paper presents new metrics and a benchmark dataset - RoPOR, to measure\nthe effectiveness of rearrangement planning. Experimental results show that our\nmethod significantly outperforms the state-of-the-art rearrangement methods Weihs\net al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18030",
      "pdf_url": "https://openreview.net/pdf?id=jJvXNpvOdM",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karan_Mirakhor1",
        "name": "Karan Mirakhor",
        "name_site": "Karan Mirakhor, Sourav Ghosh, DIPANJAN DAS, Brojeshwar Bhowmick",
        "openreview_id": "~Karan_Mirakhor1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "wpeFm64AAAAJ",
        "orcid": null,
        "linkedin_url": "karan-mirakhor-b065b7142",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services Limited (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jjA4O1vJRz",
      "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities,several new instances of these models are being trained towards new domains and tasks.  In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end,  we propose CALM—Composition to Augment Language Models—which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly,when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18011",
      "pdf_url": "https://openreview.net/pdf?id=jjA4O1vJRz",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rachit_Bansal1",
        "name": "Rachit Bansal",
        "name_site": null,
        "openreview_id": "~Rachit_Bansal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rachitbansal.github.io",
        "dblp_id": "228/6038",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7-x28WYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 44,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rxVBKhyfSo",
      "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective.  We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17671",
      "pdf_url": "https://openreview.net/pdf?id=rxVBKhyfSo",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uvFhCUPjtI",
      "title": "Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with \\eft for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17560",
      "pdf_url": "https://openreview.net/pdf?id=uvFhCUPjtI",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anson_Bastos1",
        "name": "Anson Bastos",
        "name_site": null,
        "openreview_id": "~Anson_Bastos1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "220/4367",
        "google_scholar_url": "is7rRuAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uz7d2N2zul",
      "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES : a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds (upper bounded by $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}} \\log ^{2 \\delta^{\\prime}}(n_k))$) and lower bounds of $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}})$, and how the overall generalization error on the data likelihood differs from a vanilla Federated Learning setup as a closed form function ${\\boldsymbol{\\Im}}(\\boldsymbol{w}, n_k)$ of the coreset weights $\\boldsymbol{w}$ and coreset sample size $n_k$. \nOur experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains as compared to  other submodular optimization based approaches used for subset selection on client's data.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17557",
      "pdf_url": "https://openreview.net/pdf?id=uz7d2N2zul",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prateek_Chanda2",
        "name": "Prateek Chanda",
        "name_site": "Prateek Chanda, Shrey Modi, Ganesh Ramakrishnan",
        "openreview_id": "~Prateek_Chanda2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://prateekiiest.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=2CiQLkYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "vtyasLn4RM",
      "title": "CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph. Beginning at the coarsest level, we iteratively refine and un-coarsen the layout, until we generate an embedding for the original graph. To enhance information propagation within the network, we propose a novel positional rewiring technique based on intermediate node positions. Our empirical evaluation demonstrates that the framework achieves state-of-the-art performance while remaining scalable.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17514",
      "pdf_url": "https://openreview.net/pdf?id=vtyasLn4RM",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Florian_Grötschla1",
        "name": "Florian Grötschla",
        "name_site": null,
        "openreview_id": "~Florian_Grötschla1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://disco.ethz.ch/members/fgroetschla",
        "dblp_id": "334/1811",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Oracle Corporation (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xtOydkE1Ku",
      "title": "TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series. Code is made available at https://github.com/ServiceNow/TACTiS.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17439",
      "pdf_url": "https://openreview.net/pdf?id=xtOydkE1Ku",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nebojsa_Bozanic1",
        "name": "Nebojsa Bozanic",
        "name_site": null,
        "openreview_id": "~Arjun_Ashok1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashok-arjun.github.io/",
        "dblp_id": "https://dblp.uni-trier.de/pid/318/2945",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ServiceNow (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "09xFexjhqE",
      "title": "AutoLoRa: An Automated Robust Fine-Tuning Framework",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, \nwhere optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art results across a range of downstream tasks. AutoLoRa holds significant practical utility, as it automatically converts a pre-trained FE into an adversarially robust model for downstream tasks without the need for searching hyperparameters. Our source code is available at [the GitHub](https://github.com/GodXuxilie/RobustSSL_Benchmark/tree/main/Finetuning_Methods/AutoLoRa).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19622",
      "pdf_url": "https://openreview.net/pdf?id=09xFexjhqE",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2iGiSHmeAN",
      "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19540",
      "pdf_url": "https://openreview.net/pdf?id=2iGiSHmeAN",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "78iGZdqxYY",
      "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19373",
      "pdf_url": "https://openreview.net/pdf?id=78iGZdqxYY",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 16,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7M0EzjugaN",
      "title": "Online Continual Learning for Interactive Instruction Following Agents",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19364",
      "pdf_url": "https://openreview.net/pdf?id=7M0EzjugaN",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Yonsei University (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ixi4j6LtdX",
      "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18953",
      "pdf_url": "https://openreview.net/pdf?id=Ixi4j6LtdX",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tanmoy_Chakraborty2",
        "name": "Tanmoy Chakraborty",
        "name_site": null,
        "openreview_id": "~Tanmoy_Chakraborty2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://tanmoychak.com",
        "dblp_id": "65/2136-2.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=C5S9JnIAAAAJ",
        "orcid": "0000-0002-0210-0369",
        "linkedin_url": "tanmoy-chakraborty-89553324/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VJvbOSXRUq",
      "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that\nall algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18507",
      "pdf_url": "https://openreview.net/pdf?id=VJvbOSXRUq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VVgGbB9TNV",
      "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18503",
      "pdf_url": "https://openreview.net/pdf?id=VVgGbB9TNV",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 32,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bDWXhzZT40",
      "title": "Learning model uncertainty as variance-minimizing instance weights",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Predictive uncertainty--a model’s self-awareness regarding its accuracy on an input--is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures predictive uncertainty using an auxiliary network, and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing dropout variance, an approximation of Bayesian predictive uncertainty, We show in controlled experiments that we effectively capture diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings–selective classification, label noise, domain adaptation, calibration–and across datasets–Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing-1.6M, etc. For Diabetic Retinopathy, we see upto 3.4\\%/3.3\\% accuracy & AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18342",
      "pdf_url": "https://openreview.net/pdf?id=bDWXhzZT40",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pradeep_Shenoy1",
        "name": "Pradeep Shenoy",
        "name_site": null,
        "openreview_id": "~Pradeep_Shenoy1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "12/771",
        "google_scholar_url": "lXbPKmkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jJvXNpvOdM",
      "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper presents a novel hierarchical task planner under partial observability\nthat empowers an embodied agent to use visual input to efficiently plan a sequence\nof actions for simultaneous object search and rearrangement in an untidy room,\nto achieve a desired tidy state. The paper introduces (i) a novel Search Network\nthat utilizes commonsense knowledge from large language models to find unseen\nobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel\ngraph-based state representation to produce a scalable and effective planner that\ninterleaves object search and rearrangement to minimize the number of steps taken\nand overall traversal of the agent, as well as to resolve blocked goal and swap\ncases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training\nof the proxy reward network along with the Deep RL network. Furthermore,\nthe paper presents new metrics and a benchmark dataset - RoPOR, to measure\nthe effectiveness of rearrangement planning. Experimental results show that our\nmethod significantly outperforms the state-of-the-art rearrangement methods Weihs\net al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18030",
      "pdf_url": "https://openreview.net/pdf?id=jJvXNpvOdM",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Brojeshwar_Bhowmick3",
        "name": "Brojeshwar Bhowmick",
        "name_site": null,
        "openreview_id": "~Brojeshwar_Bhowmick3",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/brojeshwar/home",
        "dblp_id": "88/7529",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Eqf8NrEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uz7d2N2zul",
      "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES : a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds (upper bounded by $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}} \\log ^{2 \\delta^{\\prime}}(n_k))$) and lower bounds of $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}})$, and how the overall generalization error on the data likelihood differs from a vanilla Federated Learning setup as a closed form function ${\\boldsymbol{\\Im}}(\\boldsymbol{w}, n_k)$ of the coreset weights $\\boldsymbol{w}$ and coreset sample size $n_k$. \nOur experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains as compared to  other submodular optimization based approaches used for subset selection on client's data.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17557",
      "pdf_url": "https://openreview.net/pdf?id=uz7d2N2zul",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ganesh_Ramakrishnan1",
        "name": "Ganesh Ramakrishnan",
        "name_site": null,
        "openreview_id": "~Ganesh_Ramakrishnan1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~ganesh/",
        "dblp_id": "r/GaneshRamakrishnan",
        "google_scholar_url": "https://scholar.google.com/scholar?hl=hi",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xw29VvOMmU",
      "title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative  algorithm  to decompose each  pretrained  matrix into a high-precision low-rank component  and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the  low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic  configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget.  We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the  reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17436",
      "pdf_url": "https://openreview.net/pdf?id=xw29VvOMmU",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 4,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 56,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Agrawal3",
        "name": "Aditya Agrawal",
        "name_site": null,
        "openreview_id": "~Aditya_Agrawal3",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://aditya-agrawal-30502.netlify.app/",
        "dblp_id": null,
        "google_scholar_url": "5OP8PEEAAAAJ",
        "orcid": null,
        "linkedin_url": "aditya-agrawal-9aba24208/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 6.75,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aanisha_Bhattacharyya2",
        "name": "Aanisha Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Aanisha_Bhattacharyya2",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://www.linkedin.com/in/aanisha-bhattacharyya/",
        "dblp_id": null,
        "google_scholar_url": "PkCeGdoAAAAJ",
        "orcid": null,
        "linkedin_url": "aanisha-bhattacharyya/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 6.0,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rxVBKhyfSo",
      "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective.  We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17671",
      "pdf_url": "https://openreview.net/pdf?id=rxVBKhyfSo",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Rangwani1",
        "name": "Harsh Rangwani",
        "name_site": null,
        "openreview_id": "~Harsh_Rangwani1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://rangwani-harsh.github.io/about/",
        "dblp_id": "220/0991",
        "google_scholar_url": "OQK0WREAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 6.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yaman_Kumar1",
        "name": "Yaman Kumar",
        "name_site": null,
        "openreview_id": "~Yaman_Kumar1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/yaman-kumar/",
        "dblp_id": "239/5601",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0001-7880-8219",
        "linkedin_url": "yaman-kumar/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 5.25,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Somesh_Singh1",
        "name": "Somesh Singh",
        "name_site": null,
        "openreview_id": "~Somesh_Singh1",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.5,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VJvbOSXRUq",
      "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that\nall algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18507",
      "pdf_url": "https://openreview.net/pdf?id=VJvbOSXRUq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samidha_Verma1",
        "name": "Samidha Verma",
        "name_site": null,
        "openreview_id": "~Samidha_Verma1",
        "position": 2,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "YPXPa7QAAAAJ",
        "orcid": null,
        "linkedin_url": "samidha-verma-a5b3b0125/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "dLoAdIKENc",
      "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of non-watermarked images detected as watermarked ones) upon an application of a diffusion purification attack. In this regime, we also empirically show that diffusion purification effectively removes watermarks with minimal changes to images. For high perturbation watermarking methods where notable changes are applied to images, the diffusion purification attack is not effective. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images (potentially obscene) identified as watermarked ones, damaging the reputation of the developers. In particular, by just having black-box access to the watermarking method, we show that one can generate a watermarked noise image which can be added to the real images to have them falsely flagged as watermarked ones. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments. Code is available at https://github.com/mehrdadsaberi/watermark_robustness.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18264",
      "pdf_url": "https://openreview.net/pdf?id=dLoAdIKENc",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinu_Sankar_Sadasivan1",
        "name": "Vinu Sankar Sadasivan",
        "name_site": null,
        "openreview_id": "~Vinu_Sankar_Sadasivan1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://vinusankars.github.io/",
        "dblp_id": "244/8052",
        "google_scholar_url": "y1IKIw0AAAAJ",
        "orcid": null,
        "linkedin_url": "vinusankars/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 47,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mw1PWNSWZP",
      "title": "OctoPack: Instruction Tuning Code Large Language Models",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17875",
      "pdf_url": "https://openreview.net/pdf?id=mw1PWNSWZP",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Binyuan_Hui1_1",
        "name": "Binyuan Hui",
        "name_site": null,
        "openreview_id": "~Swayam_Singh1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://swayaminsync.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?view_op=list_works",
        "orcid": null,
        "linkedin_url": "swayam-singh-406610213/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Allahabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 7.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 235,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Uttaran_Bhattacharya1",
        "name": "Uttaran Bhattacharya",
        "name_site": null,
        "openreview_id": "~Uttaran_Bhattacharya1",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://www.cs.umd.edu/~uttaranb/",
        "dblp_id": "220/7744",
        "google_scholar_url": "xx9nrfoAAAAJ",
        "orcid": "0000-0003-2141-9276",
        "linkedin_url": "uttaran-bhattacharya/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nqlymMx42E",
      "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17841",
      "pdf_url": "https://openreview.net/pdf?id=nqlymMx42E",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wISvONp3Kq",
      "title": "Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Generalized Linear Models (GLMs) encompass a wide array of regression and classification models, where prediction is a function of a linear combination of the input variables. Often in real-world scenarios, a number of observations would be added into or removed from the existing training dataset, necessitating the development of learning systems that can efficiently train optimal models with varying observations in an online (sequential) manner instead of retraining from scratch. Despite the significance of data-varying scenarios, most existing approaches to sparse GLMs concentrate on offline batch updates, leaving online solutions largely underexplored. In this work, we present the first algorithm without compromising accuracy for GLMs regularized by sparsity-enforcing penalties trained on varying observations. Our methodology is capable of handling the addition and deletion of observations simultaneously, while adaptively updating data-dependent regularization parameters to ensure the best statistical performance. Specifically, we recast sparse GLMs as a bilevel optimization objective upon varying observations and characterize it as an explicit gradient flow in the underlying space for the inner and outer subproblems we are optimizing over, respectively. We further derive a set of rules to ensure a proper transition at regions of non-smoothness, and establish the guarantees of theoretical consistency and finite convergence. Encouraging results are exhibited on real-world benchmarks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17497",
      "pdf_url": "https://openreview.net/pdf?id=wISvONp3Kq",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": 7.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2iGiSHmeAN",
      "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19540",
      "pdf_url": "https://openreview.net/pdf?id=2iGiSHmeAN",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayadeva_Jayadeva1",
        "name": "Jayadeva Jayadeva",
        "name_site": null,
        "openreview_id": "~Jayadeva_Jayadeva1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "58/4288",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "4Zz5UELkIt",
      "title": "Adaptive Instrument Design for Indirect Experiments",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Indirect experiments provide a valuable framework for estimating treatment effects in situations where conducting randomized control trials (RCTs) is impractical or unethical. Unlike RCTs, indirect experiments estimate treatment effects by leveraging (conditional) instrumental variables, enabling estimation through encouragement and recommendation rather than strict treatment assignment.  However, the sample efficiency of such estimators depends not only on the inherent variability in outcomes but also on the varying compliance levels of users with the instrumental variables and the choice of estimator being used, especially when dealing with numerous instrumental variables.  While adaptive experiment design has a rich literature for \\textit{direct} experiments, in this paper we take the initial steps towards enhancing sample efficiency for \\textit{indirect} experiments by adaptively designing a data collection policy over instrumental variables.  Our main contribution is a practical computational procedure that utilizes influence functions to search for an optimal data collection policy, minimizing the mean-squared error of the desired (non-linear) estimator. Through experiments conducted in various domains inspired by real-world applications, we showcase how our method can significantly improve the sample efficiency of indirect experiments.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19457",
      "pdf_url": "https://openreview.net/pdf?id=4Zz5UELkIt",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shiv_Shankar2",
        "name": "Shiv Shankar",
        "name_site": null,
        "openreview_id": "~Shiv_Shankar2",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "203/9123",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "78iGZdqxYY",
      "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19373",
      "pdf_url": "https://openreview.net/pdf?id=78iGZdqxYY",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sahil_Manchanda1",
        "name": "Sahil Manchanda",
        "name_site": null,
        "openreview_id": "~Sahil_Manchanda1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sahilm",
        "dblp_id": "200/8052",
        "google_scholar_url": "OPyjQHwAAAAJ",
        "orcid": "0000-0001-7437-9891",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 16,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Cf4FJGmHRQ",
      "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19165",
      "pdf_url": "https://openreview.net/pdf?id=Cf4FJGmHRQ",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Oregon State University (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ixi4j6LtdX",
      "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18953",
      "pdf_url": "https://openreview.net/pdf?id=Ixi4j6LtdX",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "shantanu20118@iiitd.ac.in",
        "name": "Shantanu Dixit",
        "name_site": null,
        "openreview_id": "shantanu20118@iiitd.ac.in",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VJvbOSXRUq",
      "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that\nall algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18507",
      "pdf_url": "https://openreview.net/pdf?id=VJvbOSXRUq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Burouj_Armgaan1",
        "name": "Burouj Armgaan",
        "name_site": null,
        "openreview_id": "~Burouj_Armgaan1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://armagaan.github.io/",
        "dblp_id": "349/0229",
        "google_scholar_url": "GaOJrMYAAAAJ",
        "orcid": "0009-0007-2423-1523",
        "linkedin_url": "burouj-armgaan/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jJvXNpvOdM",
      "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper presents a novel hierarchical task planner under partial observability\nthat empowers an embodied agent to use visual input to efficiently plan a sequence\nof actions for simultaneous object search and rearrangement in an untidy room,\nto achieve a desired tidy state. The paper introduces (i) a novel Search Network\nthat utilizes commonsense knowledge from large language models to find unseen\nobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel\ngraph-based state representation to produce a scalable and effective planner that\ninterleaves object search and rearrangement to minimize the number of steps taken\nand overall traversal of the agent, as well as to resolve blocked goal and swap\ncases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training\nof the proxy reward network along with the Deep RL network. Furthermore,\nthe paper presents new metrics and a benchmark dataset - RoPOR, to measure\nthe effectiveness of rearrangement planning. Experimental results show that our\nmethod significantly outperforms the state-of-the-art rearrangement methods Weihs\net al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18030",
      "pdf_url": "https://openreview.net/pdf?id=jJvXNpvOdM",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourav_Ghosh2",
        "name": "Sourav Ghosh",
        "name_site": null,
        "openreview_id": "~Sourav_Ghosh2",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": null,
        "linkedin_url": "sourav-ghosh-5927b1128/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services Limited (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "w1JanwReU6",
      "title": "Are Models Biased on Text without Gender-related Language?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: *Do language models still exhibit gender bias in non-stereotypical settings?* To do so, we introduce **UnStereoEval (USE)**, a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.  By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models.  Concretely, models demonstrate fair behavior in only 9%-41% of  stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at [ucinlp.github.io/unstereo-eval](https://ucinlp.github.io/unstereo-eval).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17511",
      "pdf_url": "https://openreview.net/pdf?id=w1JanwReU6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Preethi_Seshadri2",
        "name": "Preethi Seshadri",
        "name_site": null,
        "openreview_id": "~Preethi_Seshadri2",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://preethiseshadri518.github.io/",
        "dblp_id": "175/6462.html",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "preethi-seshadri/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of California, Irvine (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sY5N0zY5Od",
      "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17642",
      "pdf_url": "https://openreview.net/pdf?id=sY5N0zY5Od",
      "github_url": "",
      "total_authors": 13,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashutosh_Sharma1",
        "name": "Ashutosh Sharma",
        "name_site": null,
        "openreview_id": "~Ashutosh_Sharma1",
        "position": 8,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "xMDayLoAAAAJ",
        "orcid": null,
        "linkedin_url": "ashutoshuiuc/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Databricks (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.1249999999999996,
      "reviews": {
        "rating_mean": 7.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 32,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bNt7oajl2a",
      "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\\textit{inductive reasoners}$, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18334",
      "pdf_url": "https://openreview.net/pdf?id=bNt7oajl2a",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 8,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Southern California (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0000000000000004,
      "reviews": {
        "rating_mean": 8.0,
        "rating_std": 0.0,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 78,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rxVBKhyfSo",
      "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective.  We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17671",
      "pdf_url": "https://openreview.net/pdf?id=rxVBKhyfSo",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kunal_Samanta1",
        "name": "Kunal Samanta",
        "name_site": null,
        "openreview_id": "~Kunal_Samanta1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "kunal-samanta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VJvbOSXRUq",
      "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that\nall algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18507",
      "pdf_url": "https://openreview.net/pdf?id=VJvbOSXRUq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Khushbu_Pahwa1",
        "name": "Khushbu Pahwa",
        "name_site": null,
        "openreview_id": "~Khushbu_Pahwa1",
        "position": 4,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "299/8490",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "khushbupahwa",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Rice University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "f1xnBr4WD6",
      "title": "Cycle Consistency Driven Object Discovery",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing  consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18200",
      "pdf_url": "https://openreview.net/pdf?id=f1xnBr4WD6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "s90VIdza2K",
      "title": "f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. \nWhile numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these approaches are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term ``stochastic'' refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on $f$-divergence measures ($f$-FERM). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by $f$-FERM for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. \nOur extension is based on a distributionally robust optimization reformulation of $f$-FERM objective under $\\ell_p$ norms as uncertainty sets. Again, in this distributionally robust setting, $f$-FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts. \n An efficient stochastic implementation of $f$-FERM is publicly available.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17663",
      "pdf_url": "https://openreview.net/pdf?id=s90VIdza2K",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shivam_Patel1",
        "name": "Shivam Patel",
        "name_site": null,
        "openreview_id": "~Shivam_Patel1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://patel-shivam.github.io",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "shivam-patel02/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uz7d2N2zul",
      "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES : a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds (upper bounded by $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}} \\log ^{2 \\delta^{\\prime}}(n_k))$) and lower bounds of $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}})$, and how the overall generalization error on the data likelihood differs from a vanilla Federated Learning setup as a closed form function ${\\boldsymbol{\\Im}}(\\boldsymbol{w}, n_k)$ of the coreset weights $\\boldsymbol{w}$ and coreset sample size $n_k$. \nOur experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains as compared to  other submodular optimization based approaches used for subset selection on client's data.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17557",
      "pdf_url": "https://openreview.net/pdf?id=uz7d2N2zul",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shrey_Modi1",
        "name": "Shrey Modi",
        "name_site": null,
        "openreview_id": "~Shrey_Modi1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "shrey-modi-b059941bb/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jjA4O1vJRz",
      "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities,several new instances of these models are being trained towards new domains and tasks.  In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end,  we propose CALM—Composition to Augment Language Models—which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly,when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18011",
      "pdf_url": "https://openreview.net/pdf?id=jjA4O1vJRz",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sriram_Ganapathy1",
        "name": "Sriram Ganapathy",
        "name_site": null,
        "openreview_id": "~Sriram_Ganapathy1",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://leap.ee.iisc.ac.in/sriram/",
        "dblp_id": "23/4298.html",
        "google_scholar_url": "cgpzrtcAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.1428571428571432,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 44,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6ARlSgun7J",
      "title": "Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Extreme Classification (XC) architectures, which utilize a massive One-vs-All (OvA) classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these architectures falter on tail labels with few representative samples. This phenomenon has been attributed to factors such as classifier over-fitting and missing label bias, and solutions involving regularization and loss re-calibration have been developed. This paper explores the impact of label variance - a previously unexamined factor - on the tail performance in extreme classifiers. It also develops a method to systematically reduce label variance in XC by transferring the knowledge from a specialized tail-robust teacher model to the OvA classifiers. For this purpose, it proposes a principled knowledge distillation framework, LEVER, which enhances the tail performance in extreme classifiers with formal guarantees on generalization. Comprehensive experiments are conducted on a diverse set of XC datasets, demonstrating that LEVER can enhance tail performance by around 5\\% and 6\\% points in PSP and coverage metrics, respectively, when integrated with leading extreme classifiers. Moreover, it establishes a new state-of-the-art when added to the top-performing Renee classifier. Extensive ablations and analyses substantiate the efficacy of our design choices. Another significant contribution is the release of two new XC datasets that are different from and more challenging than the available benchmark datasets, thereby encouraging more rigorous algorithmic evaluation in the future. Code for LEVER is available at: aka.ms/lever.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19401",
      "pdf_url": "https://openreview.net/pdf?id=6ARlSgun7J",
      "github_url": "",
      "total_authors": 13,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vidit_Jain2",
        "name": "Vidit Jain",
        "name_site": null,
        "openreview_id": "~Vidit_Jain2",
        "position": 8,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "68/5650",
        "google_scholar_url": null,
        "orcid": "0000-0002-7911-1074",
        "linkedin_url": "jvidit/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.083333333333333,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Th6NyL07na",
      "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by C**o**ntrasting **La**yers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18563",
      "pdf_url": "https://openreview.net/pdf?id=Th6NyL07na",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 4,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 314,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UMfcdRIotC",
      "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18527",
      "pdf_url": "https://openreview.net/pdf?id=UMfcdRIotC",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (Israel)",
        "countries": [
          "Israel"
        ],
        "country_codes": [
          "IL"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 41,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2iGiSHmeAN",
      "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19540",
      "pdf_url": "https://openreview.net/pdf?id=2iGiSHmeAN",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "78iGZdqxYY",
      "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/19373",
      "pdf_url": "https://openreview.net/pdf?id=78iGZdqxYY",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~HARIPRASAD_KODAMANA1",
        "name": "HARIPRASAD KODAMANA",
        "name_site": null,
        "openreview_id": "~HARIPRASAD_KODAMANA1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~kodamana/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=YBcs36wAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 16,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ixi4j6LtdX",
      "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18953",
      "pdf_url": "https://openreview.net/pdf?id=Ixi4j6LtdX",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Md_Shad_Akhtar1",
        "name": "Md Shad Akhtar",
        "name_site": null,
        "openreview_id": "~Md_Shad_Akhtar1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "184/8579.html",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "YgMdDQB09U",
      "title": "AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Self-supervised learning through contrastive representations is an emergent and promising avenue, aiming at alleviating the availability of labeled data. Recent research in the field also demonstrates its viability for several downstream tasks, henceforth leading to works that implement the contrastive principle through innovative loss functions and methods. However, despite achieving impressive progress, most methods depend on prohibitively large batch sizes and compute requirements for good performance. \nIn this work, we propose the $\\textbf{AUC}$-$\\textbf{C}$ontrastive $\\textbf{L}$earning, a new approach to contrastive learning that demonstrates robust and competitive performance in compute-limited regimes. \nWe propose to incorporate the contrastive objective within the AUC-maximization framework, by noting that the AUC metric is maximized upon enhancing the probability of the network's binary prediction difference between positive and negative samples which inspires adequate embedding space arrangements in representation learning. Unlike standard contrastive methods, when performing stochastic optimization, our method maintains unbiased stochastic gradients and thus is more robust to batchsizes as opposed to standard stochastic optimization problems.\nRemarkably, our method with a batch size of 256, outperforms several state-of-the-art methods that may need much larger batch sizes (e.g., 4096), on ImageNet and other standard datasets. Experiments on transfer learning, few-shot learning, and other downstream tasks also demonstrate the viability of our method.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18415",
      "pdf_url": "https://openreview.net/pdf?id=YgMdDQB09U",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jJvXNpvOdM",
      "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper presents a novel hierarchical task planner under partial observability\nthat empowers an embodied agent to use visual input to efficiently plan a sequence\nof actions for simultaneous object search and rearrangement in an untidy room,\nto achieve a desired tidy state. The paper introduces (i) a novel Search Network\nthat utilizes commonsense knowledge from large language models to find unseen\nobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel\ngraph-based state representation to produce a scalable and effective planner that\ninterleaves object search and rearrangement to minimize the number of steps taken\nand overall traversal of the agent, as well as to resolve blocked goal and swap\ncases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training\nof the proxy reward network along with the Deep RL network. Furthermore,\nthe paper presents new metrics and a benchmark dataset - RoPOR, to measure\nthe effectiveness of rearrangement planning. Experimental results show that our\nmethod significantly outperforms the state-of-the-art rearrangement methods Weihs\net al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18030",
      "pdf_url": "https://openreview.net/pdf?id=jJvXNpvOdM",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dipanjan_Das4",
        "name": "Dipanjan Das",
        "name_site": null,
        "openreview_id": "~Dipanjan_Das4",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "https://dblp.uni-trier.de/pers/hd/d/Das_0003:Dipanjan",
        "google_scholar_url": "HRZMDDsAAAAJ",
        "orcid": null,
        "linkedin_url": "dipanjan-das-38859019",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kGteeZ18Ir",
      "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like ‘_You are Yoda. Explain the Theory of Relativity._’ While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs’ capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform _basic reasoning tasks_. Our study covers 24 reasoning datasets (spanning mathematics, law, medicine, morals, and more), 4 LLMs (2 versions of ChatGPT-3.5, GPT-4-Turbo, and Llama-2-70b-chat), and 19 diverse personas (e.g., ‘an Asian person’) spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked (‘_Are Black people less skilled at mathematics?_’), they manifest stereotypical and often erroneous presumptions when prompted to answer questions while adopting a persona. These can be observed as abstentions in the model’s response, e.g., ‘_As a Black person, I am unable to answer this question as it requires math knowledge_’, and generally result in a substantial drop in performance on reasoning tasks. Our experiments with ChatGPT-3.5 show that this bias is _ubiquitous_&mdash;80% of our personas demonstrate bias; it is _significant_&mdash;some datasets show performance drops of 70%+; and can be especially _harmful for certain groups_&mdash;some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all four LLMs exhibit persona-induced bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern as they do not always manifest as explicit abstentions, and can also be hard-to-avoid&mdash;we find de-biasing prompts to have minimal to no effect. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs&mdash;a trend on the rise&mdash;can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17986",
      "pdf_url": "https://openreview.net/pdf?id=kGteeZ18Ir",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Khot1",
        "name": "Tushar Khot",
        "name_site": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal",
        "openreview_id": "~Tushar_Khot1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://allenai.org/team/tushark/",
        "dblp_id": "83/8117",
        "google_scholar_url": "_8mkIjgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Allen Institute for Artificial Intelligence (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 101,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ptXo0epLQo",
      "title": "$\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Understanding and developing optimal representations has long been foundational in machine learning (ML). While disentangled representations have shown promise in generative modeling and representation learning, their downstream usefulness remains debated. Recent studies re-defined disentanglement through a formal connection to symmetries, emphasizing the ability to reduce latent domains (i.e., ML problem spaces) and consequently enhance data efficiency and generative capabilities. However, from an information theory viewpoint, assigning a complex attribute (i.e., features) to a specific latent variable may be infeasible, limiting the applicability of disentangled representations to simple datasets. In this work, we introduce $\\alpha$-TCVAE, a variational autoencoder optimized using a novel total correlation (TC) lower bound that maximizes disentanglement and latent variables informativeness. The proposed TC bound is grounded in information theory constructs, generalizes the $\\beta$-VAE lower bound, and can be reduced to a convex combination of the known variational information bottleneck (VIB) and conditional entropy bottleneck (CEB) terms. Moreover, we present quantitative analyses and correlation studies that support the idea that smaller latent domains (i.e., disentangled representations) lead to better generative capabilities and diversity. Additionally, we perform downstream task experiments from both representation and RL domains to assess our questions from a broader ML perspective. Our results demonstrate that $\\alpha$-TCVAE consistently learns more disentangled representations than baselines and generates more diverse observations without sacrificing visual fidelity. Notably, $\\alpha$-TCVAE exhibits marked improvements on MPI3D-Real, the most realistic disentangled dataset in our study,  confirming its ability to represent complex datasets when maximizing the informativeness of individual variables. Finally, testing the proposed model off-the-shelf on a state-of-the-art model-based RL agent, Director, significantly shows $\\alpha$-TCVAE downstream usefulness on the loconav Ant Maze task. Implementation available at https://github.com/Cmeo97/Alpha-TCVAE",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17762",
      "pdf_url": "https://openreview.net/pdf?id=ptXo0epLQo",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wHBfxhZu1u",
      "title": "YaRN: Efficient Context Window Extension of Large Language Models",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17499",
      "pdf_url": "https://openreview.net/pdf?id=wHBfxhZu1u",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Honglu_Fan1",
        "name": "Honglu Fan",
        "name_site": null,
        "openreview_id": "~Honglu_Fan1",
        "position": 3,
        "gender": "Not Specified",
        "homepage_url": "https://honglu.fan",
        "dblp_id": null,
        "google_scholar_url": "XqlOVeAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Teraflop AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 352,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "SQrHpTllXa",
      "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18603",
      "pdf_url": "https://openreview.net/pdf?id=SQrHpTllXa",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yaman_Kumar1",
        "name": "Yaman Kumar",
        "name_site": null,
        "openreview_id": "~Yaman_Kumar1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/yaman-kumar/",
        "dblp_id": "239/5601",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0001-7880-8219",
        "linkedin_url": "yaman-kumar/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 1.4999999999999996,
      "reviews": {
        "rating_mean": 8.0,
        "rating_std": 0.0,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 18,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TrKq4Wlwcz",
      "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18547",
      "pdf_url": "https://openreview.net/pdf?id=TrKq4Wlwcz",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rajiv_Ratn_Shah1",
        "name": "Rajiv Ratn Shah",
        "name_site": null,
        "openreview_id": "~Rajiv_Ratn_Shah1",
        "position": 9,
        "gender": "M",
        "homepage_url": "https://iiitd.ac.in/rajivratn",
        "dblp_id": "134/3502",
        "google_scholar_url": "https://scholar.google.com.sg/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.4999999999999996,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 1.920286436967152,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "x1ptaXpOYa",
      "title": "ADOPD: A Large-Scale Document Page Decomposition Dataset",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Research in document image understanding is hindered by limited high-quality document data. To address this, we introduce ADOPD, a comprehensive dataset for document page decomposition. ADOPD stands out with its data-driven approach for document taxonomy discovery during data collection, complemented by dense annotations. Our approach integrates large-scale pretrained models with a human-in-the-loop process to guarantee diversity and balance in the resulting data collection. Leveraging our data-driven document taxonomy, we collect and densely annotate document images, addressing four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions that have been manually cleaned. We conduct comprehensive experimental analyses to validate our data and assess the four tasks using various models. We envision ADOPD as a foundational dataset with the potential to drive future research in document understanding.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/17472",
      "pdf_url": "https://openreview.net/pdf?id=x1ptaXpOYa",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 6,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 1.4285714285714284,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Jf5gplvglq",
      "title": "SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora & Goyal, 2023).\n\nThis work introduces SKILL-MIX, a new evaluation to measure ability to combine skills. Using a list of $N$  skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. \nThe paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. \n\nAdministering a version of SKILL-MIX to popular chatbots gave results that,  while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training.\n\nWe sketch how the methodology can lead to a SKILL-MIX based eco-system of open evaluations for AI capabilities of future models. We maintain a leaderboard of SKILL-MIX at [https://skill-mix.github.io](https://skill-mix.github.io).",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18931",
      "pdf_url": "https://openreview.net/pdf?id=Jf5gplvglq",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Princeton University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VJvbOSXRUq",
      "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that\nall algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
      "tldr": "",
      "site_url": "https://iclr.cc/virtual/2024/poster/18507",
      "pdf_url": "https://openreview.net/pdf?id=VJvbOSXRUq",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourav_Medya1",
        "name": "Sourav Medya",
        "name_site": null,
        "openreview_id": "~Sourav_Medya1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://souravmedya.github.io/",
        "dblp_id": "178/3021",
        "google_scholar_url": "RCFhOM4AAAAJ",
        "orcid": "0000-0003-0996-2807",
        "linkedin_url": "sourav-medya-35987a49/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8333333333333331,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "030cjlZm4a",
      "title": "Learning Predictive Checklists with Probabilistic Logic Programming",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Checklists have been widely recognized as effective tools for completing complex tasks in a systematic manner. Although originally intended for use in procedural tasks, their interpretability and ease of use have led to their adoption for predictive tasks as well, including in clinical settings. However, designing checklists can be challenging, often requiring expert knowledge and manual rule design based on available data. Recent work has attempted to address this issue by using machine learning to automatically generate predictive checklists from data, although these approaches have been limited to Boolean data. We propose a novel method for learning predictive checklists from diverse data modalities, such as images, time series, and text, by combining the power of dedicated deep learning architectures with the interpretability and conciseness of checklists. Our approach relies on probabilistic logic programming, a learning paradigm that enables matching the discrete nature of a checklist with continuous-valued data. We propose a regularization technique to tradeoff between the information captured in discrete concepts of continuous data and permit a tunable level of interpretability for the learned checklist concepts. We demonstrate that our method outperforms various explainable machine learning techniques on prediction tasks involving image sequences, clinical notes, and time series.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=030cjlZm4a",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yukti_Makhija1",
        "name": "Yukti Makhija",
        "name_site": null,
        "openreview_id": "~Yukti_Makhija1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Yale University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.666666666666667,
        "rating_std": 2.0548046676563256,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0Y26tFG3WF",
      "title": "Inducing Precision in Lagrangian Neural Networks : Proof of concept application on Chaotic systems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Solutions of dynamic systems that exhibit chaotic behavior are particularly sensitive to errors in initial/intermediate state estimates when long term dynamics is of interest. Lagrangian Neural Networks (LNN) are a class of physics induced learning methods that seamlessly integrate physical conservation laws into functional solutions, by forming a parametric Lagrangian for the system of interest. However it has been seen that the function approximation error associated with the parametric Lagrangian modelling could prove to be catastrophic for the prediction of long term dynamics of chaotic systems. This makes improving the precision of the parametric Lagrangian particularly crucial. Considering the same in this work a modified Lagrangian Neural Network approach is proposed, where a customized neural network architecture is designed to directly emphasize the relative importance of each significant bit in the Lagrangian estimates produced. We evaluate our method on two dynamic systems that are well known in the literature in exhibiting deterministic chaos, namely the double pendulum and Henon-Helies systems. Further, we compare the obtained solutions with those estimated by Finite Element solvers (under optimal conditions) to validate the relative accuracy. We observe that the trajectory deviations as a result of chaotic behavior can be significantly reduced by the process of explicitly enforcing the precision requirement for the parametric Lagrangian, as modelled using the proposed approach.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=0Y26tFG3WF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Hrithwik_Shalu1",
        "name": "Hrithwik Shalu",
        "name_site": null,
        "openreview_id": "~Hrithwik_Shalu1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "271/7953",
        "google_scholar_url": null,
        "orcid": "0000-0002-8944-434X",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0sO2euxhUQ",
      "title": "Learning Latent Structural Causal Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Causal learning has long concerned itself with the recovery of underlying causal mechanisms. Such causal modelling enables better explanations of out-of-distribution data. Prior works on causal learning assume that the causal variables are given. However, in machine learning tasks, one often operates on low-level data like image pixels or high-dimensional vectors. In such settings, the entire Structural Causal Model (SCM) -- structure, parameters, \\textit{and} high-level causal variables -- is latent and needs to be learnt from low-level data. We treat this problem as Bayesian inference of the latent SCM, given low-level data. We present BIOLS, a tractable approximate inference method which performs joint inference over the causal variables, structure and parameters of the latent SCM from known interventions. Experiments are performed on synthetic datasets and a causal benchmark image dataset to demonstrate the efficacy of our approach. We also demonstrate the ability of BIOLS to generate images from unseen interventional distributions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=0sO2euxhUQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jithendaraa_Subramanian1",
        "name": "Jithendaraa Subramanian",
        "name_site": null,
        "openreview_id": "~Jithendaraa_Subramanian1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://jithendaraa.github.io/",
        "dblp_id": "281/6755",
        "google_scholar_url": "s0BzYvYAAAAJ",
        "orcid": null,
        "linkedin_url": "jithendaraa-subramanian-85a22b176/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "McGill University (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1M8yDTa0Pp",
      "title": "Cross-Model Semi-Supervised Prompt Learning for Vision-Language Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Prompt learning, which focuses on learning continuous soft prompts, has emerged as a promising approach for\nefficiently adapting pretrained vision-language models (VLMs) to multiple downstream tasks. While prior works have shown promising performances on common benchmarks, they typically rely on labeled data samples only. This greatly discredits the information gain from the vast collection of otherwise unlabeled samples available in the wild. To mitigate this, we propose a simple yet efficient cross-model framework to leverage on the unlabeled samples achieving significant gain in model performance. Specifically, we employ a semi-supervised prompt learning approach which makes the learned prompts invariant to the different views of a given unlabeled sample. The multiple views are obtained using different augmentations on the images as well as by varying the lengths of visual and text prompts attached to these samples. Experimenting with this simple yet surprisingly effective approach over a large number of benchmark datasets, we observe a considerable improvement in the quality of soft prompts thereby making an immense gain in image classification performance. Interestingly, our approach also benefits from out-of-domain unlabeled images highlighting the robustness and generalization capabilities. Our code will be made publicly available.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1M8yDTa0Pp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Omprakash_Chakraborty1",
        "name": "Omprakash Chakraborty",
        "name_site": "Omprakash Chakraborty, Aadarsh Sahoo, Rameswar Panda, Abir Das",
        "openreview_id": "~Omprakash_Chakraborty1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "182/4466.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Z0uiqiIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1M8yDTa0Pp",
      "title": "Cross-Model Semi-Supervised Prompt Learning for Vision-Language Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Prompt learning, which focuses on learning continuous soft prompts, has emerged as a promising approach for\nefficiently adapting pretrained vision-language models (VLMs) to multiple downstream tasks. While prior works have shown promising performances on common benchmarks, they typically rely on labeled data samples only. This greatly discredits the information gain from the vast collection of otherwise unlabeled samples available in the wild. To mitigate this, we propose a simple yet efficient cross-model framework to leverage on the unlabeled samples achieving significant gain in model performance. Specifically, we employ a semi-supervised prompt learning approach which makes the learned prompts invariant to the different views of a given unlabeled sample. The multiple views are obtained using different augmentations on the images as well as by varying the lengths of visual and text prompts attached to these samples. Experimenting with this simple yet surprisingly effective approach over a large number of benchmark datasets, we observe a considerable improvement in the quality of soft prompts thereby making an immense gain in image classification performance. Interestingly, our approach also benefits from out-of-domain unlabeled images highlighting the robustness and generalization capabilities. Our code will be made publicly available.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1M8yDTa0Pp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_Das4",
        "name": "Abir Das",
        "name_site": null,
        "openreview_id": "~Abir_Das4",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~adas/",
        "dblp_id": "141/1311",
        "google_scholar_url": "L4yEk2UAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1WSd408I9M",
      "title": "Generative AI in healthcare: A trustworthy approach",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Generative AI in healthcare: A trustworthy approach\n\nAbstract: The recent advancements in self-supervised algorithms like Transformer Architecture and Diffusion models have expanded the means of applying AI in healthcare and life sciences. To achieve real world adoption, it is important to measure and audit the trustworthiness of the AI system as per the legal and compliance requirements for privacy, security, fairness, and safety. In this paper, we focus on the method to achieve trustworthiness in an LLM (Large Language Model) based decision support system for physicians. The stakeholders for this decision support system are patients, physicians, regulators, and external auditors. We focus on the limitations of large or foundation models and the method to overcome these limitations, with the aim of accelerating the adoption of this far-reaching technology in the healthcare sector. It also explores possible guardrails for safety and the methods for aligning AI systems to guardrails.\n\nOur Solution Approach:\nWe explore an approach to an AI system which can enhance decision capabilities by using the data and EHRs (Electronic Health Record) collected over many years for a vast volume of patients. The longitudinal data consists of biomarkers, disease progression indicators, treatment administered, and patient outcome. The goal of the system is to assist physicians in identifying the best treatment option for a given patient context. The LLM-based system will be able to predict optimal options based on hundreds of similar cases on which it was trained. The paper addresses the transparency, data integrity, model development, and performance validation of the system.  In the sections below, we explore the various stages of development and deployment of such a system, the challenges, and the methods to overcome the challenges.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1WSd408I9M",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~parul_berry1",
        "name": "parul berry",
        "name_site": null,
        "openreview_id": "~parul_berry1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Wipro (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.0,
        "rating_std": 0.0,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1mjbfedaye",
      "title": "Learning Equi-angular Representations for Online Continual Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Online continual learning suffers from an underfitted solution for prompt model update due to the constraint of single-epoch learning. We confront this challenge by proposing an efficient online continual learning method with the notion of neural collapse. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the learned model with single epoch can better fit the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR10/100, TinyImageNet, and ImageNet-200, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios, including Disjoint and Gaussian scheduled setups.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1mjbfedaye",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Yonsei University (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1yll8U12GT",
      "title": "Enhancing Decision Tree Learning with Deep Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1yll8U12GT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prithaj_Banerjee1",
        "name": "Prithaj Banerjee",
        "name_site": null,
        "openreview_id": "~Prithaj_Banerjee1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.cse.iitm.ac.in/profile.php?arg=Mjc4Mg==",
        "dblp_id": "206/7099.html",
        "google_scholar_url": "rlr99eAAAAAJ",
        "orcid": null,
        "linkedin_url": "prithaj-banerjee-946a3a104/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1yll8U12GT",
      "title": "Enhancing Decision Tree Learning with Deep Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1yll8U12GT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mahesh_Lorik_Yadav1",
        "name": "Mahesh Lorik Yadav",
        "name_site": null,
        "openreview_id": "~Mahesh_Lorik_Yadav1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mahesh-yadav-25779716a/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1yll8U12GT",
      "title": "Enhancing Decision Tree Learning with Deep Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1yll8U12GT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harish_Guruprasad_Ramaswamy1",
        "name": "Harish Guruprasad Ramaswamy",
        "name_site": null,
        "openreview_id": "~Harish_Guruprasad_Ramaswamy1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "126/1729",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1yll8U12GT",
      "title": "Enhancing Decision Tree Learning with Deep Networks",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1yll8U12GT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chandra_Shekar_Lakshminarayanan2",
        "name": "Chandra Shekar Lakshminarayanan",
        "name_site": null,
        "openreview_id": "~Chandra_Shekar_Lakshminarayanan2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://iitpkd.ac.in/people/cnarayanan",
        "dblp_id": "143/7535",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1zhM0XkQh0",
      "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Supervised adversarial training has been the most successful approach for improving the robustness of Deep Neural Networks against adversarial attacks. While several recent works have attempted to overcome the need for supervision or labeled training data by integrating adversarial training with contrastive Self-Supervised Learning (SSL) approaches such as SimCLR, their performance has been sub-optimal due to the increased training complexity. A recent approach mitigates this by utilizing supervision from a standard self-supervised trained model in a teacher-student setting that mimics supervised adversarial training. However, we find that there is still a large gap in performance when compared to supervised training, specifically on larger capacity models. We show that this is a result of mismatch in training objectives of the teacher and student, and propose Projected Feature Adversarial Training (ProFeAT) to bridge this gap by using a projection head in the adversarial training step. We further propose appropriate attack and defense losses at the feature and projector spaces, coupled with a  combination of weak and strong augmentations for the teacher and student respectively, to improve generalization without increasing the training complexity. We demonstrate significant improvements in performance when compared to existing SSL methods, and performance on par with TRADES, a popular supervised adversarial training method, on several benchmark datasets and models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1zhM0XkQh0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sravanti_Addepalli1",
        "name": "Sravanti Addepalli",
        "name_site": null,
        "openreview_id": "~Sravanti_Addepalli1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "127/7715",
        "google_scholar_url": "MOO12i0AAAAJ",
        "orcid": null,
        "linkedin_url": "sravanti-addepalli/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1zhM0XkQh0",
      "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Supervised adversarial training has been the most successful approach for improving the robustness of Deep Neural Networks against adversarial attacks. While several recent works have attempted to overcome the need for supervision or labeled training data by integrating adversarial training with contrastive Self-Supervised Learning (SSL) approaches such as SimCLR, their performance has been sub-optimal due to the increased training complexity. A recent approach mitigates this by utilizing supervision from a standard self-supervised trained model in a teacher-student setting that mimics supervised adversarial training. However, we find that there is still a large gap in performance when compared to supervised training, specifically on larger capacity models. We show that this is a result of mismatch in training objectives of the teacher and student, and propose Projected Feature Adversarial Training (ProFeAT) to bridge this gap by using a projection head in the adversarial training step. We further propose appropriate attack and defense losses at the feature and projector spaces, coupled with a  combination of weak and strong augmentations for the teacher and student respectively, to improve generalization without increasing the training complexity. We demonstrate significant improvements in performance when compared to existing SSL methods, and performance on par with TRADES, a popular supervised adversarial training method, on several benchmark datasets and models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1zhM0XkQh0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Priyam_Dey1",
        "name": "Priyam Dey",
        "name_site": null,
        "openreview_id": "~Priyam_Dey1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "YMu3SJ8AAAAJ",
        "orcid": "0000-0001-5807-1379",
        "linkedin_url": "priyam-dey33",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1zhM0XkQh0",
      "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Supervised adversarial training has been the most successful approach for improving the robustness of Deep Neural Networks against adversarial attacks. While several recent works have attempted to overcome the need for supervision or labeled training data by integrating adversarial training with contrastive Self-Supervised Learning (SSL) approaches such as SimCLR, their performance has been sub-optimal due to the increased training complexity. A recent approach mitigates this by utilizing supervision from a standard self-supervised trained model in a teacher-student setting that mimics supervised adversarial training. However, we find that there is still a large gap in performance when compared to supervised training, specifically on larger capacity models. We show that this is a result of mismatch in training objectives of the teacher and student, and propose Projected Feature Adversarial Training (ProFeAT) to bridge this gap by using a projection head in the adversarial training step. We further propose appropriate attack and defense losses at the feature and projector spaces, coupled with a  combination of weak and strong augmentations for the teacher and student respectively, to improve generalization without increasing the training complexity. We demonstrate significant improvements in performance when compared to existing SSL methods, and performance on par with TRADES, a popular supervised adversarial training method, on several benchmark datasets and models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=1zhM0XkQh0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2RJAzSphy9",
      "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Preference-based feedback is important for many applications in reinforcement learning where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback (RLHF) on large language models. For many applications of RLHF, the cost of acquiring the human feedback can be substantial. In this work, we take advantage of the fact that one can often choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and formalize this as an *offline contextual dueling bandit* problem. We give an upper-confidence-bound style algorithm for this problem and prove a polynomial worst-case regret bound. We then provide empirical confirmation in a synthetic setting that our approach outperforms existing methods. After, we extend the setting and methodology for practical use in RLHF training of large language models. Here, our method is able to reach better performance with fewer samples of human preferences than multiple baselines on three real-world datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=2RJAzSphy9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ilija_Bogunovic2",
        "name": "Ilija Bogunovic",
        "name_site": null,
        "openreview_id": "~Ilija_Bogunovic1",
        "position": 7,
        "gender": "M",
        "homepage_url": "http://ilijabogunovic.com/",
        "dblp_id": "142/2725",
        "google_scholar_url": "xMvt3NEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Swiss Federal Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.8,
        "rating_std": 0.9797958971132712,
        "confidence_mean": 3.0,
        "confidence_std": 1.0954451150103321,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3NMYMLL92j",
      "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Multimodal associative learning of sensory stimuli (images, text, audio) has created powerful representations for these modalities that work across a multitude of tasks with simple task heads without even (fine)tuning features on target datasets. Such representations are being increasingly used to study neural activity and understand how our brain responds to such stimuli. While previous work has focused on static images, deep understanding of a video involves not just recognizing the individual objects present in each frame, but also requires a detailed semantic description of their interactions over time and their narrative roles. In this paper, we seek to evaluate whether new multimodally aligned features (like ImageBind) are better than previous ones in explaining fMRI responses to external stimuli, thereby allowing for a better understanding of how the brain and its different areas process external stimuli, converting them into meaningful high-level understanding, and actionable signals. In addition, we explore whether generative AI based modality conversion helps to disentangle the semantic part of the visual stimulus allowing for a more granular localization of such processing in the brain. Towards this end, given a dataset of fMRI responses from subjects watching short video clips, we first generate detailed multi-event video captions. Next, we synthesize audio from these generated text captions using a text-to-speech model. Further, we use a joint embedding across different modalities (audio, text and video) using the recently proposed ImageBind model. We use this joint embedding to train encoding models that predict fMRI brain responses. We infer from our experimental findings and computational results that the visual system's primary goal may revolve around converting visual input into comprehensive semantic scene descriptions. Further, multimodal feature alignment helps obtain richer representations for all modalities (audio, text and video) leading to improved performance compared to unimodal representations across well-known multimodal processing brain regions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3NMYMLL92j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Khushbu_Pahwa1",
        "name": "Khushbu Pahwa",
        "name_site": null,
        "openreview_id": "~Khushbu_Pahwa1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "299/8490",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "khushbupahwa",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Rice University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 2.943920288775949,
        "confidence_mean": 5.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3NMYMLL92j",
      "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Multimodal associative learning of sensory stimuli (images, text, audio) has created powerful representations for these modalities that work across a multitude of tasks with simple task heads without even (fine)tuning features on target datasets. Such representations are being increasingly used to study neural activity and understand how our brain responds to such stimuli. While previous work has focused on static images, deep understanding of a video involves not just recognizing the individual objects present in each frame, but also requires a detailed semantic description of their interactions over time and their narrative roles. In this paper, we seek to evaluate whether new multimodally aligned features (like ImageBind) are better than previous ones in explaining fMRI responses to external stimuli, thereby allowing for a better understanding of how the brain and its different areas process external stimuli, converting them into meaningful high-level understanding, and actionable signals. In addition, we explore whether generative AI based modality conversion helps to disentangle the semantic part of the visual stimulus allowing for a more granular localization of such processing in the brain. Towards this end, given a dataset of fMRI responses from subjects watching short video clips, we first generate detailed multi-event video captions. Next, we synthesize audio from these generated text captions using a text-to-speech model. Further, we use a joint embedding across different modalities (audio, text and video) using the recently proposed ImageBind model. We use this joint embedding to train encoding models that predict fMRI brain responses. We infer from our experimental findings and computational results that the visual system's primary goal may revolve around converting visual input into comprehensive semantic scene descriptions. Further, multimodal feature alignment helps obtain richer representations for all modalities (audio, text and video) leading to improved performance compared to unimodal representations across well-known multimodal processing brain regions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3NMYMLL92j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Advaith_Malladi1",
        "name": "Advaith Malladi",
        "name_site": null,
        "openreview_id": "~Advaith_Malladi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://advaithmall.github.io/",
        "dblp_id": "372/4611",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=uZZrApkAAAAJ",
        "orcid": null,
        "linkedin_url": "advaith-malladi-30175326b/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 2.943920288775949,
        "confidence_mean": 5.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3NMYMLL92j",
      "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Multimodal associative learning of sensory stimuli (images, text, audio) has created powerful representations for these modalities that work across a multitude of tasks with simple task heads without even (fine)tuning features on target datasets. Such representations are being increasingly used to study neural activity and understand how our brain responds to such stimuli. While previous work has focused on static images, deep understanding of a video involves not just recognizing the individual objects present in each frame, but also requires a detailed semantic description of their interactions over time and their narrative roles. In this paper, we seek to evaluate whether new multimodally aligned features (like ImageBind) are better than previous ones in explaining fMRI responses to external stimuli, thereby allowing for a better understanding of how the brain and its different areas process external stimuli, converting them into meaningful high-level understanding, and actionable signals. In addition, we explore whether generative AI based modality conversion helps to disentangle the semantic part of the visual stimulus allowing for a more granular localization of such processing in the brain. Towards this end, given a dataset of fMRI responses from subjects watching short video clips, we first generate detailed multi-event video captions. Next, we synthesize audio from these generated text captions using a text-to-speech model. Further, we use a joint embedding across different modalities (audio, text and video) using the recently proposed ImageBind model. We use this joint embedding to train encoding models that predict fMRI brain responses. We infer from our experimental findings and computational results that the visual system's primary goal may revolve around converting visual input into comprehensive semantic scene descriptions. Further, multimodal feature alignment helps obtain richer representations for all modalities (audio, text and video) leading to improved performance compared to unimodal representations across well-known multimodal processing brain regions.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3NMYMLL92j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manish_Gupta1",
        "name": "Manish Gupta",
        "name_site": null,
        "openreview_id": "~Manish_Gupta1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/manishg/",
        "dblp_id": "g/ManishGupta1.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=eX9PSu0AAAAJ",
        "orcid": "0000-0002-2843-3110",
        "linkedin_url": "manishsgupta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 2.943920288775949,
        "confidence_mean": 5.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Ok7ccvtf3",
      "title": "UNLEARNING THE UNWANTED DATA FROM A PERSONALIZED RECOMMENDATION MODEL",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Recommender Systems (RS) learn user behavior by monitoring their activities on the online platform. In a few scenarios, users consume the content but don’t want to get their recommendations because a). They consumed the content by mistake, and those interactions have been utilized in personalizing the model; b) The content was consumed by someone else on their behalf; c) Data acquisition was faulty because of machine failure; d) The user has lost interest in the service, etc. Out of any of these reasons, the user wants the data that was used for generating the recommendation to be unlearned by RS. The constraints with this unlearning are 1) The user’s other data should be intact, 2) Personalized experience should not be affected, and 3) We can not afford training from scratch. To solve the stated problem, a few unlearning strategies have already been proposed, but unlearning the matrix factorization-based model is not much explored. In this work, we propose a solution of unlearning from the faulty recommendation model (m1) by diluting the impact of unwanted data. To do so, we first correct the unwanted data and pre- pare an intermediate tiny model m2, referred to as the rescue model. Further, we apply the convolution fusion function (CFF) on the latent features acquired using m1 , m2 . The performance of the proposed method is evaluated on multiple public datasets. We observed that the proposed method outperforms SOTA benchmark models on recommendation tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3Ok7ccvtf3",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Brijraj_Singh1",
        "name": "Brijraj Singh",
        "name_site": null,
        "openreview_id": "~Brijraj_Singh1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3Zm6wR5Mvc",
      "title": "LangNav: Language as a Perceptual Representation for Navigation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an  agent's egocentric panoramic view at each time step  into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the  trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language  as the perceptual representation. We explore two use cases of our language-based navigation ours approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller  language model; and sim-to-real transfer where we transfer a policy learned on a simulated environment (ALFRED) to a real-world environment (R2R). Our approach is found to improve upon strong baselines that rely on visual features in settings where only a few gold trajectories (10-100) are available, demonstrating the potential of using language as a perceptual representation for learning navigation agents.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3Zm6wR5Mvc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 7,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.8973665961010275,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 19,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3klVRLhK7w",
      "title": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Majority of online continual learning (CL) places restrictions on the size of replay memory and a single-epoch training to ensure a prompt update of the model. However, the single-epoch training may imply a different amount of computations per CL algorithm, and additional storage for storing logit or model in addition to replay memory is largely ignored as a storage budget. Here, we used floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare CL algorithms with the same total budget. Interestingly, we found that the new and advanced algorithms often perform worse than simple baselines under the same budget, implying that their value is less beneficial in real-world deployment. To improve the accuracy of online continual learners in the same budget, we propose an adaptive layer freezing and frequency-based memory retrieval for episodic memory usage for a storage- and computationally-efficient online CL algorithm. The proposed adaptive layer freezing does not update the layers for less informative batches to reduce computational cost with a negligible loss of accuracy. The proposed memory retrieval balances the training usage count of samples in episodic memory with a negligible computational and memory cost. In extensive empirical validations using CIFAR-10/100, CLEAR-10, and ImageNet-1K datasets, we demonstrate that the proposed method outperforms the state-of-the-art in the same total budget.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3klVRLhK7w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Yonsei University (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.0,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3wGi5m2YHY",
      "title": "FlowHash: Accelerating Audio Search with Balanced Hashing via Normalizing Flow",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Nearest neighbor search on context representation vectors is a formidable task due to challenges posed by high dimensionality, scalability issues, and potential noise within query vectors. Our novel approach leverages normalizing flow within a self-supervised learning framework to effectively tackle these challenges, specifically in the context of audio fingerprinting tasks. Audio fingerprinting systems incorporate two key components: audio encoding and indexing. The existing systems consider these components independently, resulting in suboptimal performance. Our approach optimizes the interplay between these components, facilitating the adaptation of vectors to the indexing structure. Additionally, we distribute vectors in the latent $\\mathbb{R}^K$ space using normalizing flow, resulting in balanced $K$-bit hash codes. This allows indexing vectors using a balanced hash table, where vectors are uniformly distributed across all possible $2^K$ hash buckets. This significantly accelerates retrieval, achieving speedups of up to 3$\\times$ compared to the Locality-Sensitive Hashing (LSH). We empirically demonstrate that our system is scalable, highly effective, and efficient in identifying short audio queries ($\\leq$2s), particularly at high noise and reverberation levels.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=3wGi5m2YHY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vipul_Arora1",
        "name": "Vipul Arora",
        "name_site": null,
        "openreview_id": "~Vipul_Arora1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~vipular",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=SC9YYPAAAAAJ",
        "orcid": "0000-0002-1207-1258",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (Belgium)",
        "countries": [
          "Belgium"
        ],
        "country_codes": [
          "BE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "4WZNdnwmhk",
      "title": "Parameter-Efficient Fine-Tuning via Partially Decomposable Loss Analysis and Sharing",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language model (LLM) has become a crucial tool for many machine learning research and applications. Due to the large parameter count of these models and the enormous amount of training data, large language models are usually strong at general tasks. For most applications however, one would like a smaller, more parameter-efficient model that is specialized in a particular field. This motivates the design of fine-tuning, which tunes a pre-trained LLM for a few iterations on a dedicated dataset for specific tasks. If not handled correctly, the fine-tuning process would create another LLM that has comparable amount of parameters, significantly slowers downstream applications.\n\nOne of the most widely-known ideas for resolving this issue is the Low-Rank Adaptation (LoRA) framework, where one assumes the fine-tuning weights are low-rank therefore the number of parameters together with the inference time is drastically improved. While performing well in practice, LoRA method is still a heuristic and lacks theoretical guarantees even though the loss function might inherit certain structures. Moreover, when fine-tuning multiple similar tasks in parallel, LoRA requires one to learn a pair of distinct low-rank matrices for each task, ignoring possible shared structure between tasks.\n\nIn this work, we design a framework that further reduces parameter count compared to LoRA and enables parameter sharing across different parallel fine-tuning tasks. When the number of parallel fine-tuning tasks grows larger, we cut the parameter count almost in half compared to LoRA. Moreover, we prove why our approach --- or more generally, LoRA works for a large class of loss functions. We empirically verify the effectiveness of our method on various benchmark models and datasets, demonstrating much improved parameter count while retaining similar performance as LoRA.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=4WZNdnwmhk",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Raghavendra_Addanki1",
        "name": "Raghavendra Addanki",
        "name_site": null,
        "openreview_id": "~Raghavendra_Addanki1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://raddanki.github.io/",
        "dblp_id": "218/5579",
        "google_scholar_url": "SUPaOhgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5rrYpa2vts",
      "title": "EA2N: Evidence-based AMR Attention Network for Fake News Detection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Proliferation of fake news has become a critical issue in today's information-driven society. Our study includes external knowledge from Wikidata and deviates from the reliance on social information to detect fake news, that many  state-of-the-art (SOTA) fact-checking models adopt. This paper introduces EA$^2$N, an Evidence-based AMR Attention Network for Fake News Detection. EA$^2$N leverages Abstract Meaning Representation (AMR) and incorporates knowledge from Wikidata using proposed evidence linking algorithm, pushing the boundaries of fake news detection. The proposed framework encompasses a combination of novel language encoder and graph encoder to detect the fake news. While the language encoder effectively combines transformer encoded textual features with affective lexical features, the graph encoder encodes AMR with evidence through external knowledge, referred as WikiAMR graph. A path-aware graph learning module is designed to capture crucial semantic relationships among entities over evidences. Extensive experiments supports our model's superior performance, surpassing SOTA methodologies. This research not only advances the field of Fake News Detection but also showcases the potential of AMR and external knowledge for robust NLP applications, promising a more trustworthy information landscape.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5rrYpa2vts",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubham_Gupta5",
        "name": "Shubham Gupta",
        "name_site": null,
        "openreview_id": "~Shubham_Gupta5",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.csa-iitj.group/author/shubham-gupta/",
        "dblp_id": null,
        "google_scholar_url": "z1lEsUgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5rrYpa2vts",
      "title": "EA2N: Evidence-based AMR Attention Network for Fake News Detection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Proliferation of fake news has become a critical issue in today's information-driven society. Our study includes external knowledge from Wikidata and deviates from the reliance on social information to detect fake news, that many  state-of-the-art (SOTA) fact-checking models adopt. This paper introduces EA$^2$N, an Evidence-based AMR Attention Network for Fake News Detection. EA$^2$N leverages Abstract Meaning Representation (AMR) and incorporates knowledge from Wikidata using proposed evidence linking algorithm, pushing the boundaries of fake news detection. The proposed framework encompasses a combination of novel language encoder and graph encoder to detect the fake news. While the language encoder effectively combines transformer encoded textual features with affective lexical features, the graph encoder encodes AMR with evidence through external knowledge, referred as WikiAMR graph. A path-aware graph learning module is designed to capture crucial semantic relationships among entities over evidences. Extensive experiments supports our model's superior performance, surpassing SOTA methodologies. This research not only advances the field of Fake News Detection but also showcases the potential of AMR and external knowledge for robust NLP applications, promising a more trustworthy information landscape.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5rrYpa2vts",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhishek_Rajora1",
        "name": "Abhishek Rajora",
        "name_site": null,
        "openreview_id": "~Abhishek_Rajora1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://brillard.netlify.app/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "abhishek-rajora/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5rrYpa2vts",
      "title": "EA2N: Evidence-based AMR Attention Network for Fake News Detection",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Proliferation of fake news has become a critical issue in today's information-driven society. Our study includes external knowledge from Wikidata and deviates from the reliance on social information to detect fake news, that many  state-of-the-art (SOTA) fact-checking models adopt. This paper introduces EA$^2$N, an Evidence-based AMR Attention Network for Fake News Detection. EA$^2$N leverages Abstract Meaning Representation (AMR) and incorporates knowledge from Wikidata using proposed evidence linking algorithm, pushing the boundaries of fake news detection. The proposed framework encompasses a combination of novel language encoder and graph encoder to detect the fake news. While the language encoder effectively combines transformer encoded textual features with affective lexical features, the graph encoder encodes AMR with evidence through external knowledge, referred as WikiAMR graph. A path-aware graph learning module is designed to capture crucial semantic relationships among entities over evidences. Extensive experiments supports our model's superior performance, surpassing SOTA methodologies. This research not only advances the field of Fake News Detection but also showcases the potential of AMR and external knowledge for robust NLP applications, promising a more trustworthy information landscape.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5rrYpa2vts",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suman_Kundu1",
        "name": "Suman Kundu",
        "name_site": null,
        "openreview_id": "~Suman_Kundu1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sumankundu.info",
        "dblp_id": "45/9812",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=2h2t9cEAAAAJ",
        "orcid": "0000-0002-7856-4768",
        "linkedin_url": "drskundu/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5vJe8XKFv0",
      "title": "CoNO: Complex Neural Operator for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical evaluation of CoNO on several datasets and additional tasks such as zero-shot super-resolution, evaluation of out-of-distribution data, data efficiency, and robustness to noise. CoNO exhibits comparable or superior performance to all the state-of-the-art models in these tasks. Altogether, CoNO presents a robust and superior model for modeling continuous dynamical systems, providing a fillip to scientific machine learning. Our code implementation is available at https://anonymous.4open.science/r/anonymous-cono.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5vJe8XKFv0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karn_Tiwari1",
        "name": "Karn Tiwari",
        "name_site": null,
        "openreview_id": "~Karn_Tiwari1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "karn3003/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5vJe8XKFv0",
      "title": "CoNO: Complex Neural Operator for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical evaluation of CoNO on several datasets and additional tasks such as zero-shot super-resolution, evaluation of out-of-distribution data, data efficiency, and robustness to noise. CoNO exhibits comparable or superior performance to all the state-of-the-art models in these tasks. Altogether, CoNO presents a robust and superior model for modeling continuous dynamical systems, providing a fillip to scientific machine learning. Our code implementation is available at https://anonymous.4open.science/r/anonymous-cono.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5vJe8XKFv0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5vJe8XKFv0",
      "title": "CoNO: Complex Neural Operator for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical evaluation of CoNO on several datasets and additional tasks such as zero-shot super-resolution, evaluation of out-of-distribution data, data efficiency, and robustness to noise. CoNO exhibits comparable or superior performance to all the state-of-the-art models in these tasks. Altogether, CoNO presents a robust and superior model for modeling continuous dynamical systems, providing a fillip to scientific machine learning. Our code implementation is available at https://anonymous.4open.science/r/anonymous-cono.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5vJe8XKFv0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5zNJQV60Wm",
      "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "How can we perform  computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for  addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a  language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We further find the generated programs are often interpretable and enable post-hoc verification of the intermediate reasoning steps.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=5zNJQV60Wm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 8,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 2.0463381929681126,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6LyO8WTVTU",
      "title": "A Teacher-Guided Framework for Graph Representation Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We consider the problem of unsupervised representation learning for Graph Neural Networks (GNNs). \nSeveral state-of-the-art approaches to this problem are based on Contrastive Learning (CL) principles that generate transferable representations. \nTheir objective function can be posed as a supervised discriminative task using 'hard labels', as they consider each pair of graphs as either 'equally positive' or 'equally negative'.\nHowever, it has been observed that using 'soft labels' in a Bayesian way can reduce the variance of the risk for discriminative tasks in supervised settings. \nMotivated by this, we propose a CL framework for GNNs, called *Teacher-guided Graph Contrastive Learning (TGCL)*, that incorporates `soft labels' to facilitate a more regularized discrimination. \nIn particular, we propose a teacher-student framework where the student network learns the representation by distilling the representations produced by the teacher network trained using unlabelled graphs. \nOur proposed approach can be adapted to any existing CL methods and empirically improves the performance across diverse downstream tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6LyO8WTVTU",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal2",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal2",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "MZ8N49AAAAAJ",
        "orcid": "0000-0001-7297-374X",
        "linkedin_url": "arnab-mondal-a4448a18/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu Research and Development Center (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6LyO8WTVTU",
      "title": "A Teacher-Guided Framework for Graph Representation Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We consider the problem of unsupervised representation learning for Graph Neural Networks (GNNs). \nSeveral state-of-the-art approaches to this problem are based on Contrastive Learning (CL) principles that generate transferable representations. \nTheir objective function can be posed as a supervised discriminative task using 'hard labels', as they consider each pair of graphs as either 'equally positive' or 'equally negative'.\nHowever, it has been observed that using 'soft labels' in a Bayesian way can reduce the variance of the risk for discriminative tasks in supervised settings. \nMotivated by this, we propose a CL framework for GNNs, called *Teacher-guided Graph Contrastive Learning (TGCL)*, that incorporates `soft labels' to facilitate a more regularized discrimination. \nIn particular, we propose a teacher-student framework where the student network learns the representation by distilling the representations produced by the teacher network trained using unlabelled graphs. \nOur proposed approach can be adapted to any existing CL methods and empirically improves the performance across diverse downstream tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6LyO8WTVTU",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manohar_Kaul1",
        "name": "Manohar Kaul",
        "name_site": null,
        "openreview_id": "~Manohar_Kaul1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://manukaul.github.io/",
        "dblp_id": "29/10735",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=jNroyK4AAAAJ",
        "orcid": null,
        "linkedin_url": "manu-k-72b936287/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu Research and Development Center (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6LyO8WTVTU",
      "title": "A Teacher-Guided Framework for Graph Representation Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We consider the problem of unsupervised representation learning for Graph Neural Networks (GNNs). \nSeveral state-of-the-art approaches to this problem are based on Contrastive Learning (CL) principles that generate transferable representations. \nTheir objective function can be posed as a supervised discriminative task using 'hard labels', as they consider each pair of graphs as either 'equally positive' or 'equally negative'.\nHowever, it has been observed that using 'soft labels' in a Bayesian way can reduce the variance of the risk for discriminative tasks in supervised settings. \nMotivated by this, we propose a CL framework for GNNs, called *Teacher-guided Graph Contrastive Learning (TGCL)*, that incorporates `soft labels' to facilitate a more regularized discrimination. \nIn particular, we propose a teacher-student framework where the student network learns the representation by distilling the representations produced by the teacher network trained using unlabelled graphs. \nOur proposed approach can be adapted to any existing CL methods and empirically improves the performance across diverse downstream tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6LyO8WTVTU",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6yJuDK1DsK",
      "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6yJuDK1DsK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dhanajit_Brahma1",
        "name": "Dhanajit Brahma",
        "name_site": null,
        "openreview_id": "~Dhanajit_Brahma1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/dhanajit/",
        "dblp_id": "235/5479",
        "google_scholar_url": "3PfwL2IAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6yJuDK1DsK",
      "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6yJuDK1DsK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhinav_Joshi1",
        "name": "Abhinav Joshi",
        "name_site": null,
        "openreview_id": "~Abhinav_Joshi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitk.ac.in/users/ajoshi/",
        "dblp_id": "308/0603",
        "google_scholar_url": null,
        "orcid": "0000-0001-6756-1126",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6yJuDK1DsK",
      "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6yJuDK1DsK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashutosh_Modi1",
        "name": "Ashutosh Modi",
        "name_site": null,
        "openreview_id": "~Ashutosh_Modi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://ashutosh-modi.github.io/",
        "dblp_id": "139/0873",
        "google_scholar_url": "AWu6f60AAAAJ",
        "orcid": null,
        "linkedin_url": "dr-ashutosh-modi-3907835/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6yJuDK1DsK",
      "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=6yJuDK1DsK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Rai1",
        "name": "Piyush Rai",
        "name_site": null,
        "openreview_id": "~Piyush_Rai1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cse.iitk.ac.in/users/piyush/",
        "dblp_id": "02/525",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=D50grEgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7HdtLgsvys",
      "title": "Tube Loss: A Novel Approach for High Quality Prediction Interval Estimation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "This paper proposes a continuous loss function termed 'tube loss' for  Prediction Interval (PI) estimation. The minimizer of the proposed tube loss is a pair of functions  $\\mu_1(x)$ and $\\mu_2(x)$ such that the interval $[\\mu_1(x),\\mu_2(x)]$ contains $t$  fraction of $y_i$ values. The tube loss function also facilitates an upward or downward movement of the  PI tube so that the estimated PI may cover the densest regions of response values, thus allowing the sharpening of the width of PI, especially when the distribution of the response is skewed. The tube loss function-based machine learning models also have the privilege of trading off the calibration error and the width of PI by solving a single optimization problem. We have illustrated the use of tube loss functions in kernel machines, neural networks, and sequential deep learning models. Our numerical experiments show that the tube loss function is effective in yielding narrow and more accurate PIs compared to the existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=7HdtLgsvys",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pritam_Anand1",
        "name": "Pritam Anand",
        "name_site": null,
        "openreview_id": "~Pritam_Anand1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=ATYzQhoAAAAJ&hl=en",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Dhirubhai Ambani Institute of Information and Communication Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.25,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7HdtLgsvys",
      "title": "Tube Loss: A Novel Approach for High Quality Prediction Interval Estimation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "This paper proposes a continuous loss function termed 'tube loss' for  Prediction Interval (PI) estimation. The minimizer of the proposed tube loss is a pair of functions  $\\mu_1(x)$ and $\\mu_2(x)$ such that the interval $[\\mu_1(x),\\mu_2(x)]$ contains $t$  fraction of $y_i$ values. The tube loss function also facilitates an upward or downward movement of the  PI tube so that the estimated PI may cover the densest regions of response values, thus allowing the sharpening of the width of PI, especially when the distribution of the response is skewed. The tube loss function-based machine learning models also have the privilege of trading off the calibration error and the width of PI by solving a single optimization problem. We have illustrated the use of tube loss functions in kernel machines, neural networks, and sequential deep learning models. Our numerical experiments show that the tube loss function is effective in yielding narrow and more accurate PIs compared to the existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=7HdtLgsvys",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tathagata_Bandyopadhyay1",
        "name": "Tathagata Bandyopadhyay",
        "name_site": null,
        "openreview_id": "~Tathagata_Bandyopadhyay1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=NZjB-lUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Dhirubhai Ambani Institute of Information and Communication Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.25,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7m5jhNXklB",
      "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with explainability, fairness, and robustness being some of the key trustworthiness metrics. Data-Centric AI (DCAI) aims to construct high-quality datasets for efficient training of trustworthy models. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation, where the $\\textit{features}$ for each training datapoint is obtained in an online manner through an iterative training algorithm. We propose a novel online version of the OMP algorithm for solving this problem. We also derive conditions on the data matrix, that guarantee the exact recovery of the sparse solution. We demonstrate the generality and effectiveness of our approach by designing data-driven value functions for the above trustworthiness metrics. Experimental results show that VTruST outperforms the state-of-the-art baselines for fair learning as well as robust training, on standard fair and robust datasets. We also demonstrate that VTruST can provide effective tradeoffs between different trustworthiness metrics through pareto optimal fronts. Finally, we show that the data valuation generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=7m5jhNXklB",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubhadip_Nag1",
        "name": "Shubhadip Nag",
        "name_site": null,
        "openreview_id": "~Shubhadip_Nag1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://nagshubhadip.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "shubhadip-nag",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7m5jhNXklB",
      "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with explainability, fairness, and robustness being some of the key trustworthiness metrics. Data-Centric AI (DCAI) aims to construct high-quality datasets for efficient training of trustworthy models. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation, where the $\\textit{features}$ for each training datapoint is obtained in an online manner through an iterative training algorithm. We propose a novel online version of the OMP algorithm for solving this problem. We also derive conditions on the data matrix, that guarantee the exact recovery of the sparse solution. We demonstrate the generality and effectiveness of our approach by designing data-driven value functions for the above trustworthiness metrics. Experimental results show that VTruST outperforms the state-of-the-art baselines for fair learning as well as robust training, on standard fair and robust datasets. We also demonstrate that VTruST can provide effective tradeoffs between different trustworthiness metrics through pareto optimal fronts. Finally, we show that the data valuation generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=7m5jhNXklB",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suparna_Bhattacharya1",
        "name": "Suparna Bhattacharya",
        "name_site": null,
        "openreview_id": "~Suparna_Bhattacharya1",
        "position": 4,
        "gender": "F",
        "homepage_url": "https://www.hpe.com/psnow/doc/a00130899enw",
        "dblp_id": "10/2594",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=ubLpnh4AAAAJ",
        "orcid": "0000-0001-9541-4027",
        "linkedin_url": "suparna-bhattacharya-5a7798b/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7m5jhNXklB",
      "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with explainability, fairness, and robustness being some of the key trustworthiness metrics. Data-Centric AI (DCAI) aims to construct high-quality datasets for efficient training of trustworthy models. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation, where the $\\textit{features}$ for each training datapoint is obtained in an online manner through an iterative training algorithm. We propose a novel online version of the OMP algorithm for solving this problem. We also derive conditions on the data matrix, that guarantee the exact recovery of the sparse solution. We demonstrate the generality and effectiveness of our approach by designing data-driven value functions for the above trustworthiness metrics. Experimental results show that VTruST outperforms the state-of-the-art baselines for fair learning as well as robust training, on standard fair and robust datasets. We also demonstrate that VTruST can provide effective tradeoffs between different trustworthiness metrics through pareto optimal fronts. Finally, we show that the data valuation generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=7m5jhNXklB",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourangshu_Bhattacharya1",
        "name": "Sourangshu Bhattacharya",
        "name_site": null,
        "openreview_id": "~Sourangshu_Bhattacharya1",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~sourangshu/",
        "dblp_id": "http://dblp.uni-trier.de/pers/hd/b/Bhattacharya:Sourangshu",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=IixRsP0AAAAJ",
        "orcid": null,
        "linkedin_url": "sourangshubhattacharya",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8giiPtg6rw",
      "title": "DataFreeShield: Defending Adversarial Attacks without Training Data",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent advances in adversarial robustness rely on an abundant set of training data, where using external or additional datasets has become a common setting.\nHowever, due to security and privacy issues, it is more common that a pretrained model is available while the dataset is not.\nIn such a scenario, existing methods that assume accessibility to the original data become inapplicable.\nFor the first time, we propose a problem of learning *data-free adversarial robustness*, where given only a pretrained model, adversarial robustness should be achieved without accessing the training dataset.\nIn our preliminary study, we identify that robustness without the original dataset is difficult to achieve, even with similar domain datasets.\nWe tackle the task from two perspectives: surrogate dataset generation and adversarial training using the generated data.\nFor dataset generation, we propose  diversified sample synthesis, which largely enhances the diversity of synthetic samples that are known to have low coverage. \nFor training, we propose a soft label loss that best learns robustness from noisy synthetic samples and a gradient refinement method toward smoother loss surface. \nExtensively validating methods using four datasets, we show that the proposed solution outperforms several baselines, demonstrating that the proposed method sets the first solution for the data-free robustness problem.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=8giiPtg6rw",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9vZ8UjP2Mz",
      "title": "Exploring the Generalization Capabilities of AID-based Bi-level Optimization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Bi-level optimization has achieved considerable success in contemporary machine learning applications, especially for given proper hyperparameters. However, due to the two-level optimization structure, commonly, researchers focus on two types of bi-level optimization methods: approximate implicit differentiation (AID)-based and iterative differentiation (ITD)-based approaches. ITD-based methods can be readily transformed into single-level optimization problems, facilitating the study of their generalization capabilities. In contrast, AID-based methods cannot be easily transformed similarly but must stay in the two-level structure, leaving their generalization properties enigmatic. In this paper, although the outer-level function is nonconvex, we ascertain the uniform stability of AID-based methods, which achieves similar results to a single-level nonconvex problem. We conduct a convergence analysis for a carefully chosen step size to maintain stability. Combining the convergence and stability results, we give the generalization ability of AID-based bi-level optimization methods. Furthermore, we carry out an ablation study of the parameters and assess the performance of these methods on real-world tasks. Our experimental results corroborate the theoretical findings, demonstrating the effectiveness and potential applications of these methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=9vZ8UjP2Mz",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 2.1213203435596424,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ABIcBDLBVG",
      "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? \n\nIn this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ABIcBDLBVG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniruddha_Deb1",
        "name": "Aniruddha Deb",
        "name_site": null,
        "openreview_id": "~Aniruddha_Deb1",
        "position": 1,
        "gender": "Not Specified",
        "homepage_url": "https://www.aniruddhadeb.com",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ABIcBDLBVG",
      "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? \n\nIn this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ABIcBDLBVG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Neeva_Hareshbhai_Oza1",
        "name": "Neeva Hareshbhai Oza",
        "name_site": null,
        "openreview_id": "~Neeva_Hareshbhai_Oza1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "358/6181",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "learner4ever/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ABIcBDLBVG",
      "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? \n\nIn this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ABIcBDLBVG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sarthak_Singla1",
        "name": "Sarthak Singla",
        "name_site": null,
        "openreview_id": "~Sarthak_Singla1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "sarthak-singla/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ABIcBDLBVG",
      "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? \n\nIn this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ABIcBDLBVG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dinesh_Khandelwal2",
        "name": "Dinesh Khandelwal",
        "name_site": null,
        "openreview_id": "~Dinesh_Khandelwal2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://research.ibm.com/people/dinesh-khandelwal",
        "dblp_id": "177/0164",
        "google_scholar_url": "Pi-SqXwAAAAJ",
        "orcid": null,
        "linkedin_url": "dinesh-khandelwal-68689420/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ABIcBDLBVG",
      "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? \n\nIn this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ABIcBDLBVG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.8027756377319946,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sanket_Sanjaykumar_Gandhi1",
        "name": "Sanket Sanjaykumar Gandhi",
        "name_site": null,
        "openreview_id": "~Sanket_Sanjaykumar_Gandhi1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/sanky29",
        "dblp_id": "382/4942",
        "google_scholar_url": "https://scholar.google.de/citations?hl=en",
        "orcid": null,
        "linkedin_url": "sanketgandhi29/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vishal_Sharma1",
        "name": "Vishal Sharma",
        "name_site": null,
        "openreview_id": "~Vishal_Sharma1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~vsharma/",
        "dblp_id": null,
        "google_scholar_url": "HBxIco0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rushil_Gupta1",
        "name": "Rushil Gupta",
        "name_site": null,
        "openreview_id": "~Rushil_Gupta1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "EjrTb2wAAAAJ",
        "orcid": "0009-0006-1402-0426",
        "linkedin_url": "rusgupta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Université de Montréal (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnab_Kumar_Mondal2",
        "name": "Arnab Kumar Mondal",
        "name_site": null,
        "openreview_id": "~Arnab_Kumar_Mondal2",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "MZ8N49AAAAAJ",
        "orcid": "0000-0001-7297-374X",
        "linkedin_url": "arnab-mondal-a4448a18/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu Research and Development Center (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "mahajansamanyu@gmail.com",
        "name": "Samanyu Mahajan",
        "name_site": null,
        "openreview_id": "mahajansamanyu@gmail.com",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "AOSsLRKQrX",
      "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don’t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=AOSsLRKQrX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "B5CgCJY2po",
      "title": "Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more efficient in terms of message complexity. We study the proposed model and provide both empirical evidence and theoretical insights in terms of its expressiveness, efficiency, information exchange and ability to extrapolate.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=B5CgCJY2po",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Florian_Grötschla1",
        "name": "Florian Grötschla",
        "name_site": null,
        "openreview_id": "~Florian_Grötschla1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://disco.ethz.ch/members/fgroetschla",
        "dblp_id": "334/1811",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 2.0463381929681126,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BkRD6GsswM",
      "title": "CLA-RA: COLLABORATIVE ACTIVE LEARNING AMIDST RELABELING AMBIGUITY",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Obtaining diverse and high-quality labeled data for training efficient classifiers remains a practical challenge. Crowdsourcing, which involves employing multiple weak labelers, is a popular approach to address this issue. However, crowd labelers often introduce noise, inaccuracies, and possess limited domain knowledge. In this paper, we propose a novel framework CLA-RA to optimize the labeling process by determining what to label next and assigning tasks to the most suitable annotators. Our technique aims to optimize classifier efficiency by utilizing the collective wisdom of various annotators while limiting the influence of error-prone annotations. The key contributions of our work include an annotator disagreement based instance selection mechanism which identifies the noise present in annotations of the instances and an instance-dependent annotator confidence model, which identifies the annotator with the highest confidence to correctly label an instance.These methods, combined with a similarity based annotator inference method, result in improved classifier accuracy while reducing annotation efforts. Experimental results over 13 datasets demonstrate significant improvements over state-of-the-art multi-annotator active learning methods, highlighting the effectiveness of our approach in obtaining high-quality labeled data for training classifiers with minimal labeling costs and errors.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=BkRD6GsswM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kaizer_Rahaman1",
        "name": "Kaizer Rahaman",
        "name_site": null,
        "openreview_id": "~Kaizer_Rahaman1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "kaizer-rahaman?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B8s3DpZcVTLa%2FmQ1NWORLVg%3D%3D",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DluJpvRF69",
      "title": "StyleCL : Latent Dictionary Learning for StyleGAN Without Forgetting",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "StyleGAN is one of the most versatile generative models that have emerged in recent times. However, when it is trained continually on a stream of data (potentially previously unseen distributions), it tends to forget the distribution it has learned, as is the case with any other generative model, due to catastrophic forgetting. Recent studies have shown that the latent space of StyleGAN is very versatile, as data from a variety of distributions can be inverted onto it. In this paper, we propose to leverage this property to facilitate lifelong learning of StyleGAN without forgetting. Specifically, given a StyleGAN trained on a certain task (dataset), we propose to learn a set of dictionary vectors in its latent space, one for each novel, unseen task (or dataset). Additionally, we also learn a relatively small set of shared parameters (feature adaptors) in the weight space to complement the dictionary learning in the latent space. During inference, given a dataset/task, our method invokes the corresponding learned latent dictionary and the shared parameters for that particular task. Our method avoids catastrophic forgetting because the set of dictionary and the feature adaptor parameters are unique for each task. However, the generator for each task shares all of the parameters except for the newly added parameters of the feature adaptor. We demonstrate that our method, StyleCL, achieves better generation quality on multiple datasets. Additionally, our method requires significantly fewer additional parameters per task compared to previous methods. This is a consequence of learning task-specific dictionaries in the latent space, which has a much lower dimensionality compared to the weight space. We also demonstrate that our method, StyleCL, offers the capability for positive forward transfer when the tasks are semantically similar.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=DluJpvRF69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Adarsh_Kappiyath1",
        "name": "Adarsh Kappiyath",
        "name_site": null,
        "openreview_id": "~Adarsh_Kappiyath1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DluJpvRF69",
      "title": "StyleCL : Latent Dictionary Learning for StyleGAN Without Forgetting",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "StyleGAN is one of the most versatile generative models that have emerged in recent times. However, when it is trained continually on a stream of data (potentially previously unseen distributions), it tends to forget the distribution it has learned, as is the case with any other generative model, due to catastrophic forgetting. Recent studies have shown that the latent space of StyleGAN is very versatile, as data from a variety of distributions can be inverted onto it. In this paper, we propose to leverage this property to facilitate lifelong learning of StyleGAN without forgetting. Specifically, given a StyleGAN trained on a certain task (dataset), we propose to learn a set of dictionary vectors in its latent space, one for each novel, unseen task (or dataset). Additionally, we also learn a relatively small set of shared parameters (feature adaptors) in the weight space to complement the dictionary learning in the latent space. During inference, given a dataset/task, our method invokes the corresponding learned latent dictionary and the shared parameters for that particular task. Our method avoids catastrophic forgetting because the set of dictionary and the feature adaptor parameters are unique for each task. However, the generator for each task shares all of the parameters except for the newly added parameters of the feature adaptor. We demonstrate that our method, StyleCL, achieves better generation quality on multiple datasets. Additionally, our method requires significantly fewer additional parameters per task compared to previous methods. This is a consequence of learning task-specific dictionaries in the latent space, which has a much lower dimensionality compared to the weight space. We also demonstrate that our method, StyleCL, offers the capability for positive forward transfer when the tasks are semantically similar.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=DluJpvRF69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~ANMOL_GARG1",
        "name": "ANMOL GARG",
        "name_site": null,
        "openreview_id": "~ANMOL_GARG1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "anmol-garg-91a5b3155/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DluJpvRF69",
      "title": "StyleCL : Latent Dictionary Learning for StyleGAN Without Forgetting",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "StyleGAN is one of the most versatile generative models that have emerged in recent times. However, when it is trained continually on a stream of data (potentially previously unseen distributions), it tends to forget the distribution it has learned, as is the case with any other generative model, due to catastrophic forgetting. Recent studies have shown that the latent space of StyleGAN is very versatile, as data from a variety of distributions can be inverted onto it. In this paper, we propose to leverage this property to facilitate lifelong learning of StyleGAN without forgetting. Specifically, given a StyleGAN trained on a certain task (dataset), we propose to learn a set of dictionary vectors in its latent space, one for each novel, unseen task (or dataset). Additionally, we also learn a relatively small set of shared parameters (feature adaptors) in the weight space to complement the dictionary learning in the latent space. During inference, given a dataset/task, our method invokes the corresponding learned latent dictionary and the shared parameters for that particular task. Our method avoids catastrophic forgetting because the set of dictionary and the feature adaptor parameters are unique for each task. However, the generator for each task shares all of the parameters except for the newly added parameters of the feature adaptor. We demonstrate that our method, StyleCL, achieves better generation quality on multiple datasets. Additionally, our method requires significantly fewer additional parameters per task compared to previous methods. This is a consequence of learning task-specific dictionaries in the latent space, which has a much lower dimensionality compared to the weight space. We also demonstrate that our method, StyleCL, offers the capability for positive forward transfer when the tasks are semantically similar.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=DluJpvRF69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ramya_Hebbalaguppe2",
        "name": "Ramya Hebbalaguppe",
        "name_site": null,
        "openreview_id": "~Ramya_Hebbalaguppe2",
        "position": 3,
        "gender": "F",
        "homepage_url": "https://rhebbalaguppe.github.io/",
        "dblp_id": "145/2287",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0006-1186-6311",
        "linkedin_url": "https://in.linkedin.com/in/ramya-hebbalaguppe-620b272",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DluJpvRF69",
      "title": "StyleCL : Latent Dictionary Learning for StyleGAN Without Forgetting",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "StyleGAN is one of the most versatile generative models that have emerged in recent times. However, when it is trained continually on a stream of data (potentially previously unseen distributions), it tends to forget the distribution it has learned, as is the case with any other generative model, due to catastrophic forgetting. Recent studies have shown that the latent space of StyleGAN is very versatile, as data from a variety of distributions can be inverted onto it. In this paper, we propose to leverage this property to facilitate lifelong learning of StyleGAN without forgetting. Specifically, given a StyleGAN trained on a certain task (dataset), we propose to learn a set of dictionary vectors in its latent space, one for each novel, unseen task (or dataset). Additionally, we also learn a relatively small set of shared parameters (feature adaptors) in the weight space to complement the dictionary learning in the latent space. During inference, given a dataset/task, our method invokes the corresponding learned latent dictionary and the shared parameters for that particular task. Our method avoids catastrophic forgetting because the set of dictionary and the feature adaptor parameters are unique for each task. However, the generator for each task shares all of the parameters except for the newly added parameters of the feature adaptor. We demonstrate that our method, StyleCL, achieves better generation quality on multiple datasets. Additionally, our method requires significantly fewer additional parameters per task compared to previous methods. This is a consequence of learning task-specific dictionaries in the latent space, which has a much lower dimensionality compared to the weight space. We also demonstrate that our method, StyleCL, offers the capability for positive forward transfer when the tasks are semantically similar.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=DluJpvRF69",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4flIscNE6",
      "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Knowledge distillation (KD) approximates a large teacher model using a smaller student model. KD can be used to train multiple students of different capacities, allowing for flexible management of inference costs at test time. We propose a novel distillation method we term meta-collaboration, wherein a set of students are simultaneously distilled from a single teacher and can improve each other through information sharing during distillation. We model this information sharing through a separate network designed to predict instance-specific loss mixing for each of the students. This auxiliary network is trained jointly with the multi-student distillation, utilizing a separate meta-loss aggregating student model loss on a separate validation set. Our method improves student accuracy for all students and beats to state-of-the-art distillation baselines, including methods that use multi-step distillation, combining models of different sizes. In particular, addition of smaller students to the pool clearly benefits larger student models, through the mechanism of meta-collaboration. We show average gains of 2.5\\% on CIFAR100 \\& 2\\% on TinyImageNet datasets; our gains are consistent across a wide range of student sizes, teacher sizes, and model architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4flIscNE6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Durga_S1",
        "name": "Durga S",
        "name_site": null,
        "openreview_id": "~Durga_S1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "4JXFWTwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4flIscNE6",
      "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Knowledge distillation (KD) approximates a large teacher model using a smaller student model. KD can be used to train multiple students of different capacities, allowing for flexible management of inference costs at test time. We propose a novel distillation method we term meta-collaboration, wherein a set of students are simultaneously distilled from a single teacher and can improve each other through information sharing during distillation. We model this information sharing through a separate network designed to predict instance-specific loss mixing for each of the students. This auxiliary network is trained jointly with the multi-student distillation, utilizing a separate meta-loss aggregating student model loss on a separate validation set. Our method improves student accuracy for all students and beats to state-of-the-art distillation baselines, including methods that use multi-step distillation, combining models of different sizes. In particular, addition of smaller students to the pool clearly benefits larger student models, through the mechanism of meta-collaboration. We show average gains of 2.5\\% on CIFAR100 \\& 2\\% on TinyImageNet datasets; our gains are consistent across a wide range of student sizes, teacher sizes, and model architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4flIscNE6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shashank_Kate1",
        "name": "Shashank Kate",
        "name_site": null,
        "openreview_id": "~Shashank_Kate1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~shashankkate/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4flIscNE6",
      "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Knowledge distillation (KD) approximates a large teacher model using a smaller student model. KD can be used to train multiple students of different capacities, allowing for flexible management of inference costs at test time. We propose a novel distillation method we term meta-collaboration, wherein a set of students are simultaneously distilled from a single teacher and can improve each other through information sharing during distillation. We model this information sharing through a separate network designed to predict instance-specific loss mixing for each of the students. This auxiliary network is trained jointly with the multi-student distillation, utilizing a separate meta-loss aggregating student model loss on a separate validation set. Our method improves student accuracy for all students and beats to state-of-the-art distillation baselines, including methods that use multi-step distillation, combining models of different sizes. In particular, addition of smaller students to the pool clearly benefits larger student models, through the mechanism of meta-collaboration. We show average gains of 2.5\\% on CIFAR100 \\& 2\\% on TinyImageNet datasets; our gains are consistent across a wide range of student sizes, teacher sizes, and model architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4flIscNE6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Atharva_Abhijit_Tambat2",
        "name": "Atharva Abhijit Tambat",
        "name_site": null,
        "openreview_id": "~Atharva_Abhijit_Tambat2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~atharvatambat/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4flIscNE6",
      "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Knowledge distillation (KD) approximates a large teacher model using a smaller student model. KD can be used to train multiple students of different capacities, allowing for flexible management of inference costs at test time. We propose a novel distillation method we term meta-collaboration, wherein a set of students are simultaneously distilled from a single teacher and can improve each other through information sharing during distillation. We model this information sharing through a separate network designed to predict instance-specific loss mixing for each of the students. This auxiliary network is trained jointly with the multi-student distillation, utilizing a separate meta-loss aggregating student model loss on a separate validation set. Our method improves student accuracy for all students and beats to state-of-the-art distillation baselines, including methods that use multi-step distillation, combining models of different sizes. In particular, addition of smaller students to the pool clearly benefits larger student models, through the mechanism of meta-collaboration. We show average gains of 2.5\\% on CIFAR100 \\& 2\\% on TinyImageNet datasets; our gains are consistent across a wide range of student sizes, teacher sizes, and model architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4flIscNE6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ganesh_Ramakrishnan1",
        "name": "Ganesh Ramakrishnan",
        "name_site": null,
        "openreview_id": "~Ganesh_Ramakrishnan1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~ganesh/",
        "dblp_id": "r/GaneshRamakrishnan",
        "google_scholar_url": "https://scholar.google.com/scholar?hl=hi",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "E4flIscNE6",
      "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Knowledge distillation (KD) approximates a large teacher model using a smaller student model. KD can be used to train multiple students of different capacities, allowing for flexible management of inference costs at test time. We propose a novel distillation method we term meta-collaboration, wherein a set of students are simultaneously distilled from a single teacher and can improve each other through information sharing during distillation. We model this information sharing through a separate network designed to predict instance-specific loss mixing for each of the students. This auxiliary network is trained jointly with the multi-student distillation, utilizing a separate meta-loss aggregating student model loss on a separate validation set. Our method improves student accuracy for all students and beats to state-of-the-art distillation baselines, including methods that use multi-step distillation, combining models of different sizes. In particular, addition of smaller students to the pool clearly benefits larger student models, through the mechanism of meta-collaboration. We show average gains of 2.5\\% on CIFAR100 \\& 2\\% on TinyImageNet datasets; our gains are consistent across a wide range of student sizes, teacher sizes, and model architectures.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=E4flIscNE6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pradeep_Shenoy1",
        "name": "Pradeep Shenoy",
        "name_site": null,
        "openreview_id": "~Pradeep_Shenoy1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "12/771",
        "google_scholar_url": "lXbPKmkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EAT7gmyIH2",
      "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The frameworks for explaining the functional space learned by deep neural networks, also known as eXplainable AI (XAI) models, are majorly based on the notion of the locality. Most of the approaches for local model-agnostic explainability employ linear models. Driven by the fact that a linear model is inherently interpretable (linear coefficients being the explanation), they are used to approximate the non-linear function locally. In this paper, we argue that local linear approximation is inapt as the black boxes under investigation are often highly non linear. We present a novel perturbation-based approach for local explainability, called the Distillation Approach for Model-agnostic Explainability (DAME). It separates out the two tasks- local approximation and generating explanation, and successfully attempts generating explanations by operating on high dimensional input space. The DAME framework is a learnable, saliency-based explainability model, which is post-hoc, model-agnostic, and requires only query access to the black box. Extensive evaluations including quantitative, qualitative and subjective measures, presented on diverse object and sound classification tasks, demonstrate that the DAME approach provides improved explanation compared to other XAI methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=EAT7gmyIH2",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debarpan_Bhattacharya1",
        "name": "Debarpan Bhattacharya",
        "name_site": null,
        "openreview_id": "~Debarpan_Bhattacharya1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://debarpanbhatta123.github.io/",
        "dblp_id": "272/4019",
        "google_scholar_url": "cc-xQxIAAAAJ",
        "orcid": null,
        "linkedin_url": "debarpan98",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EAT7gmyIH2",
      "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The frameworks for explaining the functional space learned by deep neural networks, also known as eXplainable AI (XAI) models, are majorly based on the notion of the locality. Most of the approaches for local model-agnostic explainability employ linear models. Driven by the fact that a linear model is inherently interpretable (linear coefficients being the explanation), they are used to approximate the non-linear function locally. In this paper, we argue that local linear approximation is inapt as the black boxes under investigation are often highly non linear. We present a novel perturbation-based approach for local explainability, called the Distillation Approach for Model-agnostic Explainability (DAME). It separates out the two tasks- local approximation and generating explanation, and successfully attempts generating explanations by operating on high dimensional input space. The DAME framework is a learnable, saliency-based explainability model, which is post-hoc, model-agnostic, and requires only query access to the black box. Extensive evaluations including quantitative, qualitative and subjective measures, presented on diverse object and sound classification tasks, demonstrate that the DAME approach provides improved explanation compared to other XAI methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=EAT7gmyIH2",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amir_Hossein_Poorjam1",
        "name": "Amir Hossein Poorjam",
        "name_site": null,
        "openreview_id": "~Amir_Hossein_Poorjam1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "169/3629",
        "google_scholar_url": "https://scholar.google.dk/citations?user=W1pYiAQAAAAJ",
        "orcid": "0000-0002-6882-4618",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "FaunaPhotonics (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EAT7gmyIH2",
      "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The frameworks for explaining the functional space learned by deep neural networks, also known as eXplainable AI (XAI) models, are majorly based on the notion of the locality. Most of the approaches for local model-agnostic explainability employ linear models. Driven by the fact that a linear model is inherently interpretable (linear coefficients being the explanation), they are used to approximate the non-linear function locally. In this paper, we argue that local linear approximation is inapt as the black boxes under investigation are often highly non linear. We present a novel perturbation-based approach for local explainability, called the Distillation Approach for Model-agnostic Explainability (DAME). It separates out the two tasks- local approximation and generating explanation, and successfully attempts generating explanations by operating on high dimensional input space. The DAME framework is a learnable, saliency-based explainability model, which is post-hoc, model-agnostic, and requires only query access to the black box. Extensive evaluations including quantitative, qualitative and subjective measures, presented on diverse object and sound classification tasks, demonstrate that the DAME approach provides improved explanation compared to other XAI methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=EAT7gmyIH2",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sriram_Ganapathy1",
        "name": "Sriram Ganapathy",
        "name_site": null,
        "openreview_id": "~Sriram_Ganapathy1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://leap.ee.iisc.ac.in/sriram/",
        "dblp_id": "23/4298.html",
        "google_scholar_url": "cgpzrtcAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ESq3U7z6FD",
      "title": "EHI: End-to-end learning of Hierarchical Index for Efficient Dense Retrieval",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, \nEHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by  4.2% (nDCG@10) on TREC DL19 benchmarks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ESq3U7z6FD",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anshul_Mittal2",
        "name": "Anshul Mittal",
        "name_site": null,
        "openreview_id": "~Anshul_Mittal2",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://anshulmittal.org",
        "dblp_id": null,
        "google_scholar_url": "8TDNQMQAAAAJ",
        "orcid": "0000-0002-4137-0126",
        "linkedin_url": "anshumitts/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "F5ERvanO6m",
      "title": "Deep Stochastic Mechanics",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schrödinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=F5ERvanO6m",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Elena_Orlova1",
        "name": "Elena Orlova",
        "name_site": null,
        "openreview_id": "~Elena_Orlova1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ShareChat (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "F5ERvanO6m",
      "title": "Deep Stochastic Mechanics",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schrödinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=F5ERvanO6m",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aleksei_Ustimenko1",
        "name": "Aleksei Ustimenko",
        "name_site": "Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova",
        "openreview_id": "~Aleksei_Ustimenko1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "242/3873",
        "google_scholar_url": "OES5pK4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Chicago (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H8tpFITvpo",
      "title": "FedHC: Proximal Correction with Hessian and Cosine Correlation for Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL), a prominent distributed learning approach, involves collaborative\nupdates among participants and individual updates on private data.\nWhile widely-used FL methods, such as FedDC and others, traditionally rely\non first-order optimization techniques like Stochastic Gradient Descent (SGD) to\nachieve convergence, there is a growing interest in leveraging second-order optimization\nmethods to enhance convergence in complex models. However, applying\nthese second-order techniques to FL models often results in convergence challenges.\nTo address these issues, we present an innovative integrated methodology\nknown as FedHC, combining proximal correction with Hessian optimization and\ncosine correlation for FL. FedHC introduces the Hessian optimizer with proximal\ncorrection to accelerate convergence. Additionally, we employ cosine correlation\nto minimize learning discrepancies and bridge the gap between local and global\nmodels. Experimental results and analyses conducted on four datasets demonstrate\nthat FedHC significantly accelerates convergence and outperforms existing\nmethods in various image classification tasks, maintaining robustness in both IID\nand Non-IID client settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H8tpFITvpo",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kushall_Pal_Singh1",
        "name": "Kushall Pal Singh",
        "name_site": null,
        "openreview_id": "~Kushall_Pal_Singh1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://mnit.ac.in/dept_cse/people",
        "dblp_id": null,
        "google_scholar_url": "I5byoh0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Malaviya National Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.6666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H8tpFITvpo",
      "title": "FedHC: Proximal Correction with Hessian and Cosine Correlation for Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL), a prominent distributed learning approach, involves collaborative\nupdates among participants and individual updates on private data.\nWhile widely-used FL methods, such as FedDC and others, traditionally rely\non first-order optimization techniques like Stochastic Gradient Descent (SGD) to\nachieve convergence, there is a growing interest in leveraging second-order optimization\nmethods to enhance convergence in complex models. However, applying\nthese second-order techniques to FL models often results in convergence challenges.\nTo address these issues, we present an innovative integrated methodology\nknown as FedHC, combining proximal correction with Hessian optimization and\ncosine correlation for FL. FedHC introduces the Hessian optimizer with proximal\ncorrection to accelerate convergence. Additionally, we employ cosine correlation\nto minimize learning discrepancies and bridge the gap between local and global\nmodels. Experimental results and analyses conducted on four datasets demonstrate\nthat FedHC significantly accelerates convergence and outperforms existing\nmethods in various image classification tasks, maintaining robustness in both IID\nand Non-IID client settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H8tpFITvpo",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Monu_Verma1",
        "name": "Monu Verma",
        "name_site": null,
        "openreview_id": "~Monu_Verma1",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://visionintelligence.github.io/Monu.html",
        "dblp_id": "https://dblp.uni-trier.de/pers/ht/v/Verma:Monu",
        "google_scholar_url": "d8fP0LIAAAAJ",
        "orcid": "0000-0003-4962-882X",
        "linkedin_url": "monu-verma-0469b6253/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Ropar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.6666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H8tpFITvpo",
      "title": "FedHC: Proximal Correction with Hessian and Cosine Correlation for Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL), a prominent distributed learning approach, involves collaborative\nupdates among participants and individual updates on private data.\nWhile widely-used FL methods, such as FedDC and others, traditionally rely\non first-order optimization techniques like Stochastic Gradient Descent (SGD) to\nachieve convergence, there is a growing interest in leveraging second-order optimization\nmethods to enhance convergence in complex models. However, applying\nthese second-order techniques to FL models often results in convergence challenges.\nTo address these issues, we present an innovative integrated methodology\nknown as FedHC, combining proximal correction with Hessian optimization and\ncosine correlation for FL. FedHC introduces the Hessian optimizer with proximal\ncorrection to accelerate convergence. Additionally, we employ cosine correlation\nto minimize learning discrepancies and bridge the gap between local and global\nmodels. Experimental results and analyses conducted on four datasets demonstrate\nthat FedHC significantly accelerates convergence and outperforms existing\nmethods in various image classification tasks, maintaining robustness in both IID\nand Non-IID client settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H8tpFITvpo",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subrahmanyam_Murala2",
        "name": "Subrahmanyam Murala",
        "name_site": null,
        "openreview_id": "~Subrahmanyam_Murala2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.scss.tcd.ie/~muralas/",
        "dblp_id": "61/10849",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=5lGnNQoAAAAJ",
        "orcid": "0000-0003-3384-4368",
        "linkedin_url": "subrahmanyam-murala-b5114716/?originalSubdomain=ie",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Ropar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.6666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H8tpFITvpo",
      "title": "FedHC: Proximal Correction with Hessian and Cosine Correlation for Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL), a prominent distributed learning approach, involves collaborative\nupdates among participants and individual updates on private data.\nWhile widely-used FL methods, such as FedDC and others, traditionally rely\non first-order optimization techniques like Stochastic Gradient Descent (SGD) to\nachieve convergence, there is a growing interest in leveraging second-order optimization\nmethods to enhance convergence in complex models. However, applying\nthese second-order techniques to FL models often results in convergence challenges.\nTo address these issues, we present an innovative integrated methodology\nknown as FedHC, combining proximal correction with Hessian optimization and\ncosine correlation for FL. FedHC introduces the Hessian optimizer with proximal\ncorrection to accelerate convergence. Additionally, we employ cosine correlation\nto minimize learning discrepancies and bridge the gap between local and global\nmodels. Experimental results and analyses conducted on four datasets demonstrate\nthat FedHC significantly accelerates convergence and outperforms existing\nmethods in various image classification tasks, maintaining robustness in both IID\nand Non-IID client settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H8tpFITvpo",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~G_Sankara_Raju_Kosuru1",
        "name": "G Sankara Raju Kosuru",
        "name_site": null,
        "openreview_id": "~G_Sankara_Raju_Kosuru1",
        "position": 5,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=-AK3itMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Malaviya National Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.6666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H8tpFITvpo",
      "title": "FedHC: Proximal Correction with Hessian and Cosine Correlation for Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL), a prominent distributed learning approach, involves collaborative\nupdates among participants and individual updates on private data.\nWhile widely-used FL methods, such as FedDC and others, traditionally rely\non first-order optimization techniques like Stochastic Gradient Descent (SGD) to\nachieve convergence, there is a growing interest in leveraging second-order optimization\nmethods to enhance convergence in complex models. However, applying\nthese second-order techniques to FL models often results in convergence challenges.\nTo address these issues, we present an innovative integrated methodology\nknown as FedHC, combining proximal correction with Hessian optimization and\ncosine correlation for FL. FedHC introduces the Hessian optimizer with proximal\ncorrection to accelerate convergence. Additionally, we employ cosine correlation\nto minimize learning discrepancies and bridge the gap between local and global\nmodels. Experimental results and analyses conducted on four datasets demonstrate\nthat FedHC significantly accelerates convergence and outperforms existing\nmethods in various image classification tasks, maintaining robustness in both IID\nand Non-IID client settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H8tpFITvpo",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dinesh_Kumar_Tyagi1",
        "name": "Dinesh Kumar Tyagi",
        "name_site": null,
        "openreview_id": "~Dinesh_Kumar_Tyagi1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://mnit.ac.in/dept_cse/people",
        "dblp_id": "35/9571",
        "google_scholar_url": null,
        "orcid": "0000-0003-1104-7456",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Miami (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.6666666666666667,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "H9DYMIpz9c",
      "title": "Farzi Data: Autoregressive Data Distillation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=H9DYMIpz9c",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Noveen_Sachdeva2",
        "name": "Noveen Sachdeva",
        "name_site": null,
        "openreview_id": "~Noveen_Sachdeva2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.noveens.com/",
        "dblp_id": "216/7290",
        "google_scholar_url": "i6tMWAoAAAAJ",
        "orcid": null,
        "linkedin_url": "noveensachdeva/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of California, San Diego (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.0954451150103321,
        "confidence_mean": 3.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "HFXpAf88jH",
      "title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The success of SGD in deep learning has been ascribed by prior works to the *implicit bias* induced by high learning rate or small batch size (\"SGD noise\"). While prior works  that focused on *offline learning* (i.e., multiple-epoch training), we study the impact of SGD noise on *online* (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do *not* confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. \nThis suggests that SGD in the online regime can be construed as taking noisy steps along the \"golden path\" of the noiseless *gradient flow* algorithm. We study this hypothesis and provide supporting evidence in function space by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models trained with varying SGD noise levels, but at equivalent loss values. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=HFXpAf88jH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Depen_Morwani1",
        "name": "Depen Morwani",
        "name_site": null,
        "openreview_id": "~Depen_Morwani1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "277/5200",
        "google_scholar_url": "vOngxFUAAAAJ",
        "orcid": null,
        "linkedin_url": "depen-morwani-070298122/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Harvard University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Howb7fXB4V",
      "title": "Pick-or-Mix: Dynamic Channel Sampling for ConvNets",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Channel squeezing is a crucial operation in convolutional neural networks (ConvNets). It is carried out via 1 × 1 convolution layers and dominates a large portion of computations and parameters of a given network. ResNet-50, for instance, consists of 16 such layers, forming 33% of total layers and 25% (1.05B/4.12B) of total FLOPs. In light of their predominance, we present a new multi-purpose module for dynamic channel sampling, namely Pick-or-Mix (PiX). PiX divides a set of channels into subsets and then picks from them, where the picking decision is dynamically made per each pixel based on the input activations. We show that PiX allows ConvNets to learn better data representation than vanilla channel squeezing in far fewer computations. We plug PiX into prominent ConvNet architectures and verify its multi-purpose utilities. After replacing 1 × 1 channel squeezing layers in the ResNet family with PiX, the networks become 25% faster without losing accuracy. We also show that PiX can achieve state-of-the-art performance on network downscaling and dynamic channel pruning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Howb7fXB4V",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar2",
        "name": "Ashish Kumar",
        "name_site": null,
        "openreview_id": "~Ashish_Kumar2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashishkumar822.github.io",
        "dblp_id": "34/5378-6",
        "google_scholar_url": "n-oRDEYAAAAJ",
        "orcid": null,
        "linkedin_url": "ashishkumar822/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ScorelabsAI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Howb7fXB4V",
      "title": "Pick-or-Mix: Dynamic Channel Sampling for ConvNets",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Channel squeezing is a crucial operation in convolutional neural networks (ConvNets). It is carried out via 1 × 1 convolution layers and dominates a large portion of computations and parameters of a given network. ResNet-50, for instance, consists of 16 such layers, forming 33% of total layers and 25% (1.05B/4.12B) of total FLOPs. In light of their predominance, we present a new multi-purpose module for dynamic channel sampling, namely Pick-or-Mix (PiX). PiX divides a set of channels into subsets and then picks from them, where the picking decision is dynamically made per each pixel based on the input activations. We show that PiX allows ConvNets to learn better data representation than vanilla channel squeezing in far fewer computations. We plug PiX into prominent ConvNet architectures and verify its multi-purpose utilities. After replacing 1 × 1 channel squeezing layers in the ResNet family with PiX, the networks become 25% faster without losing accuracy. We also show that PiX can achieve state-of-the-art performance on network downscaling and dynamic channel pruning.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Howb7fXB4V",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laxmidhar_Behera1",
        "name": "Laxmidhar Behera",
        "name_site": null,
        "openreview_id": "~Laxmidhar_Behera1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~lbehera/",
        "dblp_id": "14/1412",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=QWTcyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "laxmidhar-behera-a74a5b174/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (South Korea)",
        "countries": [
          "South Korea"
        ],
        "country_codes": [
          "KR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IHmmnNvU2U",
      "title": "Weighted Risk Invariance for Density-Aware Domain Generalization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Learning how to generalize training performance to unseen test distributions is essential to building robust, practically useful models. To this end, many recent studies focus on learning invariant features from multiple domains. Our first observation is that the performance of existing invariant learning methods can degrade under covariate shift. To address this problem, we focus on finding invariant predictors from multiple, potentially shifted invariant feature distributions. We propose a novel optimization problem, Weighted Risk Invariance (WRI), and we show that the solution to this problem provably achieves out-of-distribution generalization. We also introduce an algorithm to practically solve the WRI problem that learns the density of invariant features and model parameters simultaneously, and we demonstrate our approach outperforms previous invariant learning methods under covariate shift in the invariant features. Finally, we show that the learned density over invariant features effectively detects when the features are out-of-distribution.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=IHmmnNvU2U",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 5,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IPayPEGwdE",
      "title": "Learning Good Interventions in Causal Contextual Bandits with Adaptive Context",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a variant of causal contextual bandits where the context is stochastically dependent on an initial action chosen by the learner. This adaptive context setting allows the environment to elicit some initial choice from the learner before providing the context. Upon observing the context, the learner picks another action (an intervention in a causal graph) based on which they receive a reward. The objective is to identify near-optimal atomic causal interventions at the initial state and post context identification, to maximize reward. We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees. This is achieved through an instance-dependent causal parameter, $\\lambda$, which characterizes our upper bound. Furthermore, we prove that our simple regret is essentially tight for a large class of instances. A key feature of our work is that we use convex optimization to address the bandit exploration problem. We also conduct experiments to validate our theoretical results",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=IPayPEGwdE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rahul_Madhavan1",
        "name": "Rahul Madhavan",
        "name_site": null,
        "openreview_id": "~Rahul_Madhavan1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "290/2008",
        "google_scholar_url": "HrM2xRcAAAAJ",
        "orcid": null,
        "linkedin_url": "rahul-madhavan/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IPayPEGwdE",
      "title": "Learning Good Interventions in Causal Contextual Bandits with Adaptive Context",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We study a variant of causal contextual bandits where the context is stochastically dependent on an initial action chosen by the learner. This adaptive context setting allows the environment to elicit some initial choice from the learner before providing the context. Upon observing the context, the learner picks another action (an intervention in a causal graph) based on which they receive a reward. The objective is to identify near-optimal atomic causal interventions at the initial state and post context identification, to maximize reward. We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees. This is achieved through an instance-dependent causal parameter, $\\lambda$, which characterizes our upper bound. Furthermore, we prove that our simple regret is essentially tight for a large class of instances. A key feature of our work is that we use convex optimization to address the bandit exploration problem. We also conduct experiments to validate our theoretical results",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=IPayPEGwdE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddharth_Barman1",
        "name": "Siddharth Barman",
        "name_site": null,
        "openreview_id": "~Siddharth_Barman1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~barman/",
        "dblp_id": "63/478.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=HcGQSKIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Iv60x1iAvp",
      "title": "GNN-based Reinforcement Learning Agent for Session-based Recommendation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "This paper focuses on session-based item recommendation and the challenges of using Reinforcement Learning (RL) in recommender systems. While traditional RL methods rely on one-hot encoded vectors as user state, they often fail to capture user-specific characteristics, which may provide misleading results. In contrast, recently, Graph Neural Networks (GNNs) have emerged as a promising technique for learning user-item representations effectively. However, GNNs prioritize static rating prediction, which does not fully capture the dynamic nature of session-based recommendations. To address these limitations, we propose a novel approach called GNN-RL-based Recommender System (GRRS), which combines both frameworks to provide a unique solution for the session-based recommendation \\footnote{Code available at \\url{https://anonymous.4open.science/r/iclr24_gnn_rl/}}. We demonstrate that our method can leverage the strengths of both GNNs and RL while overcoming their respective shortcomings. Our experiments on several logged public datasets validate the efficacy of our approach over various SOTA algorithms. Additionally, we offer a solution to the \\emph{offline training problem}, which is often encountered by RL algorithms when employed on logged datasets, which may be of independent interest.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Iv60x1iAvp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohammadi_Zaki1",
        "name": "Mohammadi Zaki",
        "name_site": null,
        "openreview_id": "~Mohammadi_Zaki1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/mohammadizaki52/home",
        "dblp_id": "153/0722",
        "google_scholar_url": "H3ji_pAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sony Research India (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.6,
        "rating_std": 1.4966629547095767,
        "confidence_mean": 3.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Iv60x1iAvp",
      "title": "GNN-based Reinforcement Learning Agent for Session-based Recommendation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "This paper focuses on session-based item recommendation and the challenges of using Reinforcement Learning (RL) in recommender systems. While traditional RL methods rely on one-hot encoded vectors as user state, they often fail to capture user-specific characteristics, which may provide misleading results. In contrast, recently, Graph Neural Networks (GNNs) have emerged as a promising technique for learning user-item representations effectively. However, GNNs prioritize static rating prediction, which does not fully capture the dynamic nature of session-based recommendations. To address these limitations, we propose a novel approach called GNN-RL-based Recommender System (GRRS), which combines both frameworks to provide a unique solution for the session-based recommendation \\footnote{Code available at \\url{https://anonymous.4open.science/r/iclr24_gnn_rl/}}. We demonstrate that our method can leverage the strengths of both GNNs and RL while overcoming their respective shortcomings. Our experiments on several logged public datasets validate the efficacy of our approach over various SOTA algorithms. Additionally, we offer a solution to the \\emph{offline training problem}, which is often encountered by RL algorithms when employed on logged datasets, which may be of independent interest.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Iv60x1iAvp",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Brijraj_Singh1",
        "name": "Brijraj Singh",
        "name_site": null,
        "openreview_id": "~Brijraj_Singh1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sony Research India (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.6,
        "rating_std": 1.4966629547095767,
        "confidence_mean": 3.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J88EKENxyF",
      "title": "CAT-LLM: Context-Aware Training enhanced Large Language Models for multi-modal contextual image retrieval",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Recently, the unprecedented advancement of Large Language Models (LLMs) has revolutionized in numerous applications in the vision-language domain. Inspired by the extraordinary visual understanding and logical reasoning abilities, we pro- pose a method that employs LLMs to address the Multi-Modal Contextual Image Retrieval (MMCIR) problem, where the input hints include both visual and textual queries. Specifically, given a query comprising a sequence of images and texts, MMCIR aims to select an image from a gallery that best matches the context of the query. In this paper, we first construct a Multi-Modal Captioning (MMC) dataset by enriching existing image captioning datasets from ⟨image, caption⟩ to ⟨reference image, reference caption, text condition, target caption⟩. Then, we introduce a Context-Aware Captioning (CA-Cap) and a Context-Aware Text Matching (CA-TM) objective to instruct a frozen LLM for MMCIR. These specialized objectives enable the LLM to better understand multi-modal inputs and output visual representation from complex multi-modal contexts. Comprehensive experiments demonstrate the effectiveness of our method on recent Zero- Shot Composed Image Retrieval (ZS-CIR) benchmarks (i.e., CIRCO, CIRR, and GeneCIS), and in complex scenarios with dense multi-modal inputs like Visual Storytelling and Visual Dialog.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=J88EKENxyF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JBLHIR8kBZ",
      "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. \n\nThis paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N$2$G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. \n\nN$2$G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JBLHIR8kBZ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Esben_Kran1",
        "name": "Esben Kran",
        "name_site": null,
        "openreview_id": "~Esben_Kran1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://apartresearch.com",
        "dblp_id": null,
        "google_scholar_url": "SH5diRUAAAAJ",
        "orcid": "0000-0003-0710-2635",
        "linkedin_url": "esbenkc/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Heriot-Watt University (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.7888543819998317,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 22,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "JuyFppXzh2",
      "title": "Gandalf: Learning label correlations in Extreme Multi-label Classification via Label Features",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches, amongst others. In this paper, we propose Gandalf, a novel approach which makes use of a label correlation graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations.  While most recent advances in XMC have been algorithmic, mainly aimed towards developing novel deep-learning frameworks, our data-centric augmentation approach is orthogonal to these methodologies, and can be applied in a plug-and-play manner to a variety of them. This generality and effectiveness of \\textit{Gandalf} is demonstrated by showing up to 30\\% relative improvements for 5 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3 million labels.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=JuyFppXzh2",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Devaansh_Gupta1",
        "name": "Devaansh Gupta",
        "name_site": null,
        "openreview_id": "~Devaansh_Gupta1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://devaansh100.github.io",
        "dblp_id": "351/9786",
        "google_scholar_url": "lSBqiz4AAAAJ",
        "orcid": null,
        "linkedin_url": "devaanshgupta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Nanyang Technological University (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K1VLZ5rNuZ",
      "title": "$MC^2$: Multimodal Concept-based Continual learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The inability of deep neural networks to learn continually while retaining interpretability limit their deployment in critical settings. Existing research has made strides in either interpretability or continual learning, but the synergy of these two directions largely remains under-explored. This work examines this intersection from the perspective of concept-based models where classes are considered as combinations of text-based concepts, and thus can enhance the interpretability of models in a continual learning setting. Addressing the unique challenges of learning new concepts without forgetting past ones, our method $\\mathbf{MC^2}$ proposes an approach to seamlessly learn both classes and concepts over time. We adopt a multimodal approach to concepts, emphasizing text-based human-understandavle semantics associated with images. Through various experimental studies, we demonstrate that $\\mathbf{MC^2}$ outperforms existing concept-based approaches by a large margin in a continual setting, while performing comparably if not better in full-data settings. We also demonstrate that $\\mathbf{MC^2}$ can be used as a post-hoc interpretability method to examine image regions associated with abstract textual concepts. Our code for $\\mathbf{MC^2}$ will be publicly released on acceptance.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=K1VLZ5rNuZ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Susmit_Agrawal1",
        "name": "Susmit Agrawal",
        "name_site": null,
        "openreview_id": "~Susmit_Agrawal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://susmit-a.github.io",
        "dblp_id": "278/3579",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-7701-131X",
        "linkedin_url": "susmitagrawal",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K1VLZ5rNuZ",
      "title": "$MC^2$: Multimodal Concept-based Continual learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The inability of deep neural networks to learn continually while retaining interpretability limit their deployment in critical settings. Existing research has made strides in either interpretability or continual learning, but the synergy of these two directions largely remains under-explored. This work examines this intersection from the perspective of concept-based models where classes are considered as combinations of text-based concepts, and thus can enhance the interpretability of models in a continual learning setting. Addressing the unique challenges of learning new concepts without forgetting past ones, our method $\\mathbf{MC^2}$ proposes an approach to seamlessly learn both classes and concepts over time. We adopt a multimodal approach to concepts, emphasizing text-based human-understandavle semantics associated with images. Through various experimental studies, we demonstrate that $\\mathbf{MC^2}$ outperforms existing concept-based approaches by a large margin in a continual setting, while performing comparably if not better in full-data settings. We also demonstrate that $\\mathbf{MC^2}$ can be used as a post-hoc interpretability method to examine image regions associated with abstract textual concepts. Our code for $\\mathbf{MC^2}$ will be publicly released on acceptance.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=K1VLZ5rNuZ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepika_Vemuri1",
        "name": "Deepika Vemuri",
        "name_site": null,
        "openreview_id": "~Deepika_Vemuri1",
        "position": 2,
        "gender": null,
        "homepage_url": "https://sites.google.com/view/deepika-vemuri-homepage/home",
        "dblp_id": null,
        "google_scholar_url": "vpTuijEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K1VLZ5rNuZ",
      "title": "$MC^2$: Multimodal Concept-based Continual learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The inability of deep neural networks to learn continually while retaining interpretability limit their deployment in critical settings. Existing research has made strides in either interpretability or continual learning, but the synergy of these two directions largely remains under-explored. This work examines this intersection from the perspective of concept-based models where classes are considered as combinations of text-based concepts, and thus can enhance the interpretability of models in a continual learning setting. Addressing the unique challenges of learning new concepts without forgetting past ones, our method $\\mathbf{MC^2}$ proposes an approach to seamlessly learn both classes and concepts over time. We adopt a multimodal approach to concepts, emphasizing text-based human-understandavle semantics associated with images. Through various experimental studies, we demonstrate that $\\mathbf{MC^2}$ outperforms existing concept-based approaches by a large margin in a continual setting, while performing comparably if not better in full-data settings. We also demonstrate that $\\mathbf{MC^2}$ can be used as a post-hoc interpretability method to examine image regions associated with abstract textual concepts. Our code for $\\mathbf{MC^2}$ will be publicly released on acceptance.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=K1VLZ5rNuZ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaji_Krishnamurthy1_1",
        "name": "Balaji Krishnamurthy",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K7KQkiHanD",
      "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present Generalized LoRA (GLoRA), a flexible approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. \nOriginating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks in the vision field, achieving superior accuracy with fewer parameters and computations. Our models on LLaMA-1 and 2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=K7KQkiHanD",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Gupta2",
        "name": "Deepak Gupta",
        "name_site": null,
        "openreview_id": "~Deepak_Gupta2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://dkgupta90.github.io",
        "dblp_id": "163/3197.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Nsxpe_kAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 92,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "LRpfp86Z38",
      "title": "Logarithmic Linear Units (LogLUs): A Novel Activation Function for Training Deep Neural Networks",
      "status": "Desk Reject",
      "normalized_status": "rejected",
      "abstract": "The Logarithmic Linear Unit (LogLU) introduces a novel approach to activation functions in deep neural networks. By incorporating logarithmic methods into its mathematical equation, LogLU revolutionizes the training process, leading to faster learning and improved accuracy across diverse datasets, including numerical, image, and time series data. Much like Rectified linear unit (ReLU), Leaky ReLU, and Exponential Linear Unit (ELU), LogLU effectively tackles the vanishing gradient problem and mitigates the dead neuron issue that plagues ReLU. LogLU also has the ability to produce negative values, driving the mean unit activation closer to zero. This concept is inspired by stochastic gradient descent, which rapidly approaches the global minimum with a high learning rate before taking smaller steps as it nears the minimum. In experiments, LogLU not only accelerates learning but also yields more generalized models compared to other activation functions. Its primary goal in building deep neural networks is to achieve high generalization, ensuring that training and test accuracies closely align. We evaluated LogLU performance on three diverse datasets and its accuracy was: (i) Breast Cancer – Numerical Dataset (0.91) (ii) MNIST – Image Dataset (0.95) (iii) Jena Climate - Time Series Analysis Dataset (0.99). Results demonstrate that LogLU outperforms other activation functions in terms of learning characteristics. It represents a significant advancement in deep learning, offering researchers and practitioners a powerful tool to enhance neural network performance and generalization.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=LRpfp86Z38",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 1,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishi_Chaitanya_Sri_Prasad_Nalluri1",
        "name": "Rishi Chaitanya Sri Prasad Nalluri",
        "name_site": null,
        "openreview_id": "~Rishi_Chaitanya_Sri_Prasad_Nalluri1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "rishichaitanya/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "SRM University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MamHzZHs0h",
      "title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small \"personalized dataset\" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to \"in-context learn\" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn \"drug synergy functions\". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient's \"personalized dataset\". Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=MamHzZHs0h",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Khot1",
        "name": "Tushar Khot",
        "name_site": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal",
        "openreview_id": "~Tushar_Khot1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://allenai.org/team/tushark/",
        "dblp_id": "83/8117",
        "google_scholar_url": "_8mkIjgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Allen Institute for Artificial Intelligence (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 4.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "N581Nje6fH",
      "title": "Long Horizon Episodic Decision Making for Cognitively Inspired Robots",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The Human decision-making process works by recollecting past sequences of observations and using them to decide the best possible action in the present. These past sequences of observations are stored in a derived form which only includes important information the brain thinks might be useful in the future, while forgetting the rest. Transformers have shown great results in multi-modal robotic navigation and human-robot collaboration tasks but lack the ability to scale to large memory sizes and learn long horizon tasks efficiently as the computational requirements needed to run these models scale non-linearly with memory length. Our model for tries to mimic the human brain and improve the memory efficiency of transformers by using a modified TransformerXL architecture which uses Automatic Chunking that chunks the past memories and only attends to the relevant chunks in the transformer block. On top of this, we use ForgetSpan which is technique to remove memories that do not contribute to learning. We also theorize the technique of Similarity based forgetting where the current observations are compared with the elements in the memory and only the new observations are stored, similar to how humans do not store repetitive memories. We test our model in various visual and audio-visual tasks that demand long horizon recollection, audio-visual instruction deciphering and robotic navigation. These tasks test the abilities of the robot that would be required in a human-robot collaboration scenario. We demonstrate that Automatic Chunking with ForgetSpan can improve the memory efficiency and help models to memorize important information and also achieve better performance than the baseline TransformerXL in the tasks previously mentioned. We also show that our model generalizes well by testing the trained models in modified versions of the tasks.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=N581Nje6fH",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shweta_Singh3",
        "name": "Shweta Singh",
        "name_site": null,
        "openreview_id": "~Shweta_Singh3",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0003-7055-0251",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 1.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NLRo4qhg6t",
      "title": "HIWE: Scene Importance Weighted Encoding For Fast Neural Radiance Field Training",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Neural radiance fields (NeRFs) have emerged as a powerful scene representa-\ntion technique to implicitly encode radiance information in space. Recent works\ndemonstrated that using a grid-based positional encoding to encode 3D radiance\ninformation in space achieves fast training speeds, often requiring only a few min-\nutes of training on small-scale synthetic datasets. However, training a NeRF model\nthat uses a grid encoding on large outdoor scenes requires several hours of train-\ning. In many scenarios, large scenes may have different amounts of detailing at\ndifferent regions, with reconstruction/representation quality more important for\nsome detailing compared to others. Different regions of the scene are however\ngiven equal importance and thus typically no regions of the scene are prioritized\nin allocating parameters in the learned model. In this work, we propose a new\ngrid-based positional encoding technique that integrates scene importance infor-\nmation in large scenes to accelerate training. Our encoding flexibly allocates more\nmodel parameters to learn the radiance information in regions of the scene that\nare deemed more important. This ensures that the more detailed scene regions are\nrepresented with a larger number of parameters, allowing more detailed radiance\ninformation to be encoded. With our approach, we demonstrate higher quality\nrepresentation for the important parts of the scene compared to state-of-art tech-\nniques for instant NeRF training, while enabling on-par or faster training times as\nstate-of-art NeRF models and small model sizes.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NLRo4qhg6t",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shorya_Kumar1",
        "name": "Shorya Kumar",
        "name_site": null,
        "openreview_id": "~Shorya_Kumar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://shoryak.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NV6rn7j5p5",
      "title": "GEO: Generative Engine Optimization",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), has the potential to generate accurate and personalized responses, and is rapidly replacing traditional search engines like Google and Bing. Generative engines typically satisfy queries by synthesizing information from multiple sources and summarizing them with the help of LLMs. While this shift significantly improves user utility and generative search engine traffic, it results in a huge challenge for the third stakeholder - website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, the right tools should be provided to ensure that creator economy is not severely disadvantaged. To address this, we introduce generative engine optimization (GEO), a novel paradigm to aid content creators in improving their visibility. In this work, we propose several optimizations that can be applied to improve the visibility of content. To evaluate and compare different GEO methods, we propose a benchmark encompassing diverse user queries from multiple domains and settings, along with relevant sources needed to answer those queries. Through rigorous experiments on the proposed benchmark, we demonstrate different GEO methods involving well-designed textual enhancements, are capable of boosting source visibility by up to 40% in Generative engines responses. We find several insights that aid content creators -- for example, adding citations and quotations significantly improves visibility. We also discover that these optimizations are domain dependent, thus requiring a change in the nature of the optimization based on the source. Our work opens a new frontier in the field of information discovery systems, with profound implications for both developers of Generative enginess and content creators.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NV6rn7j5p5",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pranjal_Aggarwal1",
        "name": "Pranjal Aggarwal",
        "name_site": null,
        "openreview_id": "~Pranjal_Aggarwal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/Pranjal2041/",
        "dblp_id": "163/0764",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-2962-1535",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.0,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.9428090415820634,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NqpdT8DwGc",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under $20$.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NqpdT8DwGc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubhi_Shukla1",
        "name": "Shubhi Shukla",
        "name_site": null,
        "openreview_id": "~Shubhi_Shukla1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://www.shubhishukla.com/",
        "dblp_id": null,
        "google_scholar_url": "UDQZWrkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NqpdT8DwGc",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under $20$.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NqpdT8DwGc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manaar_Alam1",
        "name": "Manaar Alam",
        "name_site": null,
        "openreview_id": "~Manaar_Alam1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://manaaralam.github.io",
        "dblp_id": "192/5163",
        "google_scholar_url": "46jmlGgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "New York University (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NqpdT8DwGc",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under $20$.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NqpdT8DwGc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pabitra_Mitra1",
        "name": "Pabitra Mitra",
        "name_site": null,
        "openreview_id": "~Pabitra_Mitra1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~pabitra/",
        "dblp_id": "m/PabitraMitra",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=5bXSZPYAAAAJ",
        "orcid": "0000-0002-1908-9813",
        "linkedin_url": "pabitra-mitra-8028235/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvJxTjTQtq",
      "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvJxTjTQtq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Utkarsh_Pratiush1",
        "name": "Utkarsh Pratiush",
        "name_site": null,
        "openreview_id": "~Utkarsh_Pratiush1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "BqhnGGAAAAAJ",
        "orcid": null,
        "linkedin_url": "utkarsh-pratiush-376ab6171/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvJxTjTQtq",
      "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvJxTjTQtq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvJxTjTQtq",
      "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvJxTjTQtq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Matthieu_Micoulaut1",
        "name": "Matthieu Micoulaut",
        "name_site": null,
        "openreview_id": "~Matthieu_Micoulaut1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://www.lptmc.jussieu.fr/user/mmi/index.html",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvJxTjTQtq",
      "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvJxTjTQtq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 9,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvJxTjTQtq",
      "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvJxTjTQtq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 10,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 10,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 24,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NvSwR4IvLO",
      "title": "Can AI-Generated Text be Reliably Detected?",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks, including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via perplexity scores and MTurk human study. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=NvSwR4IvLO",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinu_Sankar_Sadasivan1",
        "name": "Vinu Sankar Sadasivan",
        "name_site": null,
        "openreview_id": "~Vinu_Sankar_Sadasivan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vinusankars.github.io/",
        "dblp_id": "244/8052",
        "google_scholar_url": "y1IKIw0AAAAJ",
        "orcid": null,
        "linkedin_url": "vinusankars/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.666666666666667,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 468,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OZ3syNYe7D",
      "title": "PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments and show that PEAR is able to solve tasks that require long term decision making. We empirically show that PEAR exhibits improved performance and sample efficiency over previous hierarchical and non-hierarchical approaches. We also perform real world robotic experiments on complex tasks and demonstrate that PEAR consistently outperforms the baselines.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=OZ3syNYe7D",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Utsav_Singh1",
        "name": "Utsav Singh",
        "name_site": null,
        "openreview_id": "~Utsav_Singh1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.cse.iitk.ac.in/users/utsavz/",
        "dblp_id": "241/9336",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Ow3u6br0ub",
      "title": "Progressive Fusion for Multimodal Integration",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Integration of multimodal information from various sources has been shown to boost the performance of machine learning models and thus has received increased attention in recent years. Often such models use deep modality-specific networks to obtain unimodal features which are combined to obtain \"late-fusion\" representations. However, these designs run the risk of information loss in the respective unimodal pipelines. On the other hand, \"early-fusion\" methodologies, which combine features early, suffer from the problems associated with feature heterogeneity and high sample complexity.\nIn this work, we present an iterative representation refinement approach, called Progressive Fusion, a model-agnostic technique which makes late stage fused representations available to early layers through backward connections, improving the expressiveness of the  representations. Progressive Fusion avoid the information loss which occurs when late fusion is used, while retaining the advantages of late fusion designs. We test Progressive Fusion on tasks including affective sentiment detection, multimedia analysis, and time series fusion with different models, demonstrating its versatility. We show that our approach consistently improves performance, for instance attaining a 5\\% reduction in MSE and 40\\% improvement in robustness on multimodal time series prediction.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Ow3u6br0ub",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shiv_Shankar2",
        "name": "Shiv Shankar",
        "name_site": null,
        "openreview_id": "~Shiv_Shankar2",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "203/9123",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PbpJnyewVM",
      "title": "Zero-shot Cross-task Preference Alignment for Offline RL via Optimal Transport",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In preference-based Reinforcement Learning (PbRL), aligning rewards with human intentions often necessitates a substantial volume of human-provided labels. Furthermore, the expensive preference data from prior tasks often lacks reusability for subsequent tasks, resulting in repetitive labeling for each new task. In this paper, we propose a novel zero-shot cross-task preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, direct learning from these inferred labels might introduce noisy or inaccurate reward functions. To this end, we introduce Robust Preference Transformer, which considers both reward mean and uncertainty by modeling rewards as Gaussian distributions. Through extensive empirical validation on robotic manipulation tasks from Meta-World and Robomimic, our approach exhibits strong capabilities of transferring preferences between tasks in a zero-shot way and learns reward functions from noisy labels robustly. Notably, our approach significantly surpasses existing methods in limited-data scenarios. The videos of our method are available on the website: https://sites.google.com/view/pot-rpt.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=PbpJnyewVM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Shanghai Jiao Tong University (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PcBJ4pA6bF",
      "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Conventional Federated Learning (FL) involves collaborative training of a global model by multiple client local models. In this emerging paradigm, the central server assumes a critical role in aggregating local models and maintaining the global model. However, it encounters various challenges, including scalability, management, and inefficiencies arising from idle client devices. \nRecently, studies on serverless decentralized FL have shown advantages in overcoming these challenges, enabling clients to own different local models and separately optimize local data. Despite the promising advancements in decentralized FL, it is crucial to thoroughly investigate the implications of data and model heterogeneity, which pose unique challenges that must be overcome. Therefore, the research question to be answered in this study is: How can every client's local model learn generalizable representation?\nTo address this question, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA. Inspired by the theory of domain adaptation and Knowledge distillation (KD), we leverage the synthetic anchors to design two effective regularization terms for local training: 1) anchor loss that matches the distribution of the client's latent embedding with an anchor and 2) KD loss that enables clients learning from others. \nIn contrast to previous KD-based heterogeneous FL methods, we don’t presume access to real public or a global data generator. \nDeSA enables each client's model to become robust to distribution shift across different client-domains. Through extensive experiments on diverse client data distributions, we showcase the effectiveness of \\ours{} in enhancing both inter and intra-domain accuracy of each client.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=PcBJ4pA6bF",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kartik_Srinivas1",
        "name": "Kartik Srinivas",
        "name_site": null,
        "openreview_id": "~Kartik_Srinivas1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://kartiksrinivas007.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=TxnwVpgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.8,
        "rating_std": 1.469693845669907,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Q3aKBKCqG8",
      "title": "UBERT: Unsupervised adaptive early exits in BERT",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Inference latency is an issue in pre-trained networks like BERT due to their large size. To overcome this, side branches are attached at the intermediary layers with provision for early inference instead of inference only at the last layer. This facilitates the early exit of 'easy' samples and requires only 'hard' samples to pass through all layers, thus reducing inference latency.  However, the hardness of the samples is unknown a priori. This leads to the question of how to exit so that the accuracy and latency are well balanced. Also, the optimal choice of parameters involved in deciding exits can depend on the sample domain and hence need to be adapted. We develop an online learning algorithm named UBERT to decide if a sample can exit early. The decisions are based on confidence in inference exceeding a threshold at each exit point, and the algorithm simultaneously learns the optimal thresholds for all the exits. UBERT learns the optimal threshold for the sample domain using confidence observed at the intermediary layers without requiring any ground truth labels. We perform extensive experiments on five datasets with one and two early exits. We compare the performance against the case with no early exits, i.e., all samples exit at the last layer. UBERT achieves a 10\\%-53\\% reduction in time with a drop in accuracy in the range of 0.3\\% - 5.7\\% with one early exit. For the case with two exits, the time reduction increases to 32\\%-70\\% with only a marginal drop in accuracy of 0.1\\%-3.9\\%. The anonymized source code is available at https://anonymous.4open.science/r/UBERT-F2DF/README.md.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Q3aKBKCqG8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Divya_Jyoti_Bajpai1",
        "name": "Divya Jyoti Bajpai",
        "name_site": null,
        "openreview_id": "~Divya_Jyoti_Bajpai1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.ieor.iitb.ac.in/node/2823",
        "dblp_id": "357/1369",
        "google_scholar_url": "J-z9diIAAAAJ",
        "orcid": null,
        "linkedin_url": "divya-jyoti-bajpai-139128209",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 1.299038105676658,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Q3aKBKCqG8",
      "title": "UBERT: Unsupervised adaptive early exits in BERT",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Inference latency is an issue in pre-trained networks like BERT due to their large size. To overcome this, side branches are attached at the intermediary layers with provision for early inference instead of inference only at the last layer. This facilitates the early exit of 'easy' samples and requires only 'hard' samples to pass through all layers, thus reducing inference latency.  However, the hardness of the samples is unknown a priori. This leads to the question of how to exit so that the accuracy and latency are well balanced. Also, the optimal choice of parameters involved in deciding exits can depend on the sample domain and hence need to be adapted. We develop an online learning algorithm named UBERT to decide if a sample can exit early. The decisions are based on confidence in inference exceeding a threshold at each exit point, and the algorithm simultaneously learns the optimal thresholds for all the exits. UBERT learns the optimal threshold for the sample domain using confidence observed at the intermediary layers without requiring any ground truth labels. We perform extensive experiments on five datasets with one and two early exits. We compare the performance against the case with no early exits, i.e., all samples exit at the last layer. UBERT achieves a 10\\%-53\\% reduction in time with a drop in accuracy in the range of 0.3\\% - 5.7\\% with one early exit. For the case with two exits, the time reduction increases to 32\\%-70\\% with only a marginal drop in accuracy of 0.1\\%-3.9\\%. The anonymized source code is available at https://anonymous.4open.science/r/UBERT-F2DF/README.md.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Q3aKBKCqG8",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manjesh_Kumar_Hanawal1",
        "name": "Manjesh Kumar Hanawal",
        "name_site": null,
        "openreview_id": "~Manjesh_Kumar_Hanawal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.ieor.iitb.ac.in/mlions",
        "dblp_id": "01/8397",
        "google_scholar_url": "vtVK3KUAAAAJ",
        "orcid": "0000-0002-1807-5487",
        "linkedin_url": "manjesh-kumar-hanawal-a260055/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 1.299038105676658,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Q8ibi56aM6",
      "title": "SINGLE-IMAGE COHERENT RECONSTRUCTION OF OBJECTS AND HUMANS",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Existing methods for reconstruction of objects and humans from a monocular image suffer from severe mesh collisions and performance limitations for interacting occluding objects. In this paper, we introduce a method that deduces spatial configurations and achieves globally consistent 3D reconstruction for interacting objects and people captured within a single image. Our contributions encompass: 1) an optimization framework, featuring a novel collision loss, tailored to handle complex human-object and human-human interactions, ensuring spatially coherent scene reconstruction; and 2) a novel technique for robustly estimating 6 degrees of freedom (DOF) poses, particularly for heavily occluded objects, exploiting image inpainting. Notably, our proposed method operates effectively on images from real-world scenarios, without necessitating scene or object-level 3D supervision. Through both qualitative and quantitative assessments, we demonstrate the superior quality of our reconstructions, showcasing a significant reduction in collisions in scenes with multiple interacting humans and objects.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Q8ibi56aM6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Partha_Pratim_Chakrabarti1",
        "name": "Partha Pratim Chakrabarti",
        "name_site": null,
        "openreview_id": "~Partha_Pratim_Chakrabarti1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.iitkgp.ac.in/department/CS/faculty/cs-ppchak",
        "dblp_id": "c/PPChakrabarti.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-3553-8834",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "QQ81YsbSij",
      "title": "Learning Conditional Policy for Crystal Design using Offline Reinforcement Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Navigating through the exponentially large chemical space to search for desirable materials is an extremely challenging task in material discovery. Recent developments in generative and geometric deep learning have shown promising results in molecule and material discovery but often lack evaluation with high-accuracy computational methods. This work aims to design novel and stable crystalline materials conditioned on a desired band gap. To achieve conditional generation, we:  1. Formulate crystal design as a sequential decision-making problem, create relevant trajectories based on high-quality materials data and use conservative Q-learning to learn a conditional policy from these trajectories. To do so, we formulate a reward function that incorporates constraints for energetic and electronic properties obtained directly from density functional theory (DFT) calculations;  2. Evaluate the generated materials from the policy using DFT calculations for both energy and band gap; 3. Compare our results to relevant baselines, including a random policy, behavioral cloning, and unconditioned policy learning. Our experiments show that our conditioned policies achieve more targeted crystal structure designs and demonstrate the capability to perform crystal structure design evaluated with accurate and computationally expensive DFT calculations.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=QQ81YsbSij",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Santiago_Miret1",
        "name": "Santiago Miret",
        "name_site": null,
        "openreview_id": "~Santiago_Miret1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.intel.ai/bio/santiago-miret/",
        "dblp_id": "241/5030",
        "google_scholar_url": "HLQ_te4AAAAJ",
        "orcid": "0000-0002-5121-3853",
        "linkedin_url": "santiago-miret/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 1.299038105676658,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "QV6uB196cR",
      "title": "A/B testing under Identity Fragmentation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Randomized online experimentation is a key cornerstone of the online world. The infrastructure enabling such methodologies is critically dependent on user identification. However, nowadays consumers routinely interact with online businesses across multiple devices which are often recorded with different identifiers for the same consumer. The inability to match different device identities across consumers leads to an incorrect estimation of various causal effects. Moreover, without strong assumptions about the device-user graph, the causal effects are not identifiable. In this paper, we consider the task of estimating global treatment effects (GATE) from a fragmented view of exposures and outcomes. Our experiments validate our theoretical analysis, and estimators obtained through our procedure are shown be superior to standard estimators, with a lower bias and increased robustness.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=QV6uB196cR",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shiv_Shankar2",
        "name": "Shiv Shankar",
        "name_site": null,
        "openreview_id": "~Shiv_Shankar2",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "203/9123",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RKw6AzP2BY",
      "title": "Decoding LLM's: The Interplay of Transformation Matrices and Input Complexity",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "In the realm of Large Language Models (LLMs) like GPT-3, methodologies such as In-Context Learning (ICL) and the \"Chain of Thoughts\" (CoT) approach have become prominent. Yet, a significant research gap persists: the underlying mechanics explaining their efficacy remain vague. While existing hypotheses provide some insights, they fall short in offering a comprehensive understanding. To bridge this gap, we introduce a rigorous mathematical analysis, interpreting LLM parameters as transformation matrices that convert the complexities of textual data into high-dimensional vector spaces. Our analysis robustly postulates the correctness of this interpretation, providing a fresh perspective on LLM behaviors. At their heart, LLMs primarily operate as pattern matchers. They recognize patterns from the input text, drawing from their vast training, and produce outputs. Here, the complexity of the input prompts becomes crucial. A complex input can nudge the LLM to generate a more refined response, hinting at the concept of intrinsic dimensionality, which gauges the inherent complexity of input. In light of our insights, we advocate for a strategic shift in fine-tuning LLMs. We propose fine-tuning them on logical reasoning tasks, specifically leveraging reasoning questions (Verbal Reasoning, Probability, Assertion and Reason). This approach, rooted in our mathematical framework, enables LLMs with the technique to decipher the logical layers in the data, promising to harness the true potential of LLMs, guiding them beyond mere pattern matching to deeper textual comprehension. This in turn also improves their causal inferencing ability. In essence, our paper offers a structured blueprint, seamlessly transitioning from identifying research gaps to actionable strategies, aiming to elevate the capabilities and understanding of Large Language Models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RKw6AzP2BY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rudransh_Agnihotri1",
        "name": "Rudransh Agnihotri",
        "name_site": null,
        "openreview_id": "~Rudransh_Agnihotri1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "rudransh2004/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Delhi Skill and Entrepreneurship University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RmRA7Q0lwQ",
      "title": "Stay on Topic with Classifier-Free Guidance",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75% preference for GPT4All using CFG over baseline.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RmRA7Q0lwQ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Honglu_Fan1",
        "name": "Honglu Fan",
        "name_site": null,
        "openreview_id": "~Honglu_Fan1",
        "position": 3,
        "gender": "Not Specified",
        "homepage_url": "https://honglu.fan",
        "dblp_id": null,
        "google_scholar_url": "XqlOVeAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 43,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RvmrhrPy7j",
      "title": "Causal Inference Using LLM-Guided Discovery",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "At the core of causal inference lies the critical challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs.  Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to established discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RvmrhrPy7j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abbavaram_Gowtham_Reddy1",
        "name": "Abbavaram Gowtham Reddy",
        "name_site": null,
        "openreview_id": "~Abbavaram_Gowtham_Reddy1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://gautam0707.github.io",
        "dblp_id": "294/8798",
        "google_scholar_url": "Iewg-GAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RvmrhrPy7j",
      "title": "Causal Inference Using LLM-Guided Discovery",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "At the core of causal inference lies the critical challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs.  Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to established discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RvmrhrPy7j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "RvmrhrPy7j",
      "title": "Causal Inference Using LLM-Guided Discovery",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "At the core of causal inference lies the critical challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs.  Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to established discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=RvmrhrPy7j",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaji_Krishnamurthy1_1",
        "name": "Balaji Krishnamurthy",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "S7ZQgHfW3w",
      "title": "CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continuous dynamical systems, characterized by differential equations, are ubiq- uitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CoDBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency- based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid and solid mechanics. We conduct extensive experiments, assessing the operators’ capabilities in learning, zero-shot super-resolution, data efficiency, robustness to noise, and computational efficiency. Interestingly, our findings highlight that current operators struggle with the newer mechanics datasets, motivating the need for more robust neural oper- ators. All the datasets and codes are shared in an easy-to-use fashion for the scientific community. We hope this resource will be an impetus for accelerated progress and exploration in modeling dynamical systems. For codes and datasets, see: https://anonymous.4open.science/r/cod-bench-7525.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=S7ZQgHfW3w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Priyanshu_Burark1",
        "name": "Priyanshu Burark",
        "name_site": null,
        "openreview_id": "~Priyanshu_Burark1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "priyanshu-burark-6a139b1ab",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "S7ZQgHfW3w",
      "title": "CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continuous dynamical systems, characterized by differential equations, are ubiq- uitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CoDBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency- based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid and solid mechanics. We conduct extensive experiments, assessing the operators’ capabilities in learning, zero-shot super-resolution, data efficiency, robustness to noise, and computational efficiency. Interestingly, our findings highlight that current operators struggle with the newer mechanics datasets, motivating the need for more robust neural oper- ators. All the datasets and codes are shared in an easy-to-use fashion for the scientific community. We hope this resource will be an impetus for accelerated progress and exploration in modeling dynamical systems. For codes and datasets, see: https://anonymous.4open.science/r/cod-bench-7525.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=S7ZQgHfW3w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karn_Tiwari1",
        "name": "Karn Tiwari",
        "name_site": null,
        "openreview_id": "~Karn_Tiwari1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "karn3003/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "S7ZQgHfW3w",
      "title": "CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continuous dynamical systems, characterized by differential equations, are ubiq- uitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CoDBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency- based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid and solid mechanics. We conduct extensive experiments, assessing the operators’ capabilities in learning, zero-shot super-resolution, data efficiency, robustness to noise, and computational efficiency. Interestingly, our findings highlight that current operators struggle with the newer mechanics datasets, motivating the need for more robust neural oper- ators. All the datasets and codes are shared in an easy-to-use fashion for the scientific community. We hope this resource will be an impetus for accelerated progress and exploration in modeling dynamical systems. For codes and datasets, see: https://anonymous.4open.science/r/cod-bench-7525.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=S7ZQgHfW3w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Meer_Mehran_Rashid1",
        "name": "Meer Mehran Rashid",
        "name_site": null,
        "openreview_id": "~Meer_Mehran_Rashid1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "meermehran",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Johns Hopkins University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "S7ZQgHfW3w",
      "title": "CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continuous dynamical systems, characterized by differential equations, are ubiq- uitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CoDBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency- based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid and solid mechanics. We conduct extensive experiments, assessing the operators’ capabilities in learning, zero-shot super-resolution, data efficiency, robustness to noise, and computational efficiency. Interestingly, our findings highlight that current operators struggle with the newer mechanics datasets, motivating the need for more robust neural oper- ators. All the datasets and codes are shared in an easy-to-use fashion for the scientific community. We hope this resource will be an impetus for accelerated progress and exploration in modeling dynamical systems. For codes and datasets, see: https://anonymous.4open.science/r/cod-bench-7525.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=S7ZQgHfW3w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "S7ZQgHfW3w",
      "title": "CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Continuous dynamical systems, characterized by differential equations, are ubiq- uitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CoDBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency- based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid and solid mechanics. We conduct extensive experiments, assessing the operators’ capabilities in learning, zero-shot super-resolution, data efficiency, robustness to noise, and computational efficiency. Interestingly, our findings highlight that current operators struggle with the newer mechanics datasets, motivating the need for more robust neural oper- ators. All the datasets and codes are shared in an easy-to-use fashion for the scientific community. We hope this resource will be an impetus for accelerated progress and exploration in modeling dynamical systems. For codes and datasets, see: https://anonymous.4open.science/r/cod-bench-7525.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=S7ZQgHfW3w",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "T2ToleSDk6",
      "title": "Learning Constraints from Offline Dataset via Inverse Dual Values Estimation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "To develop safe control strategies, Inverse Constrained Reinforcement learning (ICRL) infers constraints from expert demonstrations and trains policy models under these constraints. Classical ICRL algorithms typically adopt an online learning diagram that permits boundless exploration in an interactive environment. However, in realistic applications, iteratively collecting experiences from the environment is dangerous and expensive, especially for safe-critical control tasks. To address this challenge, in this work, we present a novel Inverse Dual Values Estimation (IDVE) framework. To enable offline ICRL, IDVE dynamically integrates the conservative estimation inherent in offline RL and the data-driven inference in inverse RL, thereby effectively learning constraints from limited data. Specifically, IDVE derives the dual values functions for both rewards and costs, estimating their values in a bi-level optimization problem based on the offline dataset. \nTo derive a practical IDVE algorithm for offline constraint inference, we introduce the method of 1) tacking unknown transitions, 2) scaling to continuous environments, and 3) controlling the degree of constraint regularization.  Under these advancements, empirical studies demonstrate that IDVE outperforms other baselines in terms of accurately recovering the constraints and adapting to high-dimensional environments with diverse reward configurations.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=T2ToleSDk6",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~zhiqiang_xu1",
        "name": "zhiqiang xu",
        "name_site": null,
        "openreview_id": "~zhiqiang_xu1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://scholar.google.com/citations?user=0R20iBMAAAAJ&hl=en",
        "dblp_id": "72/51-3.html",
        "google_scholar_url": null,
        "orcid": "0000-0002-5693-8933",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Mohamed bin Zayed University of Artificial Intelligence (United Arab Emirates)",
        "countries": [
          "United Arab Emirates"
        ],
        "country_codes": [
          "AE"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "TLBPjECC5D",
      "title": "Unlearning via Sparse Representations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Machine unlearning, which involves erasing knowledge about a forget set from a trained model, can prove to be costly and infeasible by existing techniques. We propose a nearly compute-free zero-shot unlearning technique based on a discrete representational bottleneck. We show that the proposed technique efficiently unlearns the forget set and incurs negligible damage to the model's performance on the rest of the data set. We evaluate the proposed technique on the problem of \\textit{class unlearning} using three datasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning. Across all three datasets, the  proposed technique performs as well as if not better than SCRUB while incurring almost no computational cost.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=TLBPjECC5D",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "V8aD5pUcVX",
      "title": "What Makes for Good Visual Tokenizers for Large Language Models",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLM’s visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observed that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with a relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically rich targets. We obtain an intriguing insight: mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer – GVT, which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=V8aD5pUcVX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohan_Kankanhalli1",
        "name": "Mohan Kankanhalli",
        "name_site": null,
        "openreview_id": "~Mohan_Kankanhalli1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.comp.nus.edu.sg/~mohan",
        "dblp_id": "09/3613.html",
        "google_scholar_url": "6Lx_eowAAAAJ",
        "orcid": "0000-0002-4846-2015",
        "linkedin_url": "mohan-kankanhalli-583417221",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National University of Singapore (Singapore)",
        "countries": [
          "Singapore"
        ],
        "country_codes": [
          "SG"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VL2txtmPYA",
      "title": "Freenets: Learning Layerfree Neural Network Topologies",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "We propose a novel data driven approach to neural architectures based on information flows in a Neural Connectivity Graph (NCG). This technique gives rise to a category of neural networks that we call Free Networks, characterized entirely by the edges of an acyclic uni-directional graph. Further, we design a unique, data-informed methodology to systematically prune and augment connections in the proposed architecture during training. We show that any layered feed forward architecture is a subset of the class of Free Networks. Therefore, we propose that our method can produce a class of neural graphs that is a superset of any existing feed-forward networks. Additionally, we demonstrate the existence of certain classes of data, which are expressible through FreeNets, but not through any other feedforward architecture over the same number of neurons. We perform extensive experiments on this new architecture, to visualize the evolution of the neural topology over real world datasets, and showcase its performance alongside comparable baselines.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=VL2txtmPYA",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rahul_Madhavan1",
        "name": "Rahul Madhavan",
        "name_site": null,
        "openreview_id": "~Rahul_Madhavan1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "290/2008",
        "google_scholar_url": "HrM2xRcAAAAJ",
        "orcid": null,
        "linkedin_url": "rahul-madhavan/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.3333333333333335,
        "rating_std": 2.0548046676563256,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Vaf4sIrRUC",
      "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets.  Due to the weakly supervised nature of their training, precise control of their behavior in downstream tasks such as maximizing human-perceived image quality,  image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp to finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations.  We show AlignProp  achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Vaf4sIrRUC",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 83,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "WGLu9Mv8mn",
      "title": "POET: Prompt Offset Tuning for Continual Few-Shot Action Recognition",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "As virtual reality and augmented reality is redefining how users interact with computing devices, research in action and gesture recognition is indeed gaining prominence. Typically, these models deployed on AR/VR devices are trained in their factory, with large proprietary datasets. Though this training would cover the major set of activity and gestures classes, the user should ideally be able to add newer\nclasses to the model, without forgetting the base set of classes. Importantly, the user would be able to provide only few samples per class in this process. In-order to protect the user’s privacy, the setting should also not allow storage and replay of a data sample, for future learning. We formalize this pragmatic problem setting as privacy aware few-shot class incremental learning for activity and gestures.\nTowards this end, we propose a novel strategy, POET: Prompt-offset Tuning. Unlike other prompt tuning approaches that demand access to transformer models pretrained on a large amount of data, our approach demonstrates the efficacy of prompting on a significantly smaller model trained exclusively on the data from the base classes. Additionally, we take advantage of the temporal sequencing in the data stream of actions and gestures to propose a unique temporal-ordered learnable prompt selection and prompt attachment. To evaluate our newly proposed problem setting, we introduce new benchmarks on NTU RGB+D dataset for action recognition and SHREC-2017 dataset for hand gesture recognition.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=WGLu9Mv8mn",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaji_Krishnamurthy1_1",
        "name": "Balaji Krishnamurthy",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 9,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "WqsYs05Ri7",
      "title": "Estimation of Concept Explanations Should be Uncertainty Aware",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Model explanations are very valuable for interpreting and debugging prediction models. We study a specific kind of global explanations called Concept Explanations, where the goal is to interpret a model using human-understandable concepts. Recent advances in multi-modal learning rekindled interest in concept explanations and led to several label-efficient proposals for estimation. However, existing estimation methods are unstable to the choice of concepts or dataset that is used for computing explanations. We observe that instability in explanations is because estimations do not model noise. We propose an uncertainty aware estimation method, which readily improved reliability of the concept explanations. We demonstrate with theoretical analysis and empirical evaluation that explanations computed by our method are stable to the choice of concepts and data shifts while also being label-efficient and faithful.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=WqsYs05Ri7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vihari_Piratla1",
        "name": "Vihari Piratla",
        "name_site": "Vihari Piratla, Praneeth Netrapalli, Sunita Sarawagi",
        "openreview_id": "~Vihari_Piratla1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://vihari.github.io/",
        "dblp_id": "161/3626",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=DQddccYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Cambridge (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xsrsj3cne4",
      "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Xsrsj3cne4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vipul_Kumar_Singh1",
        "name": "Vipul Kumar Singh",
        "name_site": null,
        "openreview_id": "~Vipul_Kumar_Singh1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "MRJqKywAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xsrsj3cne4",
      "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Xsrsj3cne4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~SARATH_MOHAN1",
        "name": "SARATH MOHAN",
        "name_site": null,
        "openreview_id": "~SARATH_MOHAN1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "sarathmohaniitd/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xsrsj3cne4",
      "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Xsrsj3cne4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Xsrsj3cne4",
      "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=Xsrsj3cne4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayadeva_Jayadeva1",
        "name": "Jayadeva Jayadeva",
        "name_site": null,
        "openreview_id": "~Jayadeva_Jayadeva1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "58/4288",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "YKIGyf215Q",
      "title": "Structured Fine-Tuning Enables Data-Efficient Adaptation of Code Language Models",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Current models tailored for code tasks often adopt the successful pre-training-then-fine-tuning paradigm from natural language processing, treating source code in plain text as in natural language. \nThis approach, however, overlooks the well-defined and unambiguous structures inherent in programming languages.\nIn this work, we explore a data-efficient adaptation of pre-trained code language models by further training and fine-tuning them with program structures, which significantly improve the performance of the downstream coding tasks. \nSpecifically, we represent programs as parse trees, also known as concrete syntax trees (CSTs), and refine a model with serialized CSTs.\nFine-tuning with structures encourages the model to learn not only the associations of code text in different languages but also the mappings of their structures and grammars, by using only a small amount of data (e.g., 100 examples).\nWith a focus on generation, we design training objectives for encoder-decoder and decoder-only architectures. \nWe rigorously evaluate the proposed approach on various coding tasks and demonstrate that integrating parse structures with the plain-text representation of source code offers notable advantages, particularly in scenarios of low-data code translation.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=YKIGyf215Q",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yoon_Kim1",
        "name": "Yoon Kim",
        "name_site": null,
        "openreview_id": "~Yoon_Kim1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://people.csail.mit.edu/yoonkim/",
        "dblp_id": null,
        "google_scholar_url": "n_ts4eYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.8,
        "rating_std": 1.8330302779823362,
        "confidence_mean": 4.4,
        "confidence_std": 0.48989794855663565,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ZzeXjCQ5CK",
      "title": "FedORION: Aggregation-Assisted Proxyless Distillation for Heterogeneous Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "System heterogeneity in Federated Learning (FL) is commonly dealt with knowledge distillation by combining the clients' knowledge via distillation into a global model. However, such knowledge transfer to the global model is often limited by distillation efficiency and unavailability of the client data. Most of the existing approaches require proxy data on the server side for distillation, which becomes a bottleneck. To circumevent these limitations, we propose a novel FL framework, FedORION (Aggregation-Assisted Proxyless Distillation for Heterogeneous Federated Learning) that comprises of deep mutual learning (DML) at client end, and global aggregation followed by noise engineered data-free distillation at the server end. DML enables server side global aggregation which otherwise is infeasible due to different client model architectures. The aggregation results in knowledge integration which is further boosted by the subsequent distillation. This, however, also increases the burden on clients, especially with low computational budget. We, therefore, further introduce the idea of selective mutual learning where only those clients perform DML that are not limited by computational capacity. This reduces the overall computational burden without any compromise in the performance. We conduct rigorous experiments on various publicly available datasets and observe a remarkable improvement in the performance over the existing heterogeneous FL methods. For example, for TinyImagenet dataset, FedORION shows almost three times better performance as compared to the best baseline. The results provide evidence for the utility and effectiveness of our approach and open up a new direction for heterogeneous FL.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ZzeXjCQ5CK",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nirbhay_sharma1",
        "name": "Nirbhay sharma",
        "name_site": null,
        "openreview_id": "~Nirbhay_sharma1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "nirbhay-sharma-a2b846204/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "a9xZqOqzEW",
      "title": "A Logical Framework for Verification of AI Fairness",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the widespread use of AI in socially important decision-making processes, it becomes crucial to ensure that AI-generated decisions do not reflect discrimination towards certain groups or populations.  To address this challenge, our research introduces a theoretical framework based on the spider diagram---a reasoning system rooted in first-order predicate logic, and an extended version of the Euler and Venn diagrams---to define and verify the fairness of AI algorithms in decision-making.  This framework compares the sets representing the actual outcome of the algorithm and the expected outcome to identify bias in the model. The expected outcome of the model is calculated by considering the similarity score between the individual instances in the dataset. If the set of actual outcomes is a subset of the set of expected outcomes and all constant spiders in the former set have a corresponding foot in the expected outcome set, then the algorithm is free from bias. We further evaluate the performance of the AI model using the spider diagram that replaces the conventional confusion matrix in the literature. The framework also permits us to define a degree of bias and evaluate the same for specific AI models. Experimental results indicate that this framework surpasses traditional approaches in efficiency, with improvements in processing time and a reduced number of function calls.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=a9xZqOqzEW",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Priya_T_V1",
        "name": "Priya T V",
        "name_site": null,
        "openreview_id": "~Priya_T_V1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0000-8365-6686",
        "linkedin_url": "priya-t-v-593716b9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "New Horizon College of Engineering (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 1.6583123951777,
        "confidence_mean": 3.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "a9xZqOqzEW",
      "title": "A Logical Framework for Verification of AI Fairness",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "With the widespread use of AI in socially important decision-making processes, it becomes crucial to ensure that AI-generated decisions do not reflect discrimination towards certain groups or populations.  To address this challenge, our research introduces a theoretical framework based on the spider diagram---a reasoning system rooted in first-order predicate logic, and an extended version of the Euler and Venn diagrams---to define and verify the fairness of AI algorithms in decision-making.  This framework compares the sets representing the actual outcome of the algorithm and the expected outcome to identify bias in the model. The expected outcome of the model is calculated by considering the similarity score between the individual instances in the dataset. If the set of actual outcomes is a subset of the set of expected outcomes and all constant spiders in the former set have a corresponding foot in the expected outcome set, then the algorithm is free from bias. We further evaluate the performance of the AI model using the spider diagram that replaces the conventional confusion matrix in the literature. The framework also permits us to define a degree of bias and evaluate the same for specific AI models. Experimental results indicate that this framework surpasses traditional approaches in efficiency, with improvements in processing time and a reduced number of function calls.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=a9xZqOqzEW",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shrisha_Rao1",
        "name": "Shrisha Rao",
        "name_site": null,
        "openreview_id": "~Shrisha_Rao1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.iiitb.ac.in/faculty/shrisha-rao",
        "dblp_id": "http://www.informatik.uni-trier.de/~ley/pers/hd/r/Rao:Shrisha.html",
        "google_scholar_url": "http://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-0625-5103",
        "linkedin_url": "shrao/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Bangalore (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 1.6583123951777,
        "confidence_mean": 3.0,
        "confidence_std": 1.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aCgybhcZFi",
      "title": "Enhancing Neural Network Transparency through Representation Analysis",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In this paper, we introduce and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase these methods can provide traction on a wide range of safety-relevant problems, including truthfulness, memorization, power-seeking, and more, demonstrating the promise of representation-centered transparency research. We hope this work catalyzes further exploration into RepE and fosters advancements in the transparency and safety of AI systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=aCgybhcZFi",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 21,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shashwat_Goel1",
        "name": "Shashwat Goel",
        "name_site": null,
        "openreview_id": "~Shashwat_Goel1",
        "position": 11,
        "gender": "M",
        "homepage_url": "https://shash42.github.io/",
        "dblp_id": "300/8333.html",
        "google_scholar_url": "exaNV-0AAAAJ",
        "orcid": null,
        "linkedin_url": "shashwatgoel42/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of California, Berkeley (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.666666666666667,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "anG8cNYQAs",
      "title": "INCYDE: A large scale cyclone detection and intensity estimation dataset using satellite infrared imagery",
      "status": "Desk Reject",
      "normalized_status": "rejected",
      "abstract": "Tropical cyclones are devastating natural phenomena that cause a significant amount of damage every year. Conventionally, the Dvorak technique is used to detect cyclones and estimate cyclone intensity from satellite infrared imagery by observing cloud patterns. Satellite infrared imagery provides valuable information for detecting cyclonic storms. Recently, deep CNN models have proven to be highly efficient in detecting relevant patterns in the images. In this work, a novel cyclone detection and intensity estimation dataset called INCYDE (INSAT-based Cyclone Detection and Intensity Estimation) dataset is presented. The cyclone images in the dataset are captured from INSAT 3D/3DR satellites over the Indian Ocean. The proposed INCYDE dataset contains over 21k cyclone images taken from cyclones over the Indian Ocean from the year 2013 to 2021. The dataset pertains to two specific tasks: cyclone detection as an object detection task, and\nintensity estimation as a regression task. In addition to the dataset, this study in troduces baseline models that were trained on the newly presented dataset. The results of this research would help develop innovative cyclone detection and intensity estimation models, which in turn could help save lives.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=anG8cNYQAs",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arindam_Ghosh4",
        "name": "Arindam Ghosh",
        "name_site": null,
        "openreview_id": "~Arindam_Ghosh4",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Amity University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 1.632993161855452,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bEAVTKUEpJ",
      "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on four datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=bEAVTKUEpJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Darshana_Saravanan1",
        "name": "Darshana Saravanan",
        "name_site": null,
        "openreview_id": "~Naresh_Manwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/nareshmanwani/home",
        "dblp_id": "17/2536",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=qz4eDmgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bEAVTKUEpJ",
      "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on four datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=bEAVTKUEpJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineet_Gandhi2",
        "name": "Vineet Gandhi",
        "name_site": null,
        "openreview_id": "~Vineet_Gandhi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://faculty.iiit.ac.in/~vgandhi/",
        "dblp_id": "117/2021",
        "google_scholar_url": "https://scholar.google.fr/citations?user=PVlBz8oAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bEAVTKUEpJ",
      "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on four datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=bEAVTKUEpJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineet_Gandhi1",
        "name": "Vineet Gandhi",
        "name_site": null,
        "openreview_id": "~Darshana_S1",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "darshana1406/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cDInj7WMQm",
      "title": "UGC: UNIVERSAL GRAPH COARSENING",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In the era of big data, graphs have emerged as a natural representation for intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining essential features. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose Universal Graph Coarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor and is a first ever linear time-complexity framework. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to state of the art methods, UGC is 4x to 15x faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at 70\\% coarsening ratios.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cDInj7WMQm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohit_Kataria1",
        "name": "Mohit Kataria",
        "name_site": null,
        "openreview_id": "~Mohit_Kataria1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "passenger/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cDInj7WMQm",
      "title": "UGC: UNIVERSAL GRAPH COARSENING",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In the era of big data, graphs have emerged as a natural representation for intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining essential features. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose Universal Graph Coarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor and is a first ever linear time-complexity framework. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to state of the art methods, UGC is 4x to 15x faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at 70\\% coarsening ratios.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cDInj7WMQm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cDInj7WMQm",
      "title": "UGC: UNIVERSAL GRAPH COARSENING",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In the era of big data, graphs have emerged as a natural representation for intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining essential features. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose Universal Graph Coarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor and is a first ever linear time-complexity framework. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to state of the art methods, UGC is 4x to 15x faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at 70\\% coarsening ratios.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cDInj7WMQm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jayadeva_Jayadeva1",
        "name": "Jayadeva Jayadeva",
        "name_site": null,
        "openreview_id": "~Jayadeva_Jayadeva1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "58/4288",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cHy00K3Och",
      "title": "GRADSIMCORE: GRADIENT SIMILARITY BASED REPRESENTATIVE INSTANCES AS CORESET",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The rise in size and complexity of modern datasets and deep learning models have resulted in the usage of extensive computational resources and a rise in training time and effort. It also has increased the carbon footprint of training and fine-tuning models. One way to reduce the computational requirement is to extract the most representative subset (referred to as $\\textit{coreset}$) that can substitute for the larger dataset. Coresets can thus replace huge datasets to train models and tune hyperparameters, especially in the early stages of training. This will result in a significant reduction of computational resource requirement and reduce carbon footprint. We propose a simple and novel framework based on the similarity of loss gradients for identifying the representative training instances as a coreset. Our method, dubbed as $\\textit{GradSimCore}$, outperforms the state-of-the-art coreset selection algorithms on popular benchmark datasets ranging from MNIST to ImageNet. Because of its simplicity and effectiveness, our method is an important baseline for evaluating the effectiveness of the coreset selection algorithms. Anonymized codes for the proposed baseline are provided at https://anonymous.4open.science/r/GradSimCore-8884",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cHy00K3Och",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saumyaranjan_Mohanty1",
        "name": "Saumyaranjan Mohanty",
        "name_site": null,
        "openreview_id": "~Saumyaranjan_Mohanty1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "saumyaranjan-mohanty-a2768646/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "cHy00K3Och",
      "title": "GRADSIMCORE: GRADIENT SIMILARITY BASED REPRESENTATIVE INSTANCES AS CORESET",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The rise in size and complexity of modern datasets and deep learning models have resulted in the usage of extensive computational resources and a rise in training time and effort. It also has increased the carbon footprint of training and fine-tuning models. One way to reduce the computational requirement is to extract the most representative subset (referred to as $\\textit{coreset}$) that can substitute for the larger dataset. Coresets can thus replace huge datasets to train models and tune hyperparameters, especially in the early stages of training. This will result in a significant reduction of computational resource requirement and reduce carbon footprint. We propose a simple and novel framework based on the similarity of loss gradients for identifying the representative training instances as a coreset. Our method, dubbed as $\\textit{GradSimCore}$, outperforms the state-of-the-art coreset selection algorithms on popular benchmark datasets ranging from MNIST to ImageNet. Because of its simplicity and effectiveness, our method is an important baseline for evaluating the effectiveness of the coreset selection algorithms. Anonymized codes for the proposed baseline are provided at https://anonymous.4open.science/r/GradSimCore-8884",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=cHy00K3Och",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Konda_Reddy_Mopuri3",
        "name": "Konda Reddy Mopuri",
        "name_site": null,
        "openreview_id": "~Konda_Reddy_Mopuri3",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://krmopuri.github.io/",
        "dblp_id": "162/0085",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-8894-7212",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 1.0897247358851685,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "eVlcdbIx2O",
      "title": "A Generative Model for Game Theory with Flow Equilibrium",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "In recent years, generative models have emerged as a groundbreaking development in the field of artificial intelligence, transforming various domains such as image synthesis, natural language processing, and data generation. While recent studies have integrated generative models into multi-agent scenarios, their game-theoretical implications have remained largely unexplored. Specifically, the relationship between solutions derived from generative models and game theoretical equilibrium concepts lacks rigorous investigation.\nThis paper aims to bridge the gap between generative models and game theory by introducing a novel probabilistic framework for modelling multi-agent decision-making problems. This innovative framework reinterprets these problems as generative processes. Furthermore, we introduce a training objective known as \"flow equilibrium\" and establish a theoretical connection between flow equilibrium and Nash equilibrium. To analyse the theoretical properties of our framework, we present a tabular version algorithm along with a convergence proof. Additionally, we propose an extended algorithm incorporating neural networks to handle more complex environments. Notably, our framework naturally incorporates opponent modelling. Harnessing the capabilities of generative models, our framework excels in capturing the intricate dynamics of strategic interactions among agents. We validate our approach through testing on various multi-agent tasks, including cooperative and general-sum games. The empirical results consistently support our theoretical findings, demonstrating that our framework consistently outperforms existing methods in terms of solution quality.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=eVlcdbIx2O",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Huawei (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "er7VhmqZEA",
      "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recommender systems have become an essential component of various online plat-\nforms, providing personalized recommendations to users. Collaborative filtering-\nbased methods, such as matrix factorization, have been widely used to capture\nlatent user-item preferences. Recently, graph-based methods have shown promising\nresults by modeling the interactions between users and items as a graph and lever-\naging knowledge graphs (KG) to learn the user and item embeddings. Motivated\nby the recent success of contrastive learning in mining supervised signals from data\nitself, in this paper, we focus on establishing a noisy contrastive learning framework\nin Knowledge-aware recommendation systems and propose a self-supervised novel\nnoisy multi-view contrastive learning framework for improving top-K recommen-\ndation. In this paper, we propose a novel recommendation system architecture that\ngenerates three different views of user-item interactions for improved recommenda-\ntion along with a noise addition module. The global-level structural view leverages\nattention-based aggregation network Wang et al. (2019d) to capture collaborative\ninformation in the entity-item-user graph. In the item-item semantic view, we\nuse a K-nearest Neighbour item-item semantic module to incorporate semantic\nrelations among items. In the local view, we apply LightGCN He et al. (2020)\nwith noisy perturbations to generate robust user-item representations. We then use\ntwo more signals such as representation loss and uniformity loss in positive pairs\nto improve the quality of the representations and ensure uniform representations\nin the representational space. Experimental results on two benchmark datasets\ndemonstrate that our proposed method achieves superior performance compared\nto state-of-the-art methods. Additionally, we conducted extensive experiments\non CTR task-based datasets to demonstrate the robustness of our framework’s\ngeneralization in learning better user-item representations which can be seen in the\nsupplementary material. All the codes to generate reproducible results are available\nin this anonymous repository.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=er7VhmqZEA",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manav_Nitin_Kapadnis1",
        "name": "Manav Nitin Kapadnis",
        "name_site": null,
        "openreview_id": "~Manav_Nitin_Kapadnis1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://manavkapadnis.github.io/",
        "dblp_id": "304/7583",
        "google_scholar_url": "L7KLra8AAAAJ",
        "orcid": "0009-0003-8640-2106",
        "linkedin_url": "manav-nitin-kapadnis/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "er7VhmqZEA",
      "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recommender systems have become an essential component of various online plat-\nforms, providing personalized recommendations to users. Collaborative filtering-\nbased methods, such as matrix factorization, have been widely used to capture\nlatent user-item preferences. Recently, graph-based methods have shown promising\nresults by modeling the interactions between users and items as a graph and lever-\naging knowledge graphs (KG) to learn the user and item embeddings. Motivated\nby the recent success of contrastive learning in mining supervised signals from data\nitself, in this paper, we focus on establishing a noisy contrastive learning framework\nin Knowledge-aware recommendation systems and propose a self-supervised novel\nnoisy multi-view contrastive learning framework for improving top-K recommen-\ndation. In this paper, we propose a novel recommendation system architecture that\ngenerates three different views of user-item interactions for improved recommenda-\ntion along with a noise addition module. The global-level structural view leverages\nattention-based aggregation network Wang et al. (2019d) to capture collaborative\ninformation in the entity-item-user graph. In the item-item semantic view, we\nuse a K-nearest Neighbour item-item semantic module to incorporate semantic\nrelations among items. In the local view, we apply LightGCN He et al. (2020)\nwith noisy perturbations to generate robust user-item representations. We then use\ntwo more signals such as representation loss and uniformity loss in positive pairs\nto improve the quality of the representations and ensure uniform representations\nin the representational space. Experimental results on two benchmark datasets\ndemonstrate that our proposed method achieves superior performance compared\nto state-of-the-art methods. Additionally, we conducted extensive experiments\non CTR task-based datasets to demonstrate the robustness of our framework’s\ngeneralization in learning better user-item representations which can be seen in the\nsupplementary material. All the codes to generate reproducible results are available\nin this anonymous repository.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=er7VhmqZEA",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prosenjit_Biswas1",
        "name": "Prosenjit Biswas",
        "name_site": null,
        "openreview_id": "~Prosenjit_Biswas1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0001-6846-2354",
        "linkedin_url": "prosenjit-biswas-3b861554/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sony Research India (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "er7VhmqZEA",
      "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recommender systems have become an essential component of various online plat-\nforms, providing personalized recommendations to users. Collaborative filtering-\nbased methods, such as matrix factorization, have been widely used to capture\nlatent user-item preferences. Recently, graph-based methods have shown promising\nresults by modeling the interactions between users and items as a graph and lever-\naging knowledge graphs (KG) to learn the user and item embeddings. Motivated\nby the recent success of contrastive learning in mining supervised signals from data\nitself, in this paper, we focus on establishing a noisy contrastive learning framework\nin Knowledge-aware recommendation systems and propose a self-supervised novel\nnoisy multi-view contrastive learning framework for improving top-K recommen-\ndation. In this paper, we propose a novel recommendation system architecture that\ngenerates three different views of user-item interactions for improved recommenda-\ntion along with a noise addition module. The global-level structural view leverages\nattention-based aggregation network Wang et al. (2019d) to capture collaborative\ninformation in the entity-item-user graph. In the item-item semantic view, we\nuse a K-nearest Neighbour item-item semantic module to incorporate semantic\nrelations among items. In the local view, we apply LightGCN He et al. (2020)\nwith noisy perturbations to generate robust user-item representations. We then use\ntwo more signals such as representation loss and uniformity loss in positive pairs\nto improve the quality of the representations and ensure uniform representations\nin the representational space. Experimental results on two benchmark datasets\ndemonstrate that our proposed method achieves superior performance compared\nto state-of-the-art methods. Additionally, we conducted extensive experiments\non CTR task-based datasets to demonstrate the robustness of our framework’s\ngeneralization in learning better user-item representations which can be seen in the\nsupplementary material. All the codes to generate reproducible results are available\nin this anonymous repository.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=er7VhmqZEA",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Naoyuki_Onoe1",
        "name": "Naoyuki Onoe",
        "name_site": null,
        "openreview_id": "~Naoyuki_Onoe1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.sonyresearchindia.com/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0000-0002-8709-7241",
        "linkedin_url": "naoyuki-onoe-1923561a0/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Sony Research India Pvt. Ltd. (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fweSF6QplV",
      "title": "Structured Graph Reduction for Efficient GNN",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications.\nHowever, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, \n $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework’s efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=fweSF6QplV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manoj_Kumar4",
        "name": "Manoj Kumar",
        "name_site": null,
        "openreview_id": "~Manoj_Kumar4",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "gdL-bokAAAAJ",
        "orcid": null,
        "linkedin_url": "manoj-kumar-9042b449/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fweSF6QplV",
      "title": "Structured Graph Reduction for Efficient GNN",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications.\nHowever, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, \n $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework’s efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=fweSF6QplV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhanu_Halder1",
        "name": "Subhanu Halder",
        "name_site": null,
        "openreview_id": "~Subhanu_Halder1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "subhanuhalder777/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fweSF6QplV",
      "title": "Structured Graph Reduction for Efficient GNN",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications.\nHowever, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, \n $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework’s efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=fweSF6QplV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhishek_Gupta8",
        "name": "Abhishek Gupta",
        "name_site": null,
        "openreview_id": "~Abhishek_Gupta8",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "abhishek-gupta-063b62144/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fweSF6QplV",
      "title": "Structured Graph Reduction for Efficient GNN",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications.\nHowever, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, \n $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework’s efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=fweSF6QplV",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gSlLbfSOq1",
      "title": "Temporally Equivariant Contrastive Learning for Disease Progression",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Self-supervised contrastive learning methods provide robust representations by ensuring their invariance to different image transformations while simultaneously preventing representational collapse across different training samples. Equivariant contrastive learning, on the other hand, provides representations sensitive to specific image transformations while remaining invariant to others. By introducing equivariance to time-induced transformations, such as the anatomical changes in longitudinal medical images of a patient caused by disease progression, the model can effectively capture such changes in the representation space. However, learning temporally meaningful representations is challenging, as each patient's disease progresses at a different pace and manifests itself as different anatomical changes. In this work, we propose a Time-equivariant Contrastive Learning (TC) method. First, an encoder projects two unlabeled scans from different time points of the same patient to the representation space. Next, a temporal equivariance module is trained to predict the representation of a later visit based on the representation from one of the previous visits and from the time interval between them. Additionally, an invariance loss is applied to a projection of the representation space to encourage it to be robust to irrelevant image transformations such as translation, rotation, and noise. The representations learned with TC are not only sensitive to the progression of time but the temporal equivariant module can also be used to predict the representation for a given image at a future time-point. Our method has been evaluated on two longitudinal ophthalmic imaging datasets outperforming other state-of-the-art equivariant contrastive learning methods. Our method also showed a higher sensitivity to temporal ordering among the scans of each patient in comparison with the existing methods.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=gSlLbfSOq1",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Taha_Emre1",
        "name": "Taha Emre",
        "name_site": null,
        "openreview_id": "~Taha_Emre1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "gaAvQ4ogUX",
      "title": "A Convergent Federated Clustering Algorithm without Initial Condition",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple users to collaboratively train a shared model without exchanging their data with a central server. However, optimal models of different users often differ due to heterogeneity of data across users. \nIn this paper, we address the dichotomy between heterogeneous models and simultaneous training in FL via a clustering structure among the users.  The clustering framework is one way to allow for   high heterogeneity level between users, while users with similar data can still train a shared model. In this paper, we define a new clustering framework for FL based on the (optimal) local models of the users: two users belong to the same cluster if their local models are close. We propose an  algorithm, Successive Refine Federated Clustering Algorithm (SR-FCA), that treats each user as a singleton cluster as an initialization, and then successively refine the cluster estimation via exploiting similarity with other users. In any intermediate step, SR-FCA uses an  error-tolerant federated learning algorithm within each cluster to exploit simultaneous training and to correct clustering errors. Unlike some prominent prior works,  SR-FCA does not require any  good initialization (or warm start), both in theory and practice. We show that with proper choice of learning rate, SR-FCA incurs arbitrarily small clustering error. Additionally, SR-FCA does not require the knowledge of the number of clusters apriori like some prior works. We also validate the performance of our algorithm on real-world FL datasets including FEMNIST and Shakespeare in non-convex problems and show the benefits of SR-FCA over several baselines.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=gaAvQ4ogUX",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Vardhan1",
        "name": "Harsh Vardhan",
        "name_site": null,
        "openreview_id": "~Harsh_Vardhan1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "grQ97sPU5T",
      "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The application of convolution in graphs is at the core of Graph Neural Network (GNN) algorithms that led to the emergence of Graph Representation Learning (GRL). Various algorithms have been proposed over the last few years to solve the classical GRL task of node classification in a transductive setting. It is widely assumed that standard GNNs perform better on graphs with high homophily, i.e., nodes belonging to the same class are highly connected to each other. This assumption has led to the designing of specialised algorithms in the last few years for datasets that do not contain the property of homophily, i.e., heterophilic datasets. In this work, we both challenge and leverage this assumption. We argue that it is not necessary to follow the common trend of designing new algorithms but instead focus on understanding and enriching the data. We present a new technique from the perspective of data engineering that enables better performance on heterophilic datasets by both heterophilic GNN algorithms and non-heterophilic GNN algorithms. Our proposed technique, Spectral Highways, enables better connectivity and information flow between nodes in a heterophilic graph. We also draw an analogy between the performance of Spectral Highways and a recently proposed network property, i.e., Adjusted homophily. We conduct experiments on 11 baselines and 8 heterophilic datasets and achieve significant improvements in results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=grQ97sPU5T",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karan_Goyal1",
        "name": "Karan Goyal",
        "name_site": null,
        "openreview_id": "~Karan_Goyal1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "246/8470",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": null,
        "linkedin_url": "karan-goyal-757727a6/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "grQ97sPU5T",
      "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The application of convolution in graphs is at the core of Graph Neural Network (GNN) algorithms that led to the emergence of Graph Representation Learning (GRL). Various algorithms have been proposed over the last few years to solve the classical GRL task of node classification in a transductive setting. It is widely assumed that standard GNNs perform better on graphs with high homophily, i.e., nodes belonging to the same class are highly connected to each other. This assumption has led to the designing of specialised algorithms in the last few years for datasets that do not contain the property of homophily, i.e., heterophilic datasets. In this work, we both challenge and leverage this assumption. We argue that it is not necessary to follow the common trend of designing new algorithms but instead focus on understanding and enriching the data. We present a new technique from the perspective of data engineering that enables better performance on heterophilic datasets by both heterophilic GNN algorithms and non-heterophilic GNN algorithms. Our proposed technique, Spectral Highways, enables better connectivity and information flow between nodes in a heterophilic graph. We also draw an analogy between the performance of Spectral Highways and a recently proposed network property, i.e., Adjusted homophily. We conduct experiments on 11 baselines and 8 heterophilic datasets and achieve significant improvements in results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=grQ97sPU5T",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saankhya_Samanta1",
        "name": "Saankhya Samanta",
        "name_site": null,
        "openreview_id": "~Saankhya_Samanta1",
        "position": 2,
        "gender": null,
        "homepage_url": "https://www.linkedin.com/in/saankhya-samanta-62a007281/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "saankhya-samanta-62a007281/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Engineering Science and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "grQ97sPU5T",
      "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The application of convolution in graphs is at the core of Graph Neural Network (GNN) algorithms that led to the emergence of Graph Representation Learning (GRL). Various algorithms have been proposed over the last few years to solve the classical GRL task of node classification in a transductive setting. It is widely assumed that standard GNNs perform better on graphs with high homophily, i.e., nodes belonging to the same class are highly connected to each other. This assumption has led to the designing of specialised algorithms in the last few years for datasets that do not contain the property of homophily, i.e., heterophilic datasets. In this work, we both challenge and leverage this assumption. We argue that it is not necessary to follow the common trend of designing new algorithms but instead focus on understanding and enriching the data. We present a new technique from the perspective of data engineering that enables better performance on heterophilic datasets by both heterophilic GNN algorithms and non-heterophilic GNN algorithms. Our proposed technique, Spectral Highways, enables better connectivity and information flow between nodes in a heterophilic graph. We also draw an analogy between the performance of Spectral Highways and a recently proposed network property, i.e., Adjusted homophily. We conduct experiments on 11 baselines and 8 heterophilic datasets and achieve significant improvements in results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=grQ97sPU5T",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mukesh_Mohania1",
        "name": "Mukesh Mohania",
        "name_site": null,
        "openreview_id": "~Mukesh_Mohania1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.iiitd.ac.in/mukesh",
        "dblp_id": "85/6670.html",
        "google_scholar_url": "zgaqvNQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "grQ97sPU5T",
      "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The application of convolution in graphs is at the core of Graph Neural Network (GNN) algorithms that led to the emergence of Graph Representation Learning (GRL). Various algorithms have been proposed over the last few years to solve the classical GRL task of node classification in a transductive setting. It is widely assumed that standard GNNs perform better on graphs with high homophily, i.e., nodes belonging to the same class are highly connected to each other. This assumption has led to the designing of specialised algorithms in the last few years for datasets that do not contain the property of homophily, i.e., heterophilic datasets. In this work, we both challenge and leverage this assumption. We argue that it is not necessary to follow the common trend of designing new algorithms but instead focus on understanding and enriching the data. We present a new technique from the perspective of data engineering that enables better performance on heterophilic datasets by both heterophilic GNN algorithms and non-heterophilic GNN algorithms. Our proposed technique, Spectral Highways, enables better connectivity and information flow between nodes in a heterophilic graph. We also draw an analogy between the performance of Spectral Highways and a recently proposed network property, i.e., Adjusted homophily. We conduct experiments on 11 baselines and 8 heterophilic datasets and achieve significant improvements in results.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=grQ97sPU5T",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vikram_Goyal1",
        "name": "Vikram Goyal",
        "name_site": null,
        "openreview_id": "~Vikram_Goyal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.iiitd.ac.in/vikram/",
        "dblp_id": "70/6404.html",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=YJaVmSwAAAAJ",
        "orcid": "0000-0003-0769-6381",
        "linkedin_url": "vikram-goyal-7a684213/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hH5HK4hsLY",
      "title": "Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=hH5HK4hsLY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kaustubh_Olpadkar1",
        "name": "Kaustubh Olpadkar",
        "name_site": null,
        "openreview_id": "~Kaustubh_Olpadkar1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.3333333333333335,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iQHL76NqJT",
      "title": "Node-CwR: Node Classification with Reject Option",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Graph attention networks (GAT) have been state-of-the-art GNN architecture used as the backbone for various graph learning problems. One of the key tasks in graph learning is node classification. While several works cover multiple aspects of node classification, there has yet to be an attempt to understand the behaviour of GAT models for node classification with a reject option. This paper proposes a new approach called Node-CwR, which models node classification with a reject option using GAT. We offer both cost-based and coverage-based models to include the reject option in the node classification task. Cost-based models find the optimal classifier for a given cost of rejection value. Such models are trained by minimizing the rejection and misclassification rates on unrejected samples. Coverage-based methods take coverage as input and find the optimal model for a given coverage rate. We empirically evaluate our approaches on three benchmark datasets and show their effectiveness in learning efficient reject option models for node classification tasks. We observe that, in general, cost-based methods outperform coverage-based models for reject option. Additionally, our results include robust learning of node classifiers using label smoothing in the presence of label noise. We observe that label smoothing works well to handle label noise in cost-based models, while it works adversely in coverage-based models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=iQHL76NqJT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Uday_Bhaskar_Kuchipudi1",
        "name": "Uday Bhaskar Kuchipudi",
        "name_site": null,
        "openreview_id": "~Uday_Bhaskar_Kuchipudi1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/BhaskarSteve",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iQHL76NqJT",
      "title": "Node-CwR: Node Classification with Reject Option",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Graph attention networks (GAT) have been state-of-the-art GNN architecture used as the backbone for various graph learning problems. One of the key tasks in graph learning is node classification. While several works cover multiple aspects of node classification, there has yet to be an attempt to understand the behaviour of GAT models for node classification with a reject option. This paper proposes a new approach called Node-CwR, which models node classification with a reject option using GAT. We offer both cost-based and coverage-based models to include the reject option in the node classification task. Cost-based models find the optimal classifier for a given cost of rejection value. Such models are trained by minimizing the rejection and misclassification rates on unrejected samples. Coverage-based methods take coverage as input and find the optimal model for a given coverage rate. We empirically evaluate our approaches on three benchmark datasets and show their effectiveness in learning efficient reject option models for node classification tasks. We observe that, in general, cost-based methods outperform coverage-based models for reject option. Additionally, our results include robust learning of node classifiers using label smoothing in the presence of label noise. We observe that label smoothing works well to handle label noise in cost-based models, while it works adversely in coverage-based models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=iQHL76NqJT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Charu_Sharma2",
        "name": "Charu Sharma",
        "name_site": null,
        "openreview_id": "~Charu_Sharma2",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://charusharma.org/",
        "dblp_id": "202/8640",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=bftN0M0AAAAJ",
        "orcid": "0000-0003-2518-5008",
        "linkedin_url": "shcharu/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "iQHL76NqJT",
      "title": "Node-CwR: Node Classification with Reject Option",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Graph attention networks (GAT) have been state-of-the-art GNN architecture used as the backbone for various graph learning problems. One of the key tasks in graph learning is node classification. While several works cover multiple aspects of node classification, there has yet to be an attempt to understand the behaviour of GAT models for node classification with a reject option. This paper proposes a new approach called Node-CwR, which models node classification with a reject option using GAT. We offer both cost-based and coverage-based models to include the reject option in the node classification task. Cost-based models find the optimal classifier for a given cost of rejection value. Such models are trained by minimizing the rejection and misclassification rates on unrejected samples. Coverage-based methods take coverage as input and find the optimal model for a given coverage rate. We empirically evaluate our approaches on three benchmark datasets and show their effectiveness in learning efficient reject option models for node classification tasks. We observe that, in general, cost-based methods outperform coverage-based models for reject option. Additionally, our results include robust learning of node classifiers using label smoothing in the presence of label noise. We observe that label smoothing works well to handle label noise in cost-based models, while it works adversely in coverage-based models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=iQHL76NqJT",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Darshana_Saravanan1",
        "name": "Darshana Saravanan",
        "name_site": null,
        "openreview_id": "~Naresh_Manwani1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/nareshmanwani/home",
        "dblp_id": "17/2536",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=qz4eDmgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.0,
        "rating_std": 0.0,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "j7S7o6ROn9",
      "title": "Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent literature introduced the notion of distributional structured pruning (DSP) in Deep Neural Networks by retaining discriminative filters that can effectively differentiate between classes.  Crucial to  DSP is the ability to estimate the discriminative ability of a filter, which is defined by the minimum pairwise Total Variation (TV) distance between the class-conditional feature distributions. Since the computation of TV distance is generally intractable, existing literature assumes the class-conditional feature distributions are Gaussian, thereby enabling the use of the tractable Hellinger lower bound to estimate discriminative ability. However, the Gaussian assumption is not only restrictive but also does not typically hold. In this work, we address this gap by deriving a lower bound on TV Distance which depends only on the moments of witness functions. Using linear witness functions, the bound establishes new relationships between the TV Distance and well-known discriminant-based classifiers, such as Fisher Discriminants and Minimax Probability machines. The lower bounds are used to produce a variety of pruning algorithms called WitnessPrune by varying the choice of witness function. We empirically show that we can achieve up to 7\\% greater accuracy for similar sparsity in hard-to-prune layers using a polynomial witness function as compared to the state-of-the-art.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=j7S7o6ROn9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chaitanya_Murti1",
        "name": "Chaitanya Murti",
        "name_site": null,
        "openreview_id": "~Chaitanya_Murti1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "j7S7o6ROn9",
      "title": "Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent literature introduced the notion of distributional structured pruning (DSP) in Deep Neural Networks by retaining discriminative filters that can effectively differentiate between classes.  Crucial to  DSP is the ability to estimate the discriminative ability of a filter, which is defined by the minimum pairwise Total Variation (TV) distance between the class-conditional feature distributions. Since the computation of TV distance is generally intractable, existing literature assumes the class-conditional feature distributions are Gaussian, thereby enabling the use of the tractable Hellinger lower bound to estimate discriminative ability. However, the Gaussian assumption is not only restrictive but also does not typically hold. In this work, we address this gap by deriving a lower bound on TV Distance which depends only on the moments of witness functions. Using linear witness functions, the bound establishes new relationships between the TV Distance and well-known discriminant-based classifiers, such as Fisher Discriminants and Minimax Probability machines. The lower bounds are used to produce a variety of pruning algorithms called WitnessPrune by varying the choice of witness function. We empirically show that we can achieve up to 7\\% greater accuracy for similar sparsity in hard-to-prune layers using a polynomial witness function as compared to the state-of-the-art.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=j7S7o6ROn9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.4142135623730951,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jFiFmHrIfD",
      "title": "Explorative Latent Self-Supervised Active Search Algorithm (ELSA)",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In computer vision, attaining exceptional performance often necessitates access to large labeled datasets. The creation of extensive datasets through manual annotation is not only cost-prohibitive but also practically infeasible due to the scarcity of positive samples in imbalanced datasets where negative samples dominate. To tackle this intricate problem, we introduce Efficient Latent Space-based Self-Supervised Active Learning Search (ELSA), an active learning-based labeling assistant. ELSA distinguishes itself from existing interactive annotation methods by focusing exclusively on positive class labeling in massively imbalanced datasets replete with a substantial number of negative samples. Through the automatic exclusion of the majority of negative samples, ELSA achieves a remarkable level of precision and accuracy in its search. This novel framework comprises three fundamental components: a)an iterative Nearest Neighbor Search, b)a Sophisticated Random Sampler, c)a Linear Head powered by Active Learning. Our comprehensive study provides insights into the interplay of these components and their collective impact on search efficiency. Notably, we demonstrate that ELSA achieves orders of magnitude superior performance, in average starting with as little as 5 or less positive samples in ImageNet 1k we managed to detect as much as 80\\% of all the examples belonging to that class by only labeling as little as 0.67\\% of the entire dataset manually.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=jFiFmHrIfD",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Nath1",
        "name": "Aniket Nath",
        "name_site": null,
        "openreview_id": "~Aniket_Nath1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://aniket-nath.github.io/portfolio/",
        "dblp_id": null,
        "google_scholar_url": "oD-kyloAAAAJ",
        "orcid": "0000-0002-9429-8973",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 1.8856180831641267,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jFiFmHrIfD",
      "title": "Explorative Latent Self-Supervised Active Search Algorithm (ELSA)",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In computer vision, attaining exceptional performance often necessitates access to large labeled datasets. The creation of extensive datasets through manual annotation is not only cost-prohibitive but also practically infeasible due to the scarcity of positive samples in imbalanced datasets where negative samples dominate. To tackle this intricate problem, we introduce Efficient Latent Space-based Self-Supervised Active Learning Search (ELSA), an active learning-based labeling assistant. ELSA distinguishes itself from existing interactive annotation methods by focusing exclusively on positive class labeling in massively imbalanced datasets replete with a substantial number of negative samples. Through the automatic exclusion of the majority of negative samples, ELSA achieves a remarkable level of precision and accuracy in its search. This novel framework comprises three fundamental components: a)an iterative Nearest Neighbor Search, b)a Sophisticated Random Sampler, c)a Linear Head powered by Active Learning. Our comprehensive study provides insights into the interplay of these components and their collective impact on search efficiency. Notably, we demonstrate that ELSA achieves orders of magnitude superior performance, in average starting with as little as 5 or less positive samples in ImageNet 1k we managed to detect as much as 80\\% of all the examples belonging to that class by only labeling as little as 0.67\\% of the entire dataset manually.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=jFiFmHrIfD",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Diptarko_Choudhury1",
        "name": "Diptarko Choudhury",
        "name_site": null,
        "openreview_id": "~Diptarko_Choudhury1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0009-0001-8105-4718",
        "linkedin_url": "diptarko-choudhury-371518205/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 1.8856180831641267,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jFiFmHrIfD",
      "title": "Explorative Latent Self-Supervised Active Search Algorithm (ELSA)",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In computer vision, attaining exceptional performance often necessitates access to large labeled datasets. The creation of extensive datasets through manual annotation is not only cost-prohibitive but also practically infeasible due to the scarcity of positive samples in imbalanced datasets where negative samples dominate. To tackle this intricate problem, we introduce Efficient Latent Space-based Self-Supervised Active Learning Search (ELSA), an active learning-based labeling assistant. ELSA distinguishes itself from existing interactive annotation methods by focusing exclusively on positive class labeling in massively imbalanced datasets replete with a substantial number of negative samples. Through the automatic exclusion of the majority of negative samples, ELSA achieves a remarkable level of precision and accuracy in its search. This novel framework comprises three fundamental components: a)an iterative Nearest Neighbor Search, b)a Sophisticated Random Sampler, c)a Linear Head powered by Active Learning. Our comprehensive study provides insights into the interplay of these components and their collective impact on search efficiency. Notably, we demonstrate that ELSA achieves orders of magnitude superior performance, in average starting with as little as 5 or less positive samples in ImageNet 1k we managed to detect as much as 80\\% of all the examples belonging to that class by only labeling as little as 0.67\\% of the entire dataset manually.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=jFiFmHrIfD",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhankar_Mishra1",
        "name": "Subhankar Mishra",
        "name_site": null,
        "openreview_id": "~Subhankar_Mishra1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.niser.ac.in/~smishra/",
        "dblp_id": "147/8391",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 1.8856180831641267,
        "confidence_mean": 3.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kxLMnvnZv0",
      "title": "CoMNet: Where Biology Meets ConvNets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Designing ConvNet and exploring its design space is a highly challenging research\narea. In this paper, inspired by the structural organization of cortical modules in the\nbiological visual cortex, we present a pragmatically designed ConvNet architecture,\ncalled CoMNet which is simplified yet powerful. The bio-inspired design of CoM-\nNet offers efficiency in multiple dimensions such as network depth, parameters,\nFLOPs, latency, branching, and memory budget at once while having a simple\ndesign space, in contrast to the existing designs which are limited only to fewer\ndimensions. We also develop a Multi-Dimensional Efficiency (MDE) evaluation\nprotocol to compare models across dimensions. Our comprehensive evaluations\nshow that in the MDE setting, CoMNet outperforms many representative ConvNet\ndesigns such as ResNet, ResNeXt, RegNet, RepVGG, and ParNet (Figure 1).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=kxLMnvnZv0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar2",
        "name": "Ashish Kumar",
        "name_site": null,
        "openreview_id": "~Ashish_Kumar2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ashishkumar822.github.io",
        "dblp_id": "34/5378-6",
        "google_scholar_url": "n-oRDEYAAAAJ",
        "orcid": null,
        "linkedin_url": "ashishkumar822/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "ScorelabsAI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kxLMnvnZv0",
      "title": "CoMNet: Where Biology Meets ConvNets",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Designing ConvNet and exploring its design space is a highly challenging research\narea. In this paper, inspired by the structural organization of cortical modules in the\nbiological visual cortex, we present a pragmatically designed ConvNet architecture,\ncalled CoMNet which is simplified yet powerful. The bio-inspired design of CoM-\nNet offers efficiency in multiple dimensions such as network depth, parameters,\nFLOPs, latency, branching, and memory budget at once while having a simple\ndesign space, in contrast to the existing designs which are limited only to fewer\ndimensions. We also develop a Multi-Dimensional Efficiency (MDE) evaluation\nprotocol to compare models across dimensions. Our comprehensive evaluations\nshow that in the MDE setting, CoMNet outperforms many representative ConvNet\ndesigns such as ResNet, ResNeXt, RegNet, RepVGG, and ParNet (Figure 1).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=kxLMnvnZv0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Laxmidhar_Behera1",
        "name": "Laxmidhar Behera",
        "name_site": null,
        "openreview_id": "~Laxmidhar_Behera1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://home.iitk.ac.in/~lbehera/",
        "dblp_id": "14/1412",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=QWTcyP8AAAAJ",
        "orcid": null,
        "linkedin_url": "laxmidhar-behera-a74a5b174/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lgA84TbHxm",
      "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc.,the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention on improving the performance of InfoNCE loss in Self-supervised learning by proposing a novel cosine-similarity dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We also provide mathematical analyses to support the construction of such a dynamically scaled temperature function. Experimental evidence shows that the proposed framework outperforms the contrastive loss-based SSL algorithms. Our code is available at https://www.github.com/subanon/dystress.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lgA84TbHxm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siladittya_Manna1",
        "name": "Siladittya Manna",
        "name_site": null,
        "openreview_id": "~Siladittya_Manna1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sadimanna.github.io",
        "dblp_id": "270/2011",
        "google_scholar_url": "6V9sqi0AAAAJ",
        "orcid": "0000-0001-6364-8654",
        "linkedin_url": "siladittya-manna-063939a0/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lgA84TbHxm",
      "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc.,the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention on improving the performance of InfoNCE loss in Self-supervised learning by proposing a novel cosine-similarity dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We also provide mathematical analyses to support the construction of such a dynamically scaled temperature function. Experimental evidence shows that the proposed framework outperforms the contrastive loss-based SSL algorithms. Our code is available at https://www.github.com/subanon/dystress.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lgA84TbHxm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rakesh_Dey1",
        "name": "Rakesh Dey",
        "name_site": null,
        "openreview_id": "~Rakesh_Dey1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "354/6521",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "rakesh-dey-161149179/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lgA84TbHxm",
      "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc.,the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention on improving the performance of InfoNCE loss in Self-supervised learning by proposing a novel cosine-similarity dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We also provide mathematical analyses to support the construction of such a dynamically scaled temperature function. Experimental evidence shows that the proposed framework outperforms the contrastive loss-based SSL algorithms. Our code is available at https://www.github.com/subanon/dystress.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lgA84TbHxm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saumik_Bhattacharya1",
        "name": "Saumik Bhattacharya",
        "name_site": null,
        "openreview_id": "~Saumik_Bhattacharya1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.iitkgp.ac.in/department/EC/faculty/ec-saumik",
        "dblp_id": "154/8318.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=8pffuA4AAAAJ",
        "orcid": "0000-0003-1273-7969",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lgA84TbHxm",
      "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc.,the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention on improving the performance of InfoNCE loss in Self-supervised learning by proposing a novel cosine-similarity dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We also provide mathematical analyses to support the construction of such a dynamically scaled temperature function. Experimental evidence shows that the proposed framework outperforms the contrastive loss-based SSL algorithms. Our code is available at https://www.github.com/subanon/dystress.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lgA84TbHxm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Umapada_Pal1",
        "name": "Umapada Pal",
        "name_site": null,
        "openreview_id": "~Umapada_Pal1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.isical.ac.in/~umapada/",
        "dblp_id": "p/UmapadaPal",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=2_z_CogAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lxlMFlzZO9",
      "title": "DS-Prover: A Dynamic Sampling Based Approach for Neural Theorem Proving",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also study the effect of augmenting the training dataset by decomposing simplification and rewrite tactics with multiple premises into tactics with single premises. This gives the model more examples to learn from and helps it to predict the tactics with premises more accurately. We perform our experiments using the Mathlib dataset of the Lean theorem prover and report the performance on two standard datasets, MiniF2F and ProofNet. Our methods achieve significant performance gains on both datasets. We achieve a new state-of-the-art performance of 30.6% on MiniF2F using Lean, and a performance of 13.65% on ProofNet, which is comparable to the state-of-the-art.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lxlMFlzZO9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rahul_Vishwakarma2",
        "name": "Rahul Vishwakarma",
        "name_site": null,
        "openreview_id": "~Rahul_Vishwakarma2",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "GVG1s7YAAAAJ",
        "orcid": null,
        "linkedin_url": "rahul3613/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lxlMFlzZO9",
      "title": "DS-Prover: A Dynamic Sampling Based Approach for Neural Theorem Proving",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also study the effect of augmenting the training dataset by decomposing simplification and rewrite tactics with multiple premises into tactics with single premises. This gives the model more examples to learn from and helps it to predict the tactics with premises more accurately. We perform our experiments using the Mathlib dataset of the Lean theorem prover and report the performance on two standard datasets, MiniF2F and ProofNet. Our methods achieve significant performance gains on both datasets. We achieve a new state-of-the-art performance of 30.6% on MiniF2F using Lean, and a performance of 13.65% on ProofNet, which is comparable to the state-of-the-art.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=lxlMFlzZO9",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Subhankar_Mishra1",
        "name": "Subhankar Mishra",
        "name_site": null,
        "openreview_id": "~Subhankar_Mishra1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.niser.ac.in/~smishra/",
        "dblp_id": "147/8391",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Science Education and Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mFBR2ksIwY",
      "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges due to partial observability and emergent behavior. Directly transferring online credit assignment method to offline settings results in suboptimal outcomes due to the absence of real-time feedback and intricate agent interactions. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to seamlessly integrate with various offline MARL methods. Theoretically, we proved that under the setting of offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. Experimentally, we tested MACCA in three environments, including discrete and continuous action settings. The results show that MACCA outperforms SOTA methods and improves performance upon their backbones.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=mFBR2ksIwY",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Eindhoven University of Technology (Netherlands)",
        "countries": [
          "Netherlands"
        ],
        "country_codes": [
          "NL"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nKYTiJhhAu",
      "title": "REDUCR: Robust Data Downsampling Using Class Priority Reweighting",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion. To reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints. However, these techniques can suffer from poor worst-class generalization performance due to class imbalance and distributional shifts. This work introduces REDUCR, a robust and efficient data downsampling method that uses class priority reweighting. REDUCR reduces the training data while preserving worst-class generalization performance. REDUCR assigns priority weights to datapoints in a class-aware manner using an online learning algorithm. We demonstrate the data efficiency and robust performance of REDUCR on vision and text classification tasks. On web-scraped datasets with imbalanced class distributions, REDUCR achieves significant test accuracy boosts for the worst-performing class (but also on average), surpassing state-of-the-art methods by around 15\\%.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=nKYTiJhhAu",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ilija_Bogunovic2",
        "name": "Ilija Bogunovic",
        "name_site": null,
        "openreview_id": "~Ilija_Bogunovic1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://ilijabogunovic.com/",
        "dblp_id": "142/2725",
        "google_scholar_url": "xMvt3NEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "p5jBLcVmhe",
      "title": "SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Expansion",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Policy gradient methods suffer from large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze the gradient variance of SoftTreeMax and reveal for the first time how tree expansion helps reduce this variance. We prove that the variance decays exponentially with the planning horizon as a function of the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the faster the decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance decay. Ours is the first result to bound the gradient bias with an approximate model. In a practical implementation of SoftTreeMax, we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=p5jBLcVmhe",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gugan_Thoppe1",
        "name": "Gugan Thoppe",
        "name_site": null,
        "openreview_id": "~Gugan_Thoppe1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "117/3710",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=X5zV3s8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDhsRC0ldq",
      "title": "Do LLMs understand Pragmatics? An Extensive Benchmark for Evaluating Pragmatic Understanding of LLMs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language models (LLMs) are typically evaluated based on semantic understanding and are believed to be capable of handling general language processing. While LLMs can mimic human-like responses, they still are a contraption in their pragmatic or contextual understanding of language. To test this hypothesis, we subject LLMs to the complex task of pragmatics. We conducted evaluation across \\textit{fourteen} tasks spanning \\textit{four} domains of pragmatics namely, Implicature, Presupposition, Reference, and Deixis. For each task, we curated high-quality test sets, consisting of Multiple Choice Question Answers (MCQA). We evaluate a wide range of LLMs with different types and sizes. Our findings reveal that LLMs with no instruction fine-tuning have near-random accuracy on many tasks. The performance gradually increases with the increase in model capacity. Additionally, we create a unified benchmark enabling the research community to better assess the underlying pragmatic understanding of the language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rDhsRC0ldq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Settaluri_Lakshmi_Sravanthi1",
        "name": "Settaluri Lakshmi Sravanthi",
        "name_site": null,
        "openreview_id": "~Settaluri_Lakshmi_Sravanthi1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "sravanthi-settaluri-83a48714/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDhsRC0ldq",
      "title": "Do LLMs understand Pragmatics? An Extensive Benchmark for Evaluating Pragmatic Understanding of LLMs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language models (LLMs) are typically evaluated based on semantic understanding and are believed to be capable of handling general language processing. While LLMs can mimic human-like responses, they still are a contraption in their pragmatic or contextual understanding of language. To test this hypothesis, we subject LLMs to the complex task of pragmatics. We conducted evaluation across \\textit{fourteen} tasks spanning \\textit{four} domains of pragmatics namely, Implicature, Presupposition, Reference, and Deixis. For each task, we curated high-quality test sets, consisting of Multiple Choice Question Answers (MCQA). We evaluate a wide range of LLMs with different types and sizes. Our findings reveal that LLMs with no instruction fine-tuning have near-random accuracy on many tasks. The performance gradually increases with the increase in model capacity. Additionally, we create a unified benchmark enabling the research community to better assess the underlying pragmatic understanding of the language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rDhsRC0ldq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Meet_Doshi1",
        "name": "Meet Doshi",
        "name_site": null,
        "openreview_id": "~Meet_Doshi1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "362/4924",
        "google_scholar_url": "Nbg-JiMAAAAJ",
        "orcid": null,
        "linkedin_url": "meetdoshi90/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDhsRC0ldq",
      "title": "Do LLMs understand Pragmatics? An Extensive Benchmark for Evaluating Pragmatic Understanding of LLMs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language models (LLMs) are typically evaluated based on semantic understanding and are believed to be capable of handling general language processing. While LLMs can mimic human-like responses, they still are a contraption in their pragmatic or contextual understanding of language. To test this hypothesis, we subject LLMs to the complex task of pragmatics. We conducted evaluation across \\textit{fourteen} tasks spanning \\textit{four} domains of pragmatics namely, Implicature, Presupposition, Reference, and Deixis. For each task, we curated high-quality test sets, consisting of Multiple Choice Question Answers (MCQA). We evaluate a wide range of LLMs with different types and sizes. Our findings reveal that LLMs with no instruction fine-tuning have near-random accuracy on many tasks. The performance gradually increases with the increase in model capacity. Additionally, we create a unified benchmark enabling the research community to better assess the underlying pragmatic understanding of the language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rDhsRC0ldq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pavan_Kalyan_Tankala1",
        "name": "Pavan Kalyan Tankala",
        "name_site": null,
        "openreview_id": "~Pavan_Kalyan_Tankala1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "371/5125",
        "google_scholar_url": "11_GsJAAAAAJ",
        "orcid": null,
        "linkedin_url": "pavan-kalyan-1b88351a0/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDhsRC0ldq",
      "title": "Do LLMs understand Pragmatics? An Extensive Benchmark for Evaluating Pragmatic Understanding of LLMs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language models (LLMs) are typically evaluated based on semantic understanding and are believed to be capable of handling general language processing. While LLMs can mimic human-like responses, they still are a contraption in their pragmatic or contextual understanding of language. To test this hypothesis, we subject LLMs to the complex task of pragmatics. We conducted evaluation across \\textit{fourteen} tasks spanning \\textit{four} domains of pragmatics namely, Implicature, Presupposition, Reference, and Deixis. For each task, we curated high-quality test sets, consisting of Multiple Choice Question Answers (MCQA). We evaluate a wide range of LLMs with different types and sizes. Our findings reveal that LLMs with no instruction fine-tuning have near-random accuracy on many tasks. The performance gradually increases with the increase in model capacity. Additionally, we create a unified benchmark enabling the research community to better assess the underlying pragmatic understanding of the language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rDhsRC0ldq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rudra_Murthy1",
        "name": "Rudra Murthy",
        "name_site": null,
        "openreview_id": "~Rudra_Murthy1",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://murthyrudra.github.io",
        "dblp_id": "216/7282",
        "google_scholar_url": "5bjj_9cAAAAJ",
        "orcid": "0000-0002-6236-1931",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "IBM (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDhsRC0ldq",
      "title": "Do LLMs understand Pragmatics? An Extensive Benchmark for Evaluating Pragmatic Understanding of LLMs",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Large language models (LLMs) are typically evaluated based on semantic understanding and are believed to be capable of handling general language processing. While LLMs can mimic human-like responses, they still are a contraption in their pragmatic or contextual understanding of language. To test this hypothesis, we subject LLMs to the complex task of pragmatics. We conducted evaluation across \\textit{fourteen} tasks spanning \\textit{four} domains of pragmatics namely, Implicature, Presupposition, Reference, and Deixis. For each task, we curated high-quality test sets, consisting of Multiple Choice Question Answers (MCQA). We evaluate a wide range of LLMs with different types and sizes. Our findings reveal that LLMs with no instruction fine-tuning have near-random accuracy on many tasks. The performance gradually increases with the increase in model capacity. Additionally, we create a unified benchmark enabling the research community to better assess the underlying pragmatic understanding of the language models.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rDhsRC0ldq",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pushpak_Bhattacharyya1",
        "name": "Pushpak Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Pushpak_Bhattacharyya1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~pb/",
        "dblp_id": "p/PushpakBhattacharyya",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=vvg-pAkAAAAJ",
        "orcid": null,
        "linkedin_url": "pushpakbh/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 0.0,
        "rating_std": 0.0,
        "confidence_mean": 0.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rO62BY3dYc",
      "title": "Pruning via Ranking (PvR): A unified structured pruning approach",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The increase in width and depth has facilitated neural networks to learn from large amounts of data leading to state-of-the-art results in both vision and NLP tasks. \n    In order to democratize such massive networks, it is important to deploy them on resource-limited devices through model compression techniques such as structured pruning. \n    Unfortunately, most pruning methods are tailored towards compressing specific models due to widely differing network architectures for distinct tasks. \n    At the same time, it is desirable for pruning algorithms to generate optimal subnetworks according to user-specified parameter budgets.\n    In this work, we propose Pruning via Ranking (PvR), a novel, global structured pruning approach which generates dense sub-networks that comply with any user-supplied parameter budget. \n    PvR consists of a grouping module and a ranking module that are used to generate smaller networks in terms of both function composition as well as network width for a given dataset. \n    The smaller networks are then trained from scratch instead of being fine-tuned as we empirically demonstrate using a recently proposed model complexity measure that re-initialization after pruning followed by re-training results in better performance. \n    We compare our method against multiple pruning approaches on benchmark datasets, namely, CIFAR10, Tiny ImageNet and IMDB 50K movie reviews, with standard models, namely, VGG16, ResNet34 and Bert-base-uncased. \n    We use both accuracy and model inference latency metrics to evaluate the performance of each approach. \n    The smaller networks proposed by PvR for a range of parameter budgets when trained from scratch outperform all other methods across all datasets and models. \n    In fact, our recommended sub-networks with fewer layers achieve less than $1$\\% test accuracy drop even after pruning $90$\\% of the original model across all networks and datasets while enjoying lower inference latency due to reduced depth.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rO62BY3dYc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Atif_Hassan1",
        "name": "Atif Hassan",
        "name_site": null,
        "openreview_id": "~Atif_Hassan1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=GW_sRXMAAAAJ",
        "orcid": "0000-0002-2699-9102",
        "linkedin_url": "atif-hassan-1a8a45127/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rO62BY3dYc",
      "title": "Pruning via Ranking (PvR): A unified structured pruning approach",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The increase in width and depth has facilitated neural networks to learn from large amounts of data leading to state-of-the-art results in both vision and NLP tasks. \n    In order to democratize such massive networks, it is important to deploy them on resource-limited devices through model compression techniques such as structured pruning. \n    Unfortunately, most pruning methods are tailored towards compressing specific models due to widely differing network architectures for distinct tasks. \n    At the same time, it is desirable for pruning algorithms to generate optimal subnetworks according to user-specified parameter budgets.\n    In this work, we propose Pruning via Ranking (PvR), a novel, global structured pruning approach which generates dense sub-networks that comply with any user-supplied parameter budget. \n    PvR consists of a grouping module and a ranking module that are used to generate smaller networks in terms of both function composition as well as network width for a given dataset. \n    The smaller networks are then trained from scratch instead of being fine-tuned as we empirically demonstrate using a recently proposed model complexity measure that re-initialization after pruning followed by re-training results in better performance. \n    We compare our method against multiple pruning approaches on benchmark datasets, namely, CIFAR10, Tiny ImageNet and IMDB 50K movie reviews, with standard models, namely, VGG16, ResNet34 and Bert-base-uncased. \n    We use both accuracy and model inference latency metrics to evaluate the performance of each approach. \n    The smaller networks proposed by PvR for a range of parameter budgets when trained from scratch outperform all other methods across all datasets and models. \n    In fact, our recommended sub-networks with fewer layers achieve less than $1$\\% test accuracy drop even after pruning $90$\\% of the original model across all networks and datasets while enjoying lower inference latency due to reduced depth.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rO62BY3dYc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swanand_Khare1",
        "name": "Swanand Khare",
        "name_site": null,
        "openreview_id": "~Swanand_Khare1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.iitkgp.ac.in/department/MA/faculty/ma-srkhare",
        "dblp_id": "58/10796.html",
        "google_scholar_url": "tnzbgNwAAAAJ",
        "orcid": "0000-0002-1175-8015",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rO62BY3dYc",
      "title": "Pruning via Ranking (PvR): A unified structured pruning approach",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "The increase in width and depth has facilitated neural networks to learn from large amounts of data leading to state-of-the-art results in both vision and NLP tasks. \n    In order to democratize such massive networks, it is important to deploy them on resource-limited devices through model compression techniques such as structured pruning. \n    Unfortunately, most pruning methods are tailored towards compressing specific models due to widely differing network architectures for distinct tasks. \n    At the same time, it is desirable for pruning algorithms to generate optimal subnetworks according to user-specified parameter budgets.\n    In this work, we propose Pruning via Ranking (PvR), a novel, global structured pruning approach which generates dense sub-networks that comply with any user-supplied parameter budget. \n    PvR consists of a grouping module and a ranking module that are used to generate smaller networks in terms of both function composition as well as network width for a given dataset. \n    The smaller networks are then trained from scratch instead of being fine-tuned as we empirically demonstrate using a recently proposed model complexity measure that re-initialization after pruning followed by re-training results in better performance. \n    We compare our method against multiple pruning approaches on benchmark datasets, namely, CIFAR10, Tiny ImageNet and IMDB 50K movie reviews, with standard models, namely, VGG16, ResNet34 and Bert-base-uncased. \n    We use both accuracy and model inference latency metrics to evaluate the performance of each approach. \n    The smaller networks proposed by PvR for a range of parameter budgets when trained from scratch outperform all other methods across all datasets and models. \n    In fact, our recommended sub-networks with fewer layers achieve less than $1$\\% test accuracy drop even after pruning $90$\\% of the original model across all networks and datasets while enjoying lower inference latency due to reduced depth.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rO62BY3dYc",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jiaul_H._Paik1",
        "name": "Jiaul H. Paik",
        "name_site": null,
        "openreview_id": "~Jiaul_H._Paik1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.iitkgp.ac.in/department/AI/faculty/ai-jiaul",
        "dblp_id": "27/9831",
        "google_scholar_url": "https://scholar.google.com.tw/citations?hl=en",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.75,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rjLgCkJH79",
      "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lead optimization in drug discovery is a pivotal phase in identifying promising drug candidates for further development. Traditionally, lead optimization in the machine learning community has been treated as a constraint optimization problem where methods like generative models and reinforcement learning(RL) have been widely employed. However, these methods often rely on molecular similarity metrics to define constraints, which poses significant challenges due to the inherently ambiguous nature of molecular similarity. In this work, we present a similarity-agnostic approach to lead optimization, which we term \"Lead Optimization using Goal-conditioned Reinforcement Learning\" or LOGRL. Contrary to conventional methods, LOGRL is uniquely trained on a distinct task: source-to-target path prediction. This allows LOGRL to produce molecules with significantly higher Tanimoto similarity to target molecules, even without direct exposure to this metric during training. Furthermore, we incorporate a beam search strategy during the molecule generation process. This strategy empowers us to generate a substantial number of candidate molecules, facilitating further curation to meet desired properties. Notably, our unique approach permits us to leverage the Euclidean distance between learned action representations as a surrogate for molecular similarity during beam search.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rjLgCkJH79",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sean_Current1",
        "name": "Sean Current",
        "name_site": null,
        "openreview_id": "~Sean_Current1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "290/8720",
        "google_scholar_url": "UtFflEMAAAAJ",
        "orcid": "0000-0002-3510-6919",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rjLgCkJH79",
      "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lead optimization in drug discovery is a pivotal phase in identifying promising drug candidates for further development. Traditionally, lead optimization in the machine learning community has been treated as a constraint optimization problem where methods like generative models and reinforcement learning(RL) have been widely employed. However, these methods often rely on molecular similarity metrics to define constraints, which poses significant challenges due to the inherently ambiguous nature of molecular similarity. In this work, we present a similarity-agnostic approach to lead optimization, which we term \"Lead Optimization using Goal-conditioned Reinforcement Learning\" or LOGRL. Contrary to conventional methods, LOGRL is uniquely trained on a distinct task: source-to-target path prediction. This allows LOGRL to produce molecules with significantly higher Tanimoto similarity to target molecules, even without direct exposure to this metric during training. Furthermore, we incorporate a beam search strategy during the molecule generation process. This strategy empowers us to generate a substantial number of candidate molecules, facilitating further curation to meet desired properties. Notably, our unique approach permits us to leverage the Euclidean distance between learned action representations as a surrogate for molecular similarity during beam search.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rjLgCkJH79",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Balaraman_Ravindran1",
        "name": "Balaraman Ravindran",
        "name_site": null,
        "openreview_id": "~Balaraman_Ravindran1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~ravi",
        "dblp_id": "69/2281",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-5364-7639",
        "linkedin_url": "ravindran-balaraman-427a307",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rjLgCkJH79",
      "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Lead optimization in drug discovery is a pivotal phase in identifying promising drug candidates for further development. Traditionally, lead optimization in the machine learning community has been treated as a constraint optimization problem where methods like generative models and reinforcement learning(RL) have been widely employed. However, these methods often rely on molecular similarity metrics to define constraints, which poses significant challenges due to the inherently ambiguous nature of molecular similarity. In this work, we present a similarity-agnostic approach to lead optimization, which we term \"Lead Optimization using Goal-conditioned Reinforcement Learning\" or LOGRL. Contrary to conventional methods, LOGRL is uniquely trained on a distinct task: source-to-target path prediction. This allows LOGRL to produce molecules with significantly higher Tanimoto similarity to target molecules, even without direct exposure to this metric during training. Furthermore, we incorporate a beam search strategy during the molecule generation process. This strategy empowers us to generate a substantial number of candidate molecules, facilitating further curation to meet desired properties. Notably, our unique approach permits us to leverage the Euclidean distance between learned action representations as a surrogate for molecular similarity during beam search.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rjLgCkJH79",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~srinivasan_parthasarathy1",
        "name": "srinivasan parthasarathy",
        "name_site": null,
        "openreview_id": "~srinivasan_parthasarathy1",
        "position": 5,
        "gender": null,
        "homepage_url": "https://web.cse.ohio-state.edu/~parthasarathy.2/",
        "dblp_id": "p/SParathasarathy.html",
        "google_scholar_url": "2mjUsP8AAAAJ",
        "orcid": "0000-0002-6062-6449",
        "linkedin_url": "srinivasan-parthasarathy-5703761/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rleZtn5OqJ",
      "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large margins (> 14.33% on\naverage).",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rleZtn5OqJ",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonghyun_Choi1",
        "name": "Jonghyun Choi",
        "name_site": null,
        "openreview_id": "~Jonghyun_Choi1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://ppolon.github.io/",
        "dblp_id": "21/11103",
        "google_scholar_url": "uiGWnm4AAAAJ",
        "orcid": "0000-0002-7934-8434",
        "linkedin_url": "jonghyun-choi-459bb615/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rp5vfyp5Np",
      "title": "BATTLE: Towards Behavior-oriented Adversarial Attacks against Deep Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Evaluating the performance of deep reinforcement learning (DRL) agents under adversarial attacks that aim to induce specific behaviors, i.e., behavior-oriented adversarial attacks, is crucial for understanding the robustness of DRL agents. Prior research primarily focuses on directing agents towards pre-determined states or policies, lacking generality and flexibility. Therefore, it is important to devise universal attacks that target inducing specific behaviors in a victim. In this work, we propose BATTLE, a universal behavior-oriented adversarial attack method. In BATTLE, an intention policy is trained to align with human preferences for flexible behavior orientation, while the adversary is trained to guide the victim policy to imitate the intention policy. To improve the attack performance, we introduce a weighting function that assigns importance weights over each state. Our empirical results over several manipulation tasks of Meta-world show the superiority of BATTLE in behavior-oriented adversarial attack settings, outperforming current adversarial attack algorithms. Furthermore, we also demonstrate that BATTLE can improve the robustness of agents under strong attacks by training with adversary. Lastly, we showcase the strong behavior-inducing capability of BATTLE by guiding Decision Transformer agents to act in line with human preferences across various MuJoCo tasks. Our videos are available in https://sites.google.com/view/jj9uxjgmba5lr3g.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=rp5vfyp5Np",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yali_Du1",
        "name": "Yali Du",
        "name_site": null,
        "openreview_id": "~Yali_Du1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Shanghai Jiao Tong University (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ruQkcBfzpm",
      "title": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, \nwhich is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to \n`understand' the output of an image encoder.\nWith the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM.\nTo this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating  high-quality English data to 95 languages. \nOn the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data. We release our model and code at ANONYMIZED.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ruQkcBfzpm",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhay_Jain1",
        "name": "Abhay Jain",
        "name_site": null,
        "openreview_id": "~Abhay_Jain1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://jain-abhay.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "abhayjain11/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (BHU) (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 26,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sdkB5j7yNr",
      "title": "PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "It is widely known that state-of-the-art machine learning models — including vision and language ones — can be seriously compromised by adversarial perturbations, so it is also increasingly relevant to develop capability to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks, with population level risk guarantees. In particular, given a specific attack, we introduce the notion of a $(\\alpha,\\zeta)$ machine learning model safety guarantee: this guarantee, which is supported by a testing procedure based on the availability of a calibration set, entails one will only declare that a machine learning model adversarial (population) risk is less than $\\alpha$ (i.e. the model is safe) given that the model adversarial (population) risk is higher than $\\alpha$ (i.e. the model is in fact unsafe), with probability less than $\\zeta$. We  also propose Bayesian optimization algorithms to determine very efficiently whether or not a machine learning model is  $(\\alpha,\\zeta)$-safe in the presence of an adversarial attack, along with their statistical guarantees. We apply our framework to a range of machine learning models — including various sizes of vision Transformer (ViT) and ResNet models — impaired by a variety of adversarial attacks such as AutoAttack, SquareAttack and  natural evolution strategy attack, in order to illustrate the merit of our approach. Of particular relevance, we show that ViT's are generally more robust to adversarial attacks than ResNets and ViT-large is more robust than smaller models. Overall, our approach goes beyond existing empirical adversarial risk based certification guarantees, paving  the way to more effective AI regulation based on rigorous (and provable) performance guarantees.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sdkB5j7yNr",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ilija_Bogunovic2",
        "name": "Ilija Bogunovic",
        "name_site": null,
        "openreview_id": "~Ilija_Bogunovic1",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://ilijabogunovic.com/",
        "dblp_id": "142/2725",
        "google_scholar_url": "xMvt3NEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Swiss Federal Institute of Technology (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "sp666x6Gh3",
      "title": "Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Sampling-based methods, e.g., Deep Ensembles and Bayesian Neural Nets have become promising approaches to improve the quality of uncertainty estimation and robust generalization. However, they suffer from a large model size and high latency at test-time, which limits the scalability needed for low-resource devices and real-time applications. To resolve these computational issues, we propose Density-Softmax, a sampling-free deterministic framework via combining a density function built on a 1-Lipschitz feature extractor with the softmax layer. Theoretically, we show that our model is the solution of minimax uncertainty risk and is distance-aware on feature space, thus reducing the over-confidence of the standard softmax under distribution shifts. Empirically, our method achieves competitive results with state-of-the-art techniques in terms of uncertainty and robustness, while having a lower number of model parameters and a lower latency at test-time.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=sp666x6Gh3",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anqi_Liu2",
        "name": "Anqi Liu",
        "name_site": null,
        "openreview_id": "~Anqi_Liu2",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://anqiliu-ai.github.io/",
        "dblp_id": null,
        "google_scholar_url": "Q8yp6zQAAAAJ",
        "orcid": "0000-0002-0468-5698",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tuuEvgfxr5",
      "title": "Bayesian Pseudo-Coresets via Contrastive Divergence",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Bayesian methods provide an elegant framework for estimating parameter posteriors and quantification of uncertainty associated with probabilistic models. However, they often suffer from slow inference times, rendering them impractical for scalable applications.  To address this challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution. BPC methods aim to create a small synthetic dataset, known as pseudo-coresets, that approximates the posterior inference achieved with the original dataset. This approximation is achieved by optimizing a divergence measure between the true posterior and the pseudo-coreset posterior.\nVarious divergence measures have been proposed for constructing pseudo-coresets, with forward Kullback-Leibler (KL) divergence being the most successful. However, using forward KL divergence necessitates sampling from the pseudo-coreset posterior, often accomplished through approximate Gaussian variational distributions. Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for sampling, but this becomes challenging in high-dimensional parameter spaces due to slow mixing.\nIn this study, we introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process. Furthermore, it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution.\nTo validate our method's effectiveness, we conduct extensive experiments on multiple datasets, demonstrating its superiority over existing BPC techniques. \nOur implementation is available at https://anonymous.4open.science/r/BPC-CD-E762",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tuuEvgfxr5",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Tiwary1",
        "name": "Piyush Tiwary",
        "name_site": null,
        "openreview_id": "~Piyush_Tiwary1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://backpropagator.github.io/",
        "dblp_id": null,
        "google_scholar_url": "tUdHYloAAAAJ",
        "orcid": "0000-0002-4499-1059",
        "linkedin_url": "thebackpropogator/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tuuEvgfxr5",
      "title": "Bayesian Pseudo-Coresets via Contrastive Divergence",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Bayesian methods provide an elegant framework for estimating parameter posteriors and quantification of uncertainty associated with probabilistic models. However, they often suffer from slow inference times, rendering them impractical for scalable applications.  To address this challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution. BPC methods aim to create a small synthetic dataset, known as pseudo-coresets, that approximates the posterior inference achieved with the original dataset. This approximation is achieved by optimizing a divergence measure between the true posterior and the pseudo-coreset posterior.\nVarious divergence measures have been proposed for constructing pseudo-coresets, with forward Kullback-Leibler (KL) divergence being the most successful. However, using forward KL divergence necessitates sampling from the pseudo-coreset posterior, often accomplished through approximate Gaussian variational distributions. Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for sampling, but this becomes challenging in high-dimensional parameter spaces due to slow mixing.\nIn this study, we introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process. Furthermore, it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution.\nTo validate our method's effectiveness, we conduct extensive experiments on multiple datasets, demonstrating its superiority over existing BPC techniques. \nOur implementation is available at https://anonymous.4open.science/r/BPC-CD-E762",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tuuEvgfxr5",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Kumar_Shubham1",
        "name": "Kumar Shubham",
        "name_site": null,
        "openreview_id": "~Kumar_Shubham1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://kyrs.github.io/",
        "dblp_id": null,
        "google_scholar_url": "JBb0tXMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tuuEvgfxr5",
      "title": "Bayesian Pseudo-Coresets via Contrastive Divergence",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Bayesian methods provide an elegant framework for estimating parameter posteriors and quantification of uncertainty associated with probabilistic models. However, they often suffer from slow inference times, rendering them impractical for scalable applications.  To address this challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution. BPC methods aim to create a small synthetic dataset, known as pseudo-coresets, that approximates the posterior inference achieved with the original dataset. This approximation is achieved by optimizing a divergence measure between the true posterior and the pseudo-coreset posterior.\nVarious divergence measures have been proposed for constructing pseudo-coresets, with forward Kullback-Leibler (KL) divergence being the most successful. However, using forward KL divergence necessitates sampling from the pseudo-coreset posterior, often accomplished through approximate Gaussian variational distributions. Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for sampling, but this becomes challenging in high-dimensional parameter spaces due to slow mixing.\nIn this study, we introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process. Furthermore, it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution.\nTo validate our method's effectiveness, we conduct extensive experiments on multiple datasets, demonstrating its superiority over existing BPC techniques. \nOur implementation is available at https://anonymous.4open.science/r/BPC-CD-E762",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tuuEvgfxr5",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vivek_V_Kashyap1",
        "name": "Vivek V Kashyap",
        "name_site": null,
        "openreview_id": "~Vivek_V_Kashyap1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://vivekvkashyap.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "vivek-v-kashyap/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tuuEvgfxr5",
      "title": "Bayesian Pseudo-Coresets via Contrastive Divergence",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Bayesian methods provide an elegant framework for estimating parameter posteriors and quantification of uncertainty associated with probabilistic models. However, they often suffer from slow inference times, rendering them impractical for scalable applications.  To address this challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution. BPC methods aim to create a small synthetic dataset, known as pseudo-coresets, that approximates the posterior inference achieved with the original dataset. This approximation is achieved by optimizing a divergence measure between the true posterior and the pseudo-coreset posterior.\nVarious divergence measures have been proposed for constructing pseudo-coresets, with forward Kullback-Leibler (KL) divergence being the most successful. However, using forward KL divergence necessitates sampling from the pseudo-coreset posterior, often accomplished through approximate Gaussian variational distributions. Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for sampling, but this becomes challenging in high-dimensional parameter spaces due to slow mixing.\nIn this study, we introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process. Furthermore, it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution.\nTo validate our method's effectiveness, we conduct extensive experiments on multiple datasets, demonstrating its superiority over existing BPC techniques. \nOur implementation is available at https://anonymous.4open.science/r/BPC-CD-E762",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=tuuEvgfxr5",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Prathosh_AP1",
        "name": "Prathosh AP",
        "name_site": null,
        "openreview_id": "~Prathosh_AP1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/prathosh",
        "dblp_id": "218/5887",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OEwV4bsAAAAJ",
        "orcid": null,
        "linkedin_url": "prathosh-ap-phd-50ab9511/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uBpSkFGVQU",
      "title": "Depth-Guided Self-Supervised Learning: Seeing the World in 3D",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Self-Supervised Learning (SSL) methods operate on unlabeled data to learn robust representations useful for downstream tasks. Most SSL methods rely on augmentations obtained by transforming the 2D image pixel map. These augmentations ignore the fact that biological vision takes place in an immersive three-dimensional, temporally contiguous environment, and that low-level biological vision relies heavily on depth cues. Using a signal provided by a pretrained state-of-the-art monocular RGB-to-depth model (the Depth Prediction Transformer, Ranftl et al., 2021), we explore two distinct approaches to incorporating depth signals into the SSL framework. First, we evaluate self-supervised learning using an RGB+depth input representation. Second, we use the depth signal to generate novel views from slightly different camera positions, thereby producing a 3D augmentation for self-supervised learning. We also examine the combination of the two approaches. We evaluate the approaches on three different SSL methods---BYOL, SimSiam, and SwAV---using ImageNette (10 class subset of ImageNet), ImageNet-100 and ImageNet-1k datasets. We find that both approaches to incorporating depth signals improve the robustness and generalization of the baseline SSL methods, and the two approaches are complementary because the combination of depth and 3D views performs the best in most settings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=uBpSkFGVQU",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Di_He1_1",
        "name": "Di He",
        "name_site": null,
        "openreview_id": "~Anirudh_Goyal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://anirudh9119.github.io/",
        "dblp_id": "172/1039",
        "google_scholar_url": "krrh6OUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ukmh3mWFf0",
      "title": "Attributed Graph Clustering via Coarsening with Modularity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework’s efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ukmh3mWFf0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samarth_Bhatia1",
        "name": "Samarth Bhatia",
        "name_site": null,
        "openreview_id": "~Samarth_Bhatia1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://plutonium-239.github.io/",
        "dblp_id": "283/5525",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ukmh3mWFf0",
      "title": "Attributed Graph Clustering via Coarsening with Modularity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework’s efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ukmh3mWFf0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yukti_Makhija1",
        "name": "Yukti Makhija",
        "name_site": null,
        "openreview_id": "~Yukti_Makhija1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ukmh3mWFf0",
      "title": "Attributed Graph Clustering via Coarsening with Modularity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework’s efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ukmh3mWFf0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manoj_Kumar4",
        "name": "Manoj Kumar",
        "name_site": null,
        "openreview_id": "~Manoj_Kumar4",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "gdL-bokAAAAJ",
        "orcid": null,
        "linkedin_url": "manoj-kumar-9042b449/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ukmh3mWFf0",
      "title": "Attributed Graph Clustering via Coarsening with Modularity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework’s efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ukmh3mWFf0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mohit_Kataria1",
        "name": "Mohit Kataria",
        "name_site": null,
        "openreview_id": "~Mohit_Kataria1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "passenger/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ukmh3mWFf0",
      "title": "Attributed Graph Clustering via Coarsening with Modularity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework’s efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=ukmh3mWFf0",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "unxTEvHOW7",
      "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Memes, originally created for humor and social commentary, have evolved into vehicles for offensive and harmful content online. Detecting such content is crucial for upholding the integrity of digital spaces. However, binary classification of memes as offensive or not often falls short in practical applications. Ensuring the reliability of these classifiers and addressing inadvertent biases during training are essential tasks. While numerous input-attribution based interpretability methods exist to shed light on the model's decision-making process, they frequently yield insufficient and semantically irrelevant keywords extracted from input memes. In response, we propose a novel, theoretically grounded approach that extracts meaningful ``tokens\" from a global vocabulary, yielding both relevant and exhaustive set of interpretable keywords. This method provides valuable insights into the model's behavior and uncovers hidden meanings within memes, significantly enhancing transparency and fostering user trust. Through comprehensive quantitative and qualitative evaluations, we demonstrate the superior effectiveness of our approach compared to conventional baselines. Our research contributes to a deeper understanding of meme content analysis and the development of more robust and interpretable multimodal systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=unxTEvHOW7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Dibyanayan_Bandyopadhyay1",
        "name": "Dibyanayan Bandyopadhyay",
        "name_site": null,
        "openreview_id": "~Dibyanayan_Bandyopadhyay1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "243/2978.html",
        "google_scholar_url": "t_kZ1qsAAAAJ",
        "orcid": "0000-0001-5279-6344",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Patna (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 2.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "unxTEvHOW7",
      "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Memes, originally created for humor and social commentary, have evolved into vehicles for offensive and harmful content online. Detecting such content is crucial for upholding the integrity of digital spaces. However, binary classification of memes as offensive or not often falls short in practical applications. Ensuring the reliability of these classifiers and addressing inadvertent biases during training are essential tasks. While numerous input-attribution based interpretability methods exist to shed light on the model's decision-making process, they frequently yield insufficient and semantically irrelevant keywords extracted from input memes. In response, we propose a novel, theoretically grounded approach that extracts meaningful ``tokens\" from a global vocabulary, yielding both relevant and exhaustive set of interpretable keywords. This method provides valuable insights into the model's behavior and uncovers hidden meanings within memes, significantly enhancing transparency and fostering user trust. Through comprehensive quantitative and qualitative evaluations, we demonstrate the superior effectiveness of our approach compared to conventional baselines. Our research contributes to a deeper understanding of meme content analysis and the development of more robust and interpretable multimodal systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=unxTEvHOW7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~ASMIT_GANGULY1",
        "name": "ASMIT GANGULY",
        "name_site": null,
        "openreview_id": "~ASMIT_GANGULY1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://asmit203.github.io/",
        "dblp_id": "387/3854",
        "google_scholar_url": "o99k5ccAAAAJ",
        "orcid": "0009-0009-4249-9552",
        "linkedin_url": "asmitganguly/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Patna (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 2.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "unxTEvHOW7",
      "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Memes, originally created for humor and social commentary, have evolved into vehicles for offensive and harmful content online. Detecting such content is crucial for upholding the integrity of digital spaces. However, binary classification of memes as offensive or not often falls short in practical applications. Ensuring the reliability of these classifiers and addressing inadvertent biases during training are essential tasks. While numerous input-attribution based interpretability methods exist to shed light on the model's decision-making process, they frequently yield insufficient and semantically irrelevant keywords extracted from input memes. In response, we propose a novel, theoretically grounded approach that extracts meaningful ``tokens\" from a global vocabulary, yielding both relevant and exhaustive set of interpretable keywords. This method provides valuable insights into the model's behavior and uncovers hidden meanings within memes, significantly enhancing transparency and fostering user trust. Through comprehensive quantitative and qualitative evaluations, we demonstrate the superior effectiveness of our approach compared to conventional baselines. Our research contributes to a deeper understanding of meme content analysis and the development of more robust and interpretable multimodal systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=unxTEvHOW7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Baban_Gain1",
        "name": "Baban Gain",
        "name_site": null,
        "openreview_id": "~Baban_Gain1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "243/3240",
        "google_scholar_url": "vI5Y7koAAAAJ",
        "orcid": "0000-0001-8673-7078",
        "linkedin_url": "babangain/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Patna (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 2.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "unxTEvHOW7",
      "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Memes, originally created for humor and social commentary, have evolved into vehicles for offensive and harmful content online. Detecting such content is crucial for upholding the integrity of digital spaces. However, binary classification of memes as offensive or not often falls short in practical applications. Ensuring the reliability of these classifiers and addressing inadvertent biases during training are essential tasks. While numerous input-attribution based interpretability methods exist to shed light on the model's decision-making process, they frequently yield insufficient and semantically irrelevant keywords extracted from input memes. In response, we propose a novel, theoretically grounded approach that extracts meaningful ``tokens\" from a global vocabulary, yielding both relevant and exhaustive set of interpretable keywords. This method provides valuable insights into the model's behavior and uncovers hidden meanings within memes, significantly enhancing transparency and fostering user trust. Through comprehensive quantitative and qualitative evaluations, we demonstrate the superior effectiveness of our approach compared to conventional baselines. Our research contributes to a deeper understanding of meme content analysis and the development of more robust and interpretable multimodal systems.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=unxTEvHOW7",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Asif_Ekbal1",
        "name": "Asif Ekbal",
        "name_site": null,
        "openreview_id": "~Asif_Ekbal1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://ekbalasif.github.io",
        "dblp_id": "11/3590",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=IAL_F04AAAAJ",
        "orcid": "0000-0003-3612-8834",
        "linkedin_url": "asif-ekbal-3b8a4517/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Patna (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 2.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.8660254037844386,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "vrjDNgAfp4",
      "title": "SEMANTIC RHEOLOGY: THE FLOW OF IDEAS IN LANGUAGE MODELS",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The flow of ideas has been extensively studied by physicists, psychologists, and machine learning engineers. This paper adopts certain tools from microrheology to investigate the similarity-based flow of ideas. We introduce a random walker in the word embeddings and study its behaviour. Such similarity mediated random walks through the embedding space shows signatures of anomalous diffusion, commonly observed in complex structured systems such as biological cells and complex fluids.  The paper concludes by proposing the application of popular tools employed in the study of random walks and diffusion of particles under Brownian motion to quantitatively assess the incorporation of diverse ideas in a document. Overall, this paper presents a self-referenced method that combines concepts from microrheology and machine learning to explore the meandering tendencies of language models and their potential association with creativity.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=vrjDNgAfp4",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 1,
      "track_name": "Main Conference",
      "author": {
        "id": "~Debayan_Dasgupta1",
        "name": "Debayan Dasgupta",
        "name_site": null,
        "openreview_id": "~Debayan_Dasgupta1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "wsMAWL0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.25,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "x6gnuUXpxM",
      "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present a sparsely connected neural network architecture constructed using the theory of Ramanujan graphs which provide comparable performance to a dense network. The method can be considered as a before-training, deterministic, weight free, pruning at initialization (PaI) technique. The deterministic Ramanujan graphs occur either as Cayley graphs of certain algebraic groups or as Ramanujan $r$-coverings of the full $(k,l)$ bi-regular bipartite graph on $k + l$ vertices. Sparse networks are constructed for bipartite graphs representing both the convolution and the fully connected layers. We experimentally show that the proposed sparse architecture provides comparable accuracy with a lower sparsity ratio than those achieved by previous approaches based on non-deterministic methods for benchmark datasets. In addition, they retain other desirable properties such as path connectivity and symmetricity.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=x6gnuUXpxM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arindam_Biswas1",
        "name": "Arindam Biswas",
        "name_site": null,
        "openreview_id": "~Arindam_Biswas1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "x6gnuUXpxM",
      "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present a sparsely connected neural network architecture constructed using the theory of Ramanujan graphs which provide comparable performance to a dense network. The method can be considered as a before-training, deterministic, weight free, pruning at initialization (PaI) technique. The deterministic Ramanujan graphs occur either as Cayley graphs of certain algebraic groups or as Ramanujan $r$-coverings of the full $(k,l)$ bi-regular bipartite graph on $k + l$ vertices. Sparse networks are constructed for bipartite graphs representing both the convolution and the fully connected layers. We experimentally show that the proposed sparse architecture provides comparable accuracy with a lower sparsity ratio than those achieved by previous approaches based on non-deterministic methods for benchmark datasets. In addition, they retain other desirable properties such as path connectivity and symmetricity.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=x6gnuUXpxM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suryam_Arnav_Kalra1",
        "name": "Suryam Arnav Kalra",
        "name_site": null,
        "openreview_id": "~Suryam_Arnav_Kalra1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "suryam-arnav-kalra-8ba107193/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "x6gnuUXpxM",
      "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "We present a sparsely connected neural network architecture constructed using the theory of Ramanujan graphs which provide comparable performance to a dense network. The method can be considered as a before-training, deterministic, weight free, pruning at initialization (PaI) technique. The deterministic Ramanujan graphs occur either as Cayley graphs of certain algebraic groups or as Ramanujan $r$-coverings of the full $(k,l)$ bi-regular bipartite graph on $k + l$ vertices. Sparse networks are constructed for bipartite graphs representing both the convolution and the fully connected layers. We experimentally show that the proposed sparse architecture provides comparable accuracy with a lower sparsity ratio than those achieved by previous approaches based on non-deterministic methods for benchmark datasets. In addition, they retain other desirable properties such as path connectivity and symmetricity.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=x6gnuUXpxM",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Pabitra_Mitra1",
        "name": "Pabitra Mitra",
        "name_site": null,
        "openreview_id": "~Pabitra_Mitra1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~pabitra/",
        "dblp_id": "m/PabitraMitra",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=5bXSZPYAAAAJ",
        "orcid": "0000-0002-1908-9813",
        "linkedin_url": "pabitra-mitra-8028235/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Trinity College Dublin (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xThb6APBoG",
      "title": "Adapting Retrieval Models to Task-Specific Goals using Reinforcement Learning",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Given an input query, retrieval models are trained using user feedback data (e.g., click data) to output a ranked list of items. However, it is difficult to optimize task-specific goals  using supervised learning because the goals often correspond to non-differentiable losses. For example, we may want to optimize recall or novelty of the top-k items for a recommendation task or optimize accuracy of a blackbox large language model (LLM) for the retrieval-augmented generation task. To optimize arbitrary task-specific losses, we propose a reinforcement learning-based framework that applies to any pretrained retrieval model. Specifically, our solution uses policy gradient and addresses the key challenge of large action spaces by reduction to a binary action space, given both the query and the retrieved item. Our formulation also allows for exploration based on auxiliary retrieval models.  We empirically evaluate the proposed algorithm on improving recall for a query-ad retrieval task on two datasets with 4K and 1.9M actions respectively. We also show the benefit of the proposed algorithm on improving a custom metric---novelty of the retrieved items w.r.t. existing algorithms---for a commercial search engine.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=xThb6APBoG",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amit_Sharma3",
        "name": "Amit Sharma",
        "name_site": null,
        "openreview_id": "~Amit_Sharma3",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://amitsharma.in/",
        "dblp_id": "72/2540-7",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=CXgQufgAAAAJ",
        "orcid": "0000-0002-2086-3191",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.0,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yCYnKMHX3u",
      "title": "MultiLayerDiffusion: Composing Global Contexts and Local Details in Image Generation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Diffusion models have demonstrated their capability to synthesize high-quality and diverse images from textual prompts. However, simultaneous control over both global contexts (e.g., object layouts and interactions) and local details (e.g., colors and emotions) still remains a significant challenge. The models often fail to understand complex descriptions involving multiple objects and reflect specified visual attributes to wrong targets or forget to reflect them. This paper presents MultiLayerDiffusion, a novel framework which allows simultaneous control over the global contexts and the local details in text-to-image generation without requiring training or fine-tuning. It assigns multiple global and local prompts to corresponding layers and composes them to generate images using pre-trained diffusion models. Our framework enables complex global-local compositions, decomposing intricate prompts into manageable concepts and controlling object details while preserving global contexts. We demonstrate that MultiLayerDiffusion effectively generates complex images that adhere to both user-provided object interactions and object details. We also show its effectiveness not only in image generation but also in image editing.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yCYnKMHX3u",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Manohar_Kaul1",
        "name": "Manohar Kaul",
        "name_site": null,
        "openreview_id": "~Manohar_Kaul1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://manukaul.github.io/",
        "dblp_id": "29/10735",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=jNroyK4AAAAJ",
        "orcid": null,
        "linkedin_url": "manu-k-72b936287/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Fujitsu Research and Development Center (Japan)",
        "countries": [
          "Japan"
        ],
        "country_codes": [
          "JP"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.6666666666666665,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yIEKq72cTE",
      "title": "Byzantine-Robust Dynamic Weighted Aggregation Framework for Optimal Attack Mitigation in Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning on sensitive data without centralized storage and sharing. However, FL is vulnerable to data poisoning attacks, where malicious clients aim to manipulate the training process by injecting poisonous data. Existing defense mechanisms for FL suffer from limitations, including a trade-off between precision and robustness, assumptions on asymptotic optimal bounds on error rates of parameters, i.i.d. data distributions, and strong-convexity assumptions on the optimization problem. To address these limitations, we propose a novel framework called Federated Learning Optimal Transport (FLOT). Our method leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained models on client devices. Additionally, FLOT introduces a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. Our experimental results demonstrate that FLOT outperforms existing baseline methods under single and multi-client attack settings. Also, it serves as a robust client selection technique under no attack. We also prove the Byzantine resilience of FLOT to demonstrate its effectiveness. These results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. The robustness and effectiveness of FLOT make it a promising solution for real-world applications where data privacy and security are critical.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yIEKq72cTE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~K_Naveen_Kumar1",
        "name": "K Naveen Kumar",
        "name_site": null,
        "openreview_id": "~K_Naveen_Kumar1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://naveenkumar1311.github.io/",
        "dblp_id": "271/8085",
        "google_scholar_url": "Kb0j5RwAAAAJ",
        "orcid": null,
        "linkedin_url": "naveen-kumar-k-a035a2129/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yIEKq72cTE",
      "title": "Byzantine-Robust Dynamic Weighted Aggregation Framework for Optimal Attack Mitigation in Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning on sensitive data without centralized storage and sharing. However, FL is vulnerable to data poisoning attacks, where malicious clients aim to manipulate the training process by injecting poisonous data. Existing defense mechanisms for FL suffer from limitations, including a trade-off between precision and robustness, assumptions on asymptotic optimal bounds on error rates of parameters, i.i.d. data distributions, and strong-convexity assumptions on the optimization problem. To address these limitations, we propose a novel framework called Federated Learning Optimal Transport (FLOT). Our method leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained models on client devices. Additionally, FLOT introduces a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. Our experimental results demonstrate that FLOT outperforms existing baseline methods under single and multi-client attack settings. Also, it serves as a robust client selection technique under no attack. We also prove the Byzantine resilience of FLOT to demonstrate its effectiveness. These results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. The robustness and effectiveness of FLOT make it a promising solution for real-world applications where data privacy and security are critical.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yIEKq72cTE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vishnu_Chalavadi1",
        "name": "Vishnu Chalavadi",
        "name_site": null,
        "openreview_id": "~Vishnu_Chalavadi1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://vish9u.github.io",
        "dblp_id": "302/7458",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-9184-3545",
        "linkedin_url": "chalavadi-vishnu-phd/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yIEKq72cTE",
      "title": "Byzantine-Robust Dynamic Weighted Aggregation Framework for Optimal Attack Mitigation in Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning on sensitive data without centralized storage and sharing. However, FL is vulnerable to data poisoning attacks, where malicious clients aim to manipulate the training process by injecting poisonous data. Existing defense mechanisms for FL suffer from limitations, including a trade-off between precision and robustness, assumptions on asymptotic optimal bounds on error rates of parameters, i.i.d. data distributions, and strong-convexity assumptions on the optimization problem. To address these limitations, we propose a novel framework called Federated Learning Optimal Transport (FLOT). Our method leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained models on client devices. Additionally, FLOT introduces a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. Our experimental results demonstrate that FLOT outperforms existing baseline methods under single and multi-client attack settings. Also, it serves as a robust client selection technique under no attack. We also prove the Byzantine resilience of FLOT to demonstrate its effectiveness. These results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. The robustness and effectiveness of FLOT make it a promising solution for real-world applications where data privacy and security are critical.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yIEKq72cTE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srinivasa_Rao_Chalamala1",
        "name": "Srinivasa Rao Chalamala",
        "name_site": null,
        "openreview_id": "~Srinivasa_Rao_Chalamala1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://srinivaschalamala.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=UZ-2WMgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services Limited (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yIEKq72cTE",
      "title": "Byzantine-Robust Dynamic Weighted Aggregation Framework for Optimal Attack Mitigation in Federated Learning",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Federated learning (FL) has emerged as a promising solution to enable distributed learning on sensitive data without centralized storage and sharing. However, FL is vulnerable to data poisoning attacks, where malicious clients aim to manipulate the training process by injecting poisonous data. Existing defense mechanisms for FL suffer from limitations, including a trade-off between precision and robustness, assumptions on asymptotic optimal bounds on error rates of parameters, i.i.d. data distributions, and strong-convexity assumptions on the optimization problem. To address these limitations, we propose a novel framework called Federated Learning Optimal Transport (FLOT). Our method leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained models on client devices. Additionally, FLOT introduces a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. Our experimental results demonstrate that FLOT outperforms existing baseline methods under single and multi-client attack settings. Also, it serves as a robust client selection technique under no attack. We also prove the Byzantine resilience of FLOT to demonstrate its effectiveness. These results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. The robustness and effectiveness of FLOT make it a promising solution for real-world applications where data privacy and security are critical.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=yIEKq72cTE",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ajeet_Kumar_Singh2",
        "name": "Ajeet Kumar Singh",
        "name_site": null,
        "openreview_id": "~Ajeet_Kumar_Singh2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://ajeetksingh.github.io/",
        "dblp_id": "147/1487",
        "google_scholar_url": "mtaDclEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 3.4,
        "rating_std": 0.8,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zEkvV65Wi1",
      "title": "Understanding Calibration Transfer in Knowledge Distillation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Modern deep neural networks are often miscalibrated, leading to overconfident mistakes that erode their reliability, and limit their use in critical applications. The existing confidence calibration techniques range from train-time modification of loss functions to post-hoc smoothing of the classifier's predicted confidence vector. Despite the success of these approaches, it is relatively unclear if supervision from an already trained expert classifier can further enhance a given classifier's confidence calibration. Knowledge distillation (KD) has been shown to help classifiers achieve better accuracy. However, little to no attention has been paid to a systematic understanding if the calibration can also be transferred via KD. In this work, we provide new insights into how and when expert supervision can produce well-calibrated classifiers, by studying a special class of linear teacher and student classifiers. Specifically, we provide theoretical insights into the working mechanisms of KD and show that calibrated teachers can distill calibrated students. We further show that unlike traditional KD where a smaller capacity classifier learns reliably from a larger capacity expert, transfer of calibration can be induced from lower capacity teachers to larger capacity students (aka reverse-KD). Furthermore, our findings indicate that not all training regimes are equally suitable and that a teacher classifier trained using dynamic label smoothing leads to the better calibration of student classifiers via KD. Moreover, the proposed KD-based calibration leads to a state-of-the-art(SOTA) calibration framework surpassing all existing calibration techniques. Our claims are backed up by extensive experiments on standard computer vision classification tasks. On CIFAR100 using the WRN-40-1 feature extractor, we report an  ECE of 0.98 compared to 7.61 and 2.1 by the current SOTA calibration techniques Adafocal (Ghosh, NeurIPS 2022) and CPC (Cheng and Vasconcelos, CVPR 2022)  respectively, and 11.16 by the baseline NLL loss (lower ECE is better). The calibration improvement is achieved across various architectures. Using MobileNetv2 on CIFAR100 we report an ECE of 0.88/1.83/4.17/7.76 using Ours/Adafocal/CPC/\\NLL.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=zEkvV65Wi1",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ramya_Hebbalaguppe2",
        "name": "Ramya Hebbalaguppe",
        "name_site": null,
        "openreview_id": "~Ramya_Hebbalaguppe2",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://rhebbalaguppe.github.io/",
        "dblp_id": "145/2287",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0006-1186-6311",
        "linkedin_url": "https://in.linkedin.com/in/ramya-hebbalaguppe-620b272",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zEkvV65Wi1",
      "title": "Understanding Calibration Transfer in Knowledge Distillation",
      "status": "Withdraw",
      "normalized_status": "withdrawn",
      "abstract": "Modern deep neural networks are often miscalibrated, leading to overconfident mistakes that erode their reliability, and limit their use in critical applications. The existing confidence calibration techniques range from train-time modification of loss functions to post-hoc smoothing of the classifier's predicted confidence vector. Despite the success of these approaches, it is relatively unclear if supervision from an already trained expert classifier can further enhance a given classifier's confidence calibration. Knowledge distillation (KD) has been shown to help classifiers achieve better accuracy. However, little to no attention has been paid to a systematic understanding if the calibration can also be transferred via KD. In this work, we provide new insights into how and when expert supervision can produce well-calibrated classifiers, by studying a special class of linear teacher and student classifiers. Specifically, we provide theoretical insights into the working mechanisms of KD and show that calibrated teachers can distill calibrated students. We further show that unlike traditional KD where a smaller capacity classifier learns reliably from a larger capacity expert, transfer of calibration can be induced from lower capacity teachers to larger capacity students (aka reverse-KD). Furthermore, our findings indicate that not all training regimes are equally suitable and that a teacher classifier trained using dynamic label smoothing leads to the better calibration of student classifiers via KD. Moreover, the proposed KD-based calibration leads to a state-of-the-art(SOTA) calibration framework surpassing all existing calibration techniques. Our claims are backed up by extensive experiments on standard computer vision classification tasks. On CIFAR100 using the WRN-40-1 feature extractor, we report an  ECE of 0.98 compared to 7.61 and 2.1 by the current SOTA calibration techniques Adafocal (Ghosh, NeurIPS 2022) and CPC (Cheng and Vasconcelos, CVPR 2022)  respectively, and 11.16 by the baseline NLL loss (lower ECE is better). The calibration improvement is achieved across various architectures. Using MobileNetv2 on CIFAR100 we report an ECE of 0.88/1.83/4.17/7.76 using Ours/Adafocal/CPC/\\NLL.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=zEkvV65Wi1",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mayank_Baranwal1",
        "name": "Mayank Baranwal",
        "name_site": null,
        "openreview_id": "~Mayank_Baranwal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.sc.iitb.ac.in/~mayank",
        "dblp_id": "69/10832",
        "google_scholar_url": "dMAimxMAAAAJ",
        "orcid": "0000-0001-9354-2826",
        "linkedin_url": "mayank-baranwal/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Consultancy Services Limited (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    }
  ]
}