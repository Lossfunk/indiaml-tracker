{
  "conference": "NeurIPS 2022",
  "focus_country": "India",
  "total_papers": 106,
  "generated_at": "2025-07-06T10:39:19.857457",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "3LBxVcnsEkV",
      "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline.",
      "tldr": "Learning graph and subgraph edit distance using graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/54507",
      "pdf_url": "https://openreview.net/pdf?id=3LBxVcnsEkV",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_Ranjan1",
        "name": "Rishabh Ranjan",
        "name_site": "Rishabh Ranjan, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy, Yogish Sabharwal, Sayan Ranu",
        "openreview_id": "~Rishabh_Ranjan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rishabh-ranjan.github.io",
        "dblp_id": null,
        "google_scholar_url": "NNzQUrcAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jogendra_Nath_Kundu2",
        "name": "Jogendra Nath Kundu",
        "name_site": "Jogendra Nath Kundu, Siddharth Seth, Anirudh Jamkhandi, Pradyumna YM, Varun Jampani, Anirban Chakraborty, Venkatesh Babu R",
        "openreview_id": "~Jogendra_Nath_Kundu2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/jogendra",
        "dblp_id": "185/0812",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Xa44GDEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "62GLWUoOLb5",
      "title": "Scalable Distributional Robustness in a Class of Non-Convex Optimization with Guarantees",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Distributionally robust optimization (DRO) has shown a lot of promise in providing robustness in learning as well as sample-based optimization problems. We endeavor to provide DRO solutions for a class of sum of fractionals, non-convex optimization which is used for decision making in prominent areas such as facility location and security games. In contrast to previous work, we find it more tractable to optimize the equivalent variance regularized form of DRO rather than the minimax form. We transform the variance regularized form to a mixed-integer second-order cone program (MISOCP), which, while guaranteeing global optimality, does not scale enough to solve problems with real-world datasets. We further propose two abstraction approaches based on clustering and stratified sampling to increase scalability, which we then use for real-world datasets. Importantly, we provide global optimality guarantees for our approach and show experimentally that our solution quality is better than the locally optimal ones achieved by state-of-the-art gradient-based methods. We experimentally compare our different approaches and baselines and reveal nuanced properties of a DRO solution.",
      "tldr": "We propose distributionally robust optimization solutions for a class of sum of ratios, non-convex optimization which is used for decision-making in prominent areas such as facility location and security games",
      "site_url": "https://nips.cc/virtual/2022/poster/53642",
      "pdf_url": "https://openreview.net/pdf?id=62GLWUoOLb5",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Avinandan_Bose1",
        "name": "Avinandan Bose",
        "name_site": "Avinandan Bose, Arunesh Sinha, Tien Mai",
        "openreview_id": "~Avinandan_Bose1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://avinandan22.github.io/",
        "dblp_id": "305/7490",
        "google_scholar_url": "https://scholar.google.com/citations?pli=1",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 2.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "6qdUJblMHqy",
      "title": "Toward Efficient Robust Training against Union of $\\ell_p$ Threat Models",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The overwhelming vulnerability of deep neural networks to carefully crafted perturbations known as adversarial attacks has led to the development of various training techniques to produce robust models. While the primary focus of existing approaches has been directed toward addressing the worst-case performance achieved under a single-threat model, it is imperative that safety-critical systems are robust with respect to multiple threat models simultaneously. Existing approaches that address worst-case performance under the union of such threat models ($\\ell_{\\infty}, \\ell_2, \\ell_1$) either utilize adversarial training methods that require multi-step attacks which are computationally expensive in practice, or rely upon fine-tuning of pre-trained models that are robust with respect to a single-threat model. In this work, we show that by carefully choosing the objective function used for robust training, it is possible to achieve similar, or improved worst-case performance over a union of threat models while utilizing only single-step attacks, thereby achieving a significant reduction in computational resources necessary for training. Furthermore, prior work showed that adversarial training specific to the $\\ell_1$ threat model is relatively difficult, to the extent that even multi-step adversarially trained models were shown to be prone to gradient-masking. However, the proposed method—when applied on the $\\ell_1$ threat model specifically—enables us to obtain the first $\\ell_1$ robust model trained solely with single-step adversaries. Finally, to demonstrate the merits of our approach, we utilize a modern set of attack evaluations to better estimate the worst-case performance under the considered union of threat models.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/52908",
      "pdf_url": "https://openreview.net/pdf?id=6qdUJblMHqy",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurang_Sriramanan1",
        "name": "Gaurang Sriramanan",
        "name_site": "Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, Venkatesh Babu R",
        "openreview_id": "~Gaurang_Sriramanan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://gaurangsriramanan.github.io/",
        "dblp_id": "262/3916",
        "google_scholar_url": "t76Uk8oAAAAJ",
        "orcid": null,
        "linkedin_url": "gaurang-sriramanan-16141a1a3/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.7853571071357126,
        "confidence_mean": 4.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9DYKrsFSU2",
      "title": "Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converges to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\\% across imbalanced datasets. The code is available at https://github.com/val-iisc/Saddle-LongTail.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53304",
      "pdf_url": "https://openreview.net/pdf?id=9DYKrsFSU2",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Rangwani1",
        "name": "Harsh Rangwani",
        "name_site": "Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, Venkatesh Babu R",
        "openreview_id": "~Harsh_Rangwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rangwani-harsh.github.io/about/",
        "dblp_id": "220/0991",
        "google_scholar_url": "OQK0WREAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CCahlgHoQG",
      "title": "Measures of Information Reflect Memorization Patterns",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize—and subsequently show—that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis in experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabeled in-distribution examples. Lastly, we demonstrate the utility of our findings for the problem of model selection.",
      "tldr": "Notions of information organization across neural activations allow us to characterize memorization behaviour in neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/53484",
      "pdf_url": "https://openreview.net/pdf?id=CCahlgHoQG",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rachit_Bansal1",
        "name": "Rachit Bansal",
        "name_site": "Rachit Bansal, Danish Pruthi, Yonatan Belinkov",
        "openreview_id": "~Rachit_Bansal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rachitbansal.github.io",
        "dblp_id": "228/6038",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7-x28WYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Delhi Technological University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "COAcbu3_k4U",
      "title": "Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The graph retrieval problem is to search in a large corpus of graphs for ones that are most similar to a query graph.  A common consideration for scoring similarity is the maximum common subgraph (MCS) between the query and corpus graphs, usually counting the number of common edges (i.e., MCES).  In some applications, it is also desirable that the common subgraph be connected, i.e., the maximum common connected subgraph (MCCS). Finding exact MCES and MCCS is intractable, but may be unnecessary if ranking corpus graphs by relevance is the goal.  We design fast and trainable neural functions that approximate MCES and MCCS well.  Late interaction methods compute dense representations for the query and corpus graph separately, and compare these representations using simple similarity functions at the last stage, leading to highly scalable systems.  Early interaction methods combine information from both graphs right from the input stages, are usually considerably more accurate, but slower.  We propose both late and early interaction neural MCES and MCCS formulations.  They are both based on a continuous relaxation of a node alignment matrix between query and corpus nodes.  For MCCS, we propose a novel differentiable network for estimating the size of the largest connected common subgraph.  Extensive experiments with seven data sets show that our proposals are superior among late interaction models in terms of both accuracy and speed.  Our early interaction models provide accuracy competitive with the state of the art, at substantially greater speeds.",
      "tldr": "It learns late and early interaction models for maximum common subgraph based graph retrieval. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53399",
      "pdf_url": "https://openreview.net/pdf?id=COAcbu3_k4U",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Indradyumna_Roy1",
        "name": "Indradyumna Roy",
        "name_site": "Indradyumna Roy, Soumen Chakrabarti, Abir De",
        "openreview_id": "~Indradyumna_Roy1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://indradyumna.github.io/",
        "dblp_id": "124/9185.html",
        "google_scholar_url": "qb70i84AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CTqjKUAyRBt",
      "title": "Sampling without Replacement Leads to Faster Rates in Finite-Sum Minimax Optimization",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "We analyze the convergence rates of stochastic gradient algorithms for smooth finite-sum minimax optimization and show that, for many such algorithms, sampling the data points \\emph{without replacement} leads to faster convergence compared to sampling with replacement. For the smooth and strongly convex-strongly concave setting, we consider gradient descent ascent and the proximal point method, and present a unified analysis of two popular without-replacement sampling strategies, namely \\emph{Random Reshuffling} (RR), which shuffles the data every epoch, and \\emph{Single Shuffling} or \\emph{Shuffle Once} (SO), which shuffles only at the beginning. We obtain tight convergence rates for RR and SO and demonstrate that these strategies lead to faster convergence than uniform sampling. Moving beyond convexity, we obtain similar results for smooth nonconvex-nonconcave objectives satisfying a two-sided Polyak-\\L{}ojasiewicz inequality. Finally, we demonstrate that our techniques are general enough to analyze the effect of \\emph{data-ordering attacks}, where an adversary manipulates the order in which data points are supplied to the optimizer. Our analysis also recovers tight rates for the \\emph{incremental gradient} method, where the data points are not shuffled at all.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54898",
      "pdf_url": "https://openreview.net/pdf?id=CTqjKUAyRBt",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Das1",
        "name": "Aniket Das",
        "name_site": "Aniket Das, Bernhard Schölkopf, Michael Muehlebach",
        "openreview_id": "~Aniket_Das1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://aniket1998.github.io",
        "dblp_id": "248/8281",
        "google_scholar_url": "o8Dyas0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EqZuN4V_FLF",
      "title": "A Solver-free Framework for Scalable Learning in Neural ILP Architectures",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There is a recent focus on designing architectures that have an Integer Linear Programming (ILP) layer within a neural model (referred to as \\emph{Neural ILP} in this paper). Neural ILP architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning (ILP). A recent SOTA approach for end-to-end training of Neural ILP explicitly defines gradients through the ILP black box [Paulus et al. [2021]] – this trains extremely slowly, owing to a call to the underlying ILP solver for every training data point in a minibatch. In response, we present an alternative training strategy that is \\emph{solver-free}, i.e., does not call the ILP solver at all at training time. Neural ILP has a set of trainable hyperplanes (for cost and constraints in ILP), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation.  While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an ILP, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual Sudoku, to which the other Neural ILP solver is not able to scale.",
      "tldr": "For learning constraints in a neural ILP architecture, we propose a scalable solver-free framework that doesn't require calling the solver to compute gradients.",
      "site_url": "https://nips.cc/virtual/2022/poster/53251",
      "pdf_url": "https://openreview.net/pdf?id=EqZuN4V_FLF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Yatin_Nandwani1",
        "name": "Yatin Nandwani",
        "name_site": "Yatin Nandwani, Rishabh Ranjan, - Mausam, Parag Singla",
        "openreview_id": "~Yatin_Nandwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~yatin",
        "dblp_id": "255/7046",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "yatin-nandwani-0804ba9/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.0,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "HCnb1TByvx7",
      "title": "Multilingual Abusive Comment Detection at Scale for Indic Languages",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Social media platforms were conceived to act as online `town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into `mosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.\nTo facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. ",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55666",
      "pdf_url": "https://openreview.net/pdf?id=HCnb1TByvx7",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Vikram_Gupta1",
        "name": "Vikram Gupta",
        "name_site": "Vikram Gupta, Sumegh Roychowdhury, Mithun Das, Somnath Banerjee, Punyajoy Saha, Binny Mathew, hastagiri prakash vanchinathan, Animesh Mukherjee",
        "openreview_id": "~Vikram_Gupta1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "65/6215",
        "google_scholar_url": "jNjvdEgAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 4.2,
        "confidence_std": 0.9797958971132712,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 27,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K8JngctQ2Tu",
      "title": "Discovering and Overcoming Limitations of Noise-engineered Data-free Knowledge Distillation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Distillation in neural networks using only the samples randomly drawn from a Gaussian distribution is possibly the most straightforward solution one can think of for the complex problem of knowledge transfer from one network (teacher) to the other (student). If successfully done, it can eliminate the requirement of teacher's training data for knowledge distillation and avoid often arising privacy concerns in sensitive applications such as healthcare. There have been some recent attempts at Gaussian noise-based data-free knowledge distillation, however, none of them offer a consistent or reliable solution. We identify the shift in the distribution of hidden layer activation as the key limiting factor, which occurs when Gaussian noise is fed to the teacher network instead of the accustomed training data. We propose a simple solution to mitigate this shift and show that for vision tasks, such as classification, it is possible to achieve a performance close to the teacher by just using the samples randomly drawn from a Gaussian distribution. We validate our approach on CIFAR10, CIFAR100, SVHN, and Food101 datasets. We further show that in situations of sparsely available original data for distillation, the proposed Gaussian noise-based knowledge distillation method can outperform the distillation using the available data with a large margin. Our work lays the foundation for further research in the direction of noise-engineered knowledge distillation using random samples.",
      "tldr": "An approach to show that data-free knowledge distillation can be done using only the samples randomly drawn from a standard Gaussian distribution.",
      "site_url": "https://nips.cc/virtual/2022/poster/53095",
      "pdf_url": "https://openreview.net/pdf?id=K8JngctQ2Tu",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Piyush_Raikwar1",
        "name": "Piyush Raikwar",
        "name_site": "Piyush Raikwar, Deepak Mishra",
        "openreview_id": "~Piyush_Raikwar1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://piyush-555.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Atal Bihari Vajpayee Indian Institute of Information Technology and Management (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "NYpU9BRODos",
      "title": "Accelerated Training of Physics-Informed Neural Networks (PINNs) using Meshless Discretizations",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Physics-informed neural networks (PINNs) are neural networks trained by using physical laws in the form of partial differential equations (PDEs) as soft constraints. We present a new technique for the accelerated training of PINNs that combines modern scientific computing techniques with machine learning: discretely-trained PINNs (DT-PINNs). The repeated computation of the partial derivative terms in the PINN loss functions via automatic differentiation during training is known to be computationally expensive, especially for higher-order derivatives. DT-PINNs are trained by replacing these exact spatial derivatives with high-order accurate numerical discretizations computed using meshless radial basis function-finite differences (RBF-FD) and applied via sparse-matrix vector multiplication. While in principle any high-order discretization may be used, the use of RBF-FD allows for DT-PINNs to be trained even on point cloud samples placed on irregular domain geometries. Additionally, though traditional PINNs (vanilla-PINNs) are typically stored and trained in 32-bit floating-point (fp32) on the GPU, we show that for DT-PINNs, using fp64 on the GPU leads to significantly faster training times than fp32 vanilla-PINNs with comparable accuracy. We demonstrate the efficiency and accuracy of DT-PINNs via a series of experiments. First, we explore the effect of network depth on both numerical and automatic differentiation of a neural network with random weights and show that RBF-FD approximations of third-order accuracy and above are more efficient while being sufficiently accurate. We then compare the DT-PINNs to vanilla-PINNs on both linear and nonlinear Poisson equations and show that DT-PINNs achieve similar losses with 2-4x faster training times on a consumer GPU. Finally, we also demonstrate that similar results can be obtained for the PINN solution to the heat equation (a space-time problem) by discretizing the spatial derivatives using RBF-FD and using automatic differentiation for the temporal derivative. Our results show that fp64 DT-PINNs offer a superior cost-accuracy profile to fp32 vanilla-PINNs, opening the door to a new paradigm of leveraging scientific computing techniques to support machine learning.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53689",
      "pdf_url": "https://openreview.net/pdf?id=NYpU9BRODos",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ramansh_Sharma1",
        "name": "Ramansh Sharma",
        "name_site": "Ramansh Sharma, Varun Shankar",
        "openreview_id": "~Ramansh_Sharma1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ramanshsharma2806.github.io/",
        "dblp_id": "311/4652",
        "google_scholar_url": "lUmqHckAAAAJ",
        "orcid": "0000-0003-2645-8338",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "SRM Institute of Science and Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 1.699673171197595,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 47,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ODkBI1d3phW",
      "title": "Efficient and Effective Augmentation Strategy for Adversarial Training",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the \\emph{joint} learning of the \\emph{diverse augmentations}, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. \nThe code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT",
      "tldr": "We propose an effective augmentation strategy for Adversarial Training that can be integrated with several Adversarial Training algorithms and data augmentations.",
      "site_url": "https://nips.cc/virtual/2022/poster/54553",
      "pdf_url": "https://openreview.net/pdf?id=ODkBI1d3phW",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sravanti_Addepalli1",
        "name": "Sravanti Addepalli",
        "name_site": null,
        "openreview_id": "~Sravanti_Addepalli1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "127/7715",
        "google_scholar_url": "MOO12i0AAAAJ",
        "orcid": null,
        "linkedin_url": "sravanti-addepalli/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 62,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UXPXs-OYbks",
      "title": "Robustness Disparities in Face Detection",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are masculine presenting, older, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55765",
      "pdf_url": "https://openreview.net/pdf?id=UXPXs-OYbks",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Samuel_Dooley1",
        "name": "Samuel Dooley",
        "name_site": null,
        "openreview_id": "~Samuel_Dooley1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 1.6,
        "confidence_mean": 4.0,
        "confidence_std": 1.0954451150103321,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "XdMusblCkB",
      "title": "Causality Preserving Chaotic Transformation and Classification using Neurochaos Learning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Discovering cause and effect variables from observational data is an important but challenging problem in science and engineering. In this work, a recently proposed brain inspired learning algorithm namely-\\emph{Neurochaos Learning} (NL) is used for the classification of cause and effect time series generated using coupled autoregressive processes, coupled 1D chaotic skew tent maps, coupled 1D chaotic logistic maps and a real-world prey-predator system. In the case of coupled skew tent maps, the proposed method consistently outperforms a five layer Deep Neural Network (DNN) and Long Short Term Memory (LSTM) architecture for unidirectional coupling coefficient values ranging from $0.1$ to $0.7$. Further, we investigate the preservation of causality in the feature extracted space of NL using Granger Causality for coupled autoregressive processes and Compression-Complexity Causality for coupled chaotic systems and real-world prey-predator dataset. Unlike DNN, LSTM and 1D Convolutional Neural Network, it is found that NL preserves the inherent causal structures present in the input timeseries data. These findings are promising for the theory and applications of causal machine learning and open up the possibility to explore the potential of NL for more sophisticated causal learning tasks.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54299",
      "pdf_url": "https://openreview.net/pdf?id=XdMusblCkB",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harikrishnan_N_B1",
        "name": "Harikrishnan N B",
        "name_site": "Harikrishnan N B, Aditi Kathpalia, Nithin Nagaraj",
        "openreview_id": "~Harikrishnan_N_B1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/harikrishnannb8/home",
        "dblp_id": null,
        "google_scholar_url": "9fMmKMEAAAAJ",
        "orcid": "0000-0002-4575-3968",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Advanced Studies (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.7320508075688772,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ZYKWi6Ylfg",
      "title": "Harmonizing the object recognition strategies of deep neural networks with humans",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining the visual strategies humans rely on for object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. \\textit{State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves}. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy. Our work represents the first demonstration that the scaling laws that are guiding the design of DNNs today have also produced worse models of human vision. We release our code and data at https://serre-lab.github.io/Harmonization to help the field build more human-like DNNs.\n",
      "tldr": "The scaling laws that are improving deep neural network performance on ImageNet are leading to worse models of human object recognition.",
      "site_url": "https://nips.cc/virtual/2022/poster/55296",
      "pdf_url": "https://openreview.net/pdf?id=ZYKWi6Ylfg",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 4.0,
        "rating_std": 1.8708286933869707,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 85,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_D4cE66L9x3",
      "title": "Byzantine Spectral Ranking",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "We study the problem of rank aggregation where the goal is to obtain a global ranking by aggregating pair-wise comparisons of voters over a set of items. We consider an adversarial setting where the voters are partitioned into two sets. The first set votes in a stochastic manner according to the popular score-based Bradley-Terry-Luce (BTL) model for pairwise comparisons. The second set comprises malicious Byzantine voters trying to deteriorate the ranking. We consider a strongly-adversarial scenario where the Byzantine voters know the BTL scores, the votes of the good voters, the algorithm, and can collude with each other. We first show that the popular spectral ranking based Rank-Centrality algorithm, though optimal for the BTL model, does not perform well even when a small constant fraction of the voters are Byzantine.\n\nWe introduce the Byzantine Spectral Ranking Algorithm (and a faster variant of it), which produces a reliable ranking when the number of good voters exceeds the number of Byzantine voters. We show that no algorithm can produce a satisfactory ranking with probability > 1/2 for all BTL weights when there are more Byzantine voters than good voters, showing that our algorithm works for all possible population fractions. We support our theoretical results with experimental results on synthetic and real datasets to demonstrate the failure of the Rank-Centrality algorithm under several adversarial scenarios and how the proposed Byzantine Spectral Ranking algorithm is robust in obtaining good rankings.",
      "tldr": "A novel theoretically sound pairwise ranking algorithm that is robust to Byzantine voters.",
      "site_url": "https://nips.cc/virtual/2022/poster/53351",
      "pdf_url": "https://openreview.net/pdf?id=_D4cE66L9x3",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arnhav_Datar1",
        "name": "Arnhav Datar",
        "name_site": "Arnhav Datar, Arun Rajkumar, John Augustine",
        "openreview_id": "~Arnhav_Datar1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/arnhav-datar/home",
        "dblp_id": null,
        "google_scholar_url": "nzFIgWkAAAAJ",
        "orcid": null,
        "linkedin_url": "arnhav-datar-a550b4174/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aPgQdvSAuw",
      "title": "On Translation and Reconstruction Guarantees of the Cycle-Consistent Generative Adversarial Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The task of unpaired image-to-image translation has witnessed a revolution with the introduction of the cycle-consistency loss to Generative Adversarial Networks (GANs). Numerous variants, with Cycle-Consistent Adversarial Network (CycleGAN) at their forefront, have shown remarkable empirical performance. The involvement of two unalike data spaces and the existence of multiple solution maps between them are some of the facets that make such architectures unique. In this study, we investigate the statistical properties of such unpaired data translator networks between distinct spaces, bearing the additional responsibility of cycle-consistency. In a density estimation setup, we derive sharp non-asymptotic bounds on the translation errors under suitably characterized models. This, in turn, points out sufficient regularity conditions that maps must obey to carry out successful translations. We further show that cycle-consistency is achieved as a consequence of the data being successfully generated in each space based on observations from the other. In a first-of-its-kind attempt, we also provide deterministic bounds on the cumulative reconstruction error. In the process, we establish tolerable upper bounds on the discrepancy responsible for ill-posedness in such networks.",
      "tldr": "On Statistical Theory of Cycle Consistent I2I Traslators",
      "site_url": "https://nips.cc/virtual/2022/poster/54131",
      "pdf_url": "https://openreview.net/pdf?id=aPgQdvSAuw",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anish_Chakrabarty1",
        "name": "Anish Chakrabarty",
        "name_site": "Anish Chakrabarty, Swagatam Das",
        "openreview_id": "~Anish_Chakrabarty1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "304/5289",
        "google_scholar_url": "KfCQY5oAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 2.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bGo0A4bJBc",
      "title": "Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training.  Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks.  Our results demonstrate that CSST achieves an improvement over the state-of-the-art in majority of the cases across datasets and objectives.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55114",
      "pdf_url": "https://openreview.net/pdf?id=bGo0A4bJBc",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Rangwani1",
        "name": "Harsh Rangwani",
        "name_site": "Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, Venkatesh Babu R",
        "openreview_id": "~Harsh_Rangwani1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://rangwani-harsh.github.io/about/",
        "dblp_id": "220/0991",
        "google_scholar_url": "OQK0WREAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 2.160246899469287,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hYa_lseXK8",
      "title": "Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "During initial iterations of training in most Reinforcement Learning (RL) algorithms, agents perform a significant number of random exploratory steps. In the real world, this can limit the practicality of these algorithms as it can lead to potentially dangerous behavior. Hence safe exploration is a critical issue in applying RL algorithms in the real world. This problem has been recently well studied under the Constrained Markov Decision Process (CMDP) Framework, where in addition to single-stage rewards, an agent receives single-stage costs or penalties as well depending on the state transitions. The prescribed  cost functions are responsible for mapping undesirable behavior at any given time-step to a scalar value. The goal then is to find a feasible policy that maximizes reward returns while constraining the cost returns to be below a prescribed threshold during training as well as deployment.\n\nWe propose an On-policy Model-based Safe Deep RL algorithm in which we learn the transition dynamics of the environment in an online manner as well as find a feasible optimal policy using the Lagrangian Relaxation-based Proximal Policy Optimization. We use an ensemble of neural networks with different initializations to tackle epistemic and aleatoric uncertainty issues faced during environment model learning.  We compare our approach with relevant model-free and model-based approaches in Constrained RL using the  challenging Safe Reinforcement Learning benchmark - the Open AI Safety Gym.  \nWe demonstrate that our algorithm is more sample efficient and results in lower  cumulative hazard violations as compared to constrained model-free approaches. Further, our approach shows better reward performance than other constrained model-based approaches in the literature. ",
      "tldr": "Model based approach for Safe Reinforcement Learning that improves sample efficiency and reduces cumulative hazard violations.",
      "site_url": "https://nips.cc/virtual/2022/poster/55095",
      "pdf_url": "https://openreview.net/pdf?id=hYa_lseXK8",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_Kumar_Jayant1",
        "name": "Ashish Kumar Jayant",
        "name_site": "Ashish K Jayant, Shalabh Bhatnagar",
        "openreview_id": "~Ashish_Kumar_Jayant1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://akjayant.github.io",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "https://linkedin.com/in/akjayant",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mq-8p5pUnEX",
      "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector.  By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence.  Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness.  We propose a solution which divides computation into two streams.  A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors.  At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream.  In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks. \n",
      "tldr": "Introducing two streams - a fast and a slow stream -  of processing into a transformer to achieve better compression of information while maintaining high expressivity.",
      "site_url": "https://nips.cc/virtual/2022/poster/53048",
      "pdf_url": "https://openreview.net/pdf?id=mq-8p5pUnEX",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Rajiv_Didolkar1",
        "name": "Aniket Rajiv Didolkar",
        "name_site": null,
        "openreview_id": "~Aniket_Rajiv_Didolkar1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://github.com/dido1998/",
        "dblp_id": "245/8589",
        "google_scholar_url": "https://scholar.google.ca/citations?user=ekvl5o0AAAAJ",
        "orcid": null,
        "linkedin_url": "aniket-didolkar-7a9b8912a",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Manipal Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nOdfIbo3A-F",
      "title": "Learning Articulated Rigid Body Dynamics with Lagrangian Graph Neural Network",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Lagrangian  and Hamiltonian neural networks LNN and HNNs, respectively) encode strong inductive biases that allow them to outperform other models of physical systems significantly. However, these models have, thus far, mostly been limited to simple systems such as pendulums and springs or a single rigid body such as a gyroscope or a rigid rotor. Here, we present a Lagrangian graph neural network (LGNN) that can learn the dynamics of articulated rigid bodies by exploiting their topology. We demonstrate the performance of LGNN by learning the dynamics of ropes, chains, and trusses with the bars modeled as rigid bodies. LGNN also exhibits generalizability---LGNN trained on chains with a few segments exhibits generalizability to simulate a chain with large number of links and arbitrary link length. We also show that the LGNN can simulate unseen hybrid systems including bars and chains, on which they have not been trained on. Specifically, we show that the LGNN can be used to model the dynamics of complex real-world structures such as the stability of tensegrity structures. Finally, we discuss the non-diagonal nature of the mass matrix and its ability to generalize in complex systems.",
      "tldr": "We present a Lagrangian graph neural network that can learn the dynamics of rigid body and generalize to arbitrary system sizes",
      "site_url": "https://nips.cc/virtual/2022/poster/53306",
      "pdf_url": "https://openreview.net/pdf?id=nOdfIbo3A-F",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ravinder_Bhattoo1",
        "name": "Ravinder Bhattoo",
        "name_site": "Ravinder Bhattoo, Sayan Ranu, N M Anoop Krishnan",
        "openreview_id": "~Ravinder_Bhattoo1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ravinderbhattoo.github.io",
        "dblp_id": null,
        "google_scholar_url": "lPTdGRMAAAAJ",
        "orcid": "0000-0003-0323-9108",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.479019945774904,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 31,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nrOLtfeiIdh",
      "title": "Learning Recourse on Instance Environment to Enhance Prediction Accuracy",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of  alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called {\\em RecourseNet} that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier.   Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent --- for example the best camera angle for object recognition could be a function of the object's shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence.",
      "tldr": "Learning to recourse instances through interventions on the environment space so that the recoursed instances deliver enhanced prediction accuracy by the downstream model. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53322",
      "pdf_url": "https://openreview.net/pdf?id=nrOLtfeiIdh",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Lokesh_Nagalapatti1",
        "name": "Lokesh Nagalapatti",
        "name_site": "Lokesh N, Guntakanti Sai Koushik, Abir De, Sunita Sarawagi",
        "openreview_id": "~Lokesh_Nagalapatti1",
        "position": 1,
        "gender": null,
        "homepage_url": "https://nlokesh.netlify.app/",
        "dblp_id": "259/2681.html",
        "google_scholar_url": "BkkZbo0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pluyPFTiTeJ",
      "title": "Domain Generalization without Excess Empirical Risk",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, \\ie, in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization method, and we demonstrate its effectiveness by applying it to three examplar methods from the literature, showing significant improvements.",
      "tldr": "We propose an optimization algorithm for penalty-based domain generalization methods. Our proposed optimizer minimizes the domain generalization penalty under the constraint that empirical risk is optimal.",
      "site_url": "https://nips.cc/virtual/2022/poster/54052",
      "pdf_url": "https://openreview.net/pdf?id=pluyPFTiTeJ",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ozan_Sener1",
        "name": "Ozan Sener",
        "name_site": "Ozan Sener, Vladlen Koltun",
        "openreview_id": "~Ozan_Sener1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://ozansener.net",
        "dblp_id": "125/1989",
        "google_scholar_url": "BI8xFr4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Intel (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rDT-n9xysO",
      "title": "Symbolic Distillation for Learned TCP Congestion Control",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recent advances in TCP congestion control (CC) have achieved tremendous success with deep reinforcement learning (RL) approaches, which use feedforward neural networks (NN) to learn complex environment conditions and make better decisions. However, such ``black-box'' policies lack interpretability and reliability, and often, they need to operate outside the traditional TCP datapath due to the use of complex NNs. This paper proposes a novel two-stage solution to achieve the best of both worlds: first to train a deep RL agent, then distill its (over-)parameterized NN policy into white-box, light-weight rules in the form of symbolic expressions that are much easier to understand and to implement in constrained environments. At the core of our proposal is a novel symbolic branching algorithm that enables the rule to be aware of the context in terms of various network conditions, eventually converting the NN policy into a symbolic tree. The distilled symbolic rules preserve and often improve performance over state-of-the-art NN policies while being faster and simpler than a standard neural network. We validate the performance of our distilled symbolic rules on both simulation and emulation environments. Our code is available at https://github.com/VITA-Group/SymbolicPCC.",
      "tldr": "Improved the efficiency and interpretability of neural network based TCP congestion controller via a novel symbolic regression method.",
      "site_url": "https://nips.cc/virtual/2022/poster/54408",
      "pdf_url": "https://openreview.net/pdf?id=rDT-n9xysO",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~S_P_Sharan1",
        "name": "S P Sharan",
        "name_site": "S P Sharan, Wenqing Zheng, Kuo-Feng Hsu, Jiarong Xing, Ang Chen, Zhangyang Wang",
        "openreview_id": "~S_P_Sharan1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://spsharan.com/",
        "dblp_id": "324/6204",
        "google_scholar_url": "1NtGcNIAAAAJ",
        "orcid": "0000-0002-6298-6464",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Technology Tiruchirappalli (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Abishek_Thangamuthu1",
        "name": "Abishek Thangamuthu",
        "name_site": "Abishek Thangamuthu, Gunjan Kumar, Suresh Bishnoi, Ravinder Bhattoo, N M Anoop Krishnan, Sayan Ranu",
        "openreview_id": "~Abishek_Thangamuthu1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "abishek-thangamuthu/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "v7SFDrS44Cf",
      "title": "Neural Estimation of Submodular Functions with Applications to Differentiable Subset Selection",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Submodular functions and variants, through their ability to characterize diversity and coverage, have emerged as a key tool for data selection and summarization.  Many recent approaches to learn submodular functions suffer from limited expressiveness. In this work, we propose FlexSubNet, a family of flexible neural models for both monotone and non-monotone submodular functions. To fit a latent submodular function from (set, value) observations, our method applies a concave function on modular functions in a recursive manner. We do not draw the concave function from a restricted family, but rather learn from data using a highly expressive neural network that implements a differentiable quadrature procedure. Such an expressive neural model for concave functions may be of independent interest.  Next, we extend this setup to provide a novel characterization of monotone $\\alpha$-submodular functions, a recently introduced notion of approximate submodular functions.  We then use this characterization to design a novel neural model for such functions. Finally, we consider learning submodular set functions under distant supervision in the form of  (perimeter, high-value-subset) pairs.  This yields a novel subset selection method based on an order-invariant, yet greedy sampler built around the above neural set functions. Our experiments on synthetic and real data show that FlexSubNet outperforms several baselines.\n",
      "tldr": "We design novel neural models for submodular functions and an efficient differentiable methods towards differentiable data subset selection",
      "site_url": "https://nips.cc/virtual/2022/poster/53269",
      "pdf_url": "https://openreview.net/pdf?id=v7SFDrS44Cf",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wfKbtSjHA6F",
      "title": "Sparse Winning Tickets are Data-Efficient Image Recognizers",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Improving the performance of deep networks in data-limited regimes has warranted much attention. In this work, we empirically show that “winning tickets” (small sub-networks) obtained via magnitude pruning based on the lottery ticket hypothesis, apart from being sparse are also effective recognizers in data-limited regimes. Based on extensive experiments, we find that in low data regimes (datasets of 50-100 examples per class), sparse winning tickets substantially outperform the original dense networks. This approach, when combined with augmentations or fine-tuning from a self-supervised backbone network, shows further improvements in performance by as much as 16% (absolute) on low-sample datasets and long-tailed classification. Further, sparse winning tickets are more robust to synthetic noise and distribution shifts compared to their dense counterparts. Our analysis of winning tickets on small datasets indicates that, though sparse, the networks retain density in the initial layers and their representations are more generalizable. Code is available at https://github.com/VITA-Group/DataEfficientLTH.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54192",
      "pdf_url": "https://openreview.net/pdf?id=wfKbtSjHA6F",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mukund_Varma_T1",
        "name": "Mukund Varma T",
        "name_site": "Mukund Varma T, Xuxi Chen, Zhenyu Zhang, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang",
        "openreview_id": "~Mukund_Varma_T1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mukundvarmat/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xbhsFMxORxV",
      "title": "Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social Text Classification",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Social media has become the fulcrum of all forms of communication. Classifying social texts such as fake news, rumour, sarcasm, etc. has gained significant attention. The surface-level signals expressed by a social-text itself may not be adequate for such tasks; therefore, recent methods attempted to incorporate other intrinsic signals such as user behavior and the underlying graph structure. Oftentimes, the public wisdom expressed through the comments/replies to a social-text acts as a surrogate of crowd-sourced view and may provide us with complementary signals. State-of-the-art methods on social-text classification tend to ignore such a rich hierarchical signal. Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention network. Hyphen is a fusion of hyperbolic graph representation learning with a novel Fourier co-attention mechanism in an attempt to generalise the social-text classification tasks by incorporating public discourse. We parse public discourse as an Abstract Meaning Representation (AMR) graph and use the powerful hyperbolic geometric representation to model graphs with hierarchical structure. Finally, we equip it with a novel Fourier co-attention mechanism to capture the correlation between the source post and public discourse. Extensive experiments on four different social-text classification tasks, namely detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen generalises well, and achieves state-of-the-art results on ten benchmark datasets. We also employ a sentence-level fact-checked and annotated dataset to evaluate how Hyphen is capable of producing explanations as analogous evidence to the final prediction.",
      "tldr": "Hyperbolic spectral co-attention for public discourse-aware social media text classification.",
      "site_url": "https://nips.cc/virtual/2022/poster/53312",
      "pdf_url": "https://openreview.net/pdf?id=xbhsFMxORxV",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Karish_Grover1",
        "name": "Karish Grover",
        "name_site": "Karish Grover, S M Phaneendra Angara, Md Shad Akhtar, Tanmoy Chakraborty",
        "openreview_id": "~Karish_Grover1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yRhbHp_Vh8e",
      "title": "Grounded Video Situation Recognition",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Dense video understanding requires answering several questions such as who is doing what to whom, with what, how, why, and where. Recently, Video Situation Recognition (VidSitu) is framed as a task for structured prediction of multiple events, their relationships, and actions and various verb-role pairs attached to descriptive entities. This task poses several challenges in identifying, disambiguating, and co-referencing entities across multiple verb-role pairs, but also faces some challenges of evaluation. In this work, we propose the addition of spatio-temporal grounding as an essential component of the structured prediction task in a weakly supervised setting, and present a novel three stage Transformer model, VideoWhisperer, that is empowered to make joint predictions. In stage one, we learn contextualised embeddings for video features in parallel with key objects that appear in the video clips to enable fine-grained spatio-temporal reasoning. The second stage sees verb-role queries attend and pool information from object embeddings, localising answers to questions posed about the action. The final stage generates these answers as captions to describe each verb-role pair present in the video. Our model operates on a group of events (clips) simultaneously and predicts verbs, verb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on a grounding-augmented version of the VidSitu dataset, we observe a large improvement in entity captioning accuracy, as well as the ability to localize verb-roles without grounding annotations at training time.",
      "tldr": "We present a new task Grounded Video Situation Recognition(GVSR). In addition to predicting the verbs, and semantic roles in the form of captions, we also ground them in the spatio-temporal domain in weakly-supervised setup in an end-to-end fashion. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53297",
      "pdf_url": "https://openreview.net/pdf?id=yRhbHp_Vh8e",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Zeeshan_Khan1",
        "name": "Zeeshan Khan",
        "name_site": "Zeeshan Khan, C.V. Jawahar, Makarand Tapaswi",
        "openreview_id": "~Zeeshan_Khan1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "44/11265",
        "google_scholar_url": "uvhBVYoAAAAJ",
        "orcid": null,
        "linkedin_url": "khan-zeeshan-606-",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Gokul_NC1",
        "name": "Gokul NC",
        "name_site": "Gokul NC, Manideep Ladi, Sumit Negi, Prem Selvaraj, Pratyush Kumar, Mitesh Khapra",
        "openreview_id": "~Gokul_NC1",
        "position": 1,
        "gender": "Optimus Prime",
        "homepage_url": "https://github.com/GokulNC",
        "dblp_id": null,
        "google_scholar_url": "jUSyHaUAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "AI4Bharat (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3LBxVcnsEkV",
      "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline.",
      "tldr": "Learning graph and subgraph edit distance using graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/54507",
      "pdf_url": "https://openreview.net/pdf?id=3LBxVcnsEkV",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Varun_Jampani1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://varunjampani.github.io/",
        "dblp_id": "124/2785",
        "google_scholar_url": "1Cv6Sf4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9DYKrsFSU2",
      "title": "Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converges to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\\% across imbalanced datasets. The code is available at https://github.com/val-iisc/Saddle-LongTail.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53304",
      "pdf_url": "https://openreview.net/pdf?id=9DYKrsFSU2",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2_1",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Sumukh_Aithal_K1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sumukhaithal6.github.io/",
        "dblp_id": "299/5911",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": null,
        "linkedin_url": "sumukh-aithal-9801b4189",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "PES University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "COAcbu3_k4U",
      "title": "Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The graph retrieval problem is to search in a large corpus of graphs for ones that are most similar to a query graph.  A common consideration for scoring similarity is the maximum common subgraph (MCS) between the query and corpus graphs, usually counting the number of common edges (i.e., MCES).  In some applications, it is also desirable that the common subgraph be connected, i.e., the maximum common connected subgraph (MCCS). Finding exact MCES and MCCS is intractable, but may be unnecessary if ranking corpus graphs by relevance is the goal.  We design fast and trainable neural functions that approximate MCES and MCCS well.  Late interaction methods compute dense representations for the query and corpus graph separately, and compare these representations using simple similarity functions at the last stage, leading to highly scalable systems.  Early interaction methods combine information from both graphs right from the input stages, are usually considerably more accurate, but slower.  We propose both late and early interaction neural MCES and MCCS formulations.  They are both based on a continuous relaxation of a node alignment matrix between query and corpus nodes.  For MCCS, we propose a novel differentiable network for estimating the size of the largest connected common subgraph.  Extensive experiments with seven data sets show that our proposals are superior among late interaction models in terms of both accuracy and speed.  Our early interaction models provide accuracy competitive with the state of the art, at substantially greater speeds.",
      "tldr": "It learns late and early interaction models for maximum common subgraph based graph retrieval. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53399",
      "pdf_url": "https://openreview.net/pdf?id=COAcbu3_k4U",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EqZuN4V_FLF",
      "title": "A Solver-free Framework for Scalable Learning in Neural ILP Architectures",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There is a recent focus on designing architectures that have an Integer Linear Programming (ILP) layer within a neural model (referred to as \\emph{Neural ILP} in this paper). Neural ILP architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning (ILP). A recent SOTA approach for end-to-end training of Neural ILP explicitly defines gradients through the ILP black box [Paulus et al. [2021]] – this trains extremely slowly, owing to a call to the underlying ILP solver for every training data point in a minibatch. In response, we present an alternative training strategy that is \\emph{solver-free}, i.e., does not call the ILP solver at all at training time. Neural ILP has a set of trainable hyperplanes (for cost and constraints in ILP), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation.  While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an ILP, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual Sudoku, to which the other Neural ILP solver is not able to scale.",
      "tldr": "For learning constraints in a neural ILP architecture, we propose a scalable solver-free framework that doesn't require calling the solver to compute gradients.",
      "site_url": "https://nips.cc/virtual/2022/poster/53251",
      "pdf_url": "https://openreview.net/pdf?id=EqZuN4V_FLF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Parag_Singla1",
        "name": "Parag Singla",
        "name_site": null,
        "openreview_id": "~Mausam_Mausam2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~mausam",
        "dblp_id": "30/6391.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-4088-4296",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.0,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "FHgpw2Cn__",
      "title": "Consistency of Constrained Spectral Clustering under Graph Induced Fair Planted Partitions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Spectral clustering is popular among practitioners and theoreticians alike. While performance guarantees for spectral clustering are well understood, recent studies have focused on enforcing \"fairness\" in clusters, requiring them to be \"balanced\" with respect to a categorical sensitive node attribute (e.g. the race distribution in clusters must match the race distribution in the population). In this paper, we consider a setting where sensitive attributes indirectly manifest in an auxiliary representation graph rather than being directly observed. This graph specifies node pairs that can represent each other with respect to sensitive attributes and is observed in addition to the usual similarity graph. Our goal is to find clusters in the similarity graph while respecting a new individual-level fairness constraint encoded by the representation graph. We develop variants of unnormalized and normalized spectral clustering for this task and analyze their performance under a fair planted partition model induced by the representation graph. This model uses both the cluster membership of the nodes and the structure of the representation graph to generate random similarity graphs. To the best of our knowledge, these are the first consistency results for constrained spectral clustering under an individual-level fairness constraint. Numerical results corroborate our theoretical findings.",
      "tldr": "A new individual level fairness constraint for clustering and accompanying spectral algorithms with guarantees.",
      "site_url": "https://nips.cc/virtual/2022/poster/53419",
      "pdf_url": "https://openreview.net/pdf?id=FHgpw2Cn__",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ambedkar_Dukkipati1",
        "name": "Ambedkar Dukkipati",
        "name_site": null,
        "openreview_id": "~Ambedkar_Dukkipati1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~ad",
        "dblp_id": "64/1176.html",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-6352-6283",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "K8JngctQ2Tu",
      "title": "Discovering and Overcoming Limitations of Noise-engineered Data-free Knowledge Distillation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Distillation in neural networks using only the samples randomly drawn from a Gaussian distribution is possibly the most straightforward solution one can think of for the complex problem of knowledge transfer from one network (teacher) to the other (student). If successfully done, it can eliminate the requirement of teacher's training data for knowledge distillation and avoid often arising privacy concerns in sensitive applications such as healthcare. There have been some recent attempts at Gaussian noise-based data-free knowledge distillation, however, none of them offer a consistent or reliable solution. We identify the shift in the distribution of hidden layer activation as the key limiting factor, which occurs when Gaussian noise is fed to the teacher network instead of the accustomed training data. We propose a simple solution to mitigate this shift and show that for vision tasks, such as classification, it is possible to achieve a performance close to the teacher by just using the samples randomly drawn from a Gaussian distribution. We validate our approach on CIFAR10, CIFAR100, SVHN, and Food101 datasets. We further show that in situations of sparsely available original data for distillation, the proposed Gaussian noise-based knowledge distillation method can outperform the distillation using the available data with a large margin. Our work lays the foundation for further research in the direction of noise-engineered knowledge distillation using random samples.",
      "tldr": "An approach to show that data-free knowledge distillation can be done using only the samples randomly drawn from a standard Gaussian distribution.",
      "site_url": "https://nips.cc/virtual/2022/poster/53095",
      "pdf_url": "https://openreview.net/pdf?id=K8JngctQ2Tu",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Deepak_Mishra5",
        "name": "Deepak Mishra",
        "name_site": null,
        "openreview_id": "~Deepak_Mishra5",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://home.iitj.ac.in/~dmishra/",
        "dblp_id": "65/6758-3",
        "google_scholar_url": "-rOCu6sAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Jodhpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ODkBI1d3phW",
      "title": "Efficient and Effective Augmentation Strategy for Adversarial Training",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the \\emph{joint} learning of the \\emph{diverse augmentations}, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. \nThe code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT",
      "tldr": "We propose an effective augmentation strategy for Adversarial Training that can be integrated with several Adversarial Training algorithms and data augmentations.",
      "site_url": "https://nips.cc/virtual/2022/poster/54553",
      "pdf_url": "https://openreview.net/pdf?id=ODkBI1d3phW",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Chakraborty1",
        "name": "Anirban Chakraborty",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 62,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_D4cE66L9x3",
      "title": "Byzantine Spectral Ranking",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "We study the problem of rank aggregation where the goal is to obtain a global ranking by aggregating pair-wise comparisons of voters over a set of items. We consider an adversarial setting where the voters are partitioned into two sets. The first set votes in a stochastic manner according to the popular score-based Bradley-Terry-Luce (BTL) model for pairwise comparisons. The second set comprises malicious Byzantine voters trying to deteriorate the ranking. We consider a strongly-adversarial scenario where the Byzantine voters know the BTL scores, the votes of the good voters, the algorithm, and can collude with each other. We first show that the popular spectral ranking based Rank-Centrality algorithm, though optimal for the BTL model, does not perform well even when a small constant fraction of the voters are Byzantine.\n\nWe introduce the Byzantine Spectral Ranking Algorithm (and a faster variant of it), which produces a reliable ranking when the number of good voters exceeds the number of Byzantine voters. We show that no algorithm can produce a satisfactory ranking with probability > 1/2 for all BTL weights when there are more Byzantine voters than good voters, showing that our algorithm works for all possible population fractions. We support our theoretical results with experimental results on synthetic and real datasets to demonstrate the failure of the Rank-Centrality algorithm under several adversarial scenarios and how the proposed Byzantine Spectral Ranking algorithm is robust in obtaining good rankings.",
      "tldr": "A novel theoretically sound pairwise ranking algorithm that is robust to Byzantine voters.",
      "site_url": "https://nips.cc/virtual/2022/poster/53351",
      "pdf_url": "https://openreview.net/pdf?id=_D4cE66L9x3",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~John_Augustine1",
        "name": "John Augustine",
        "name_site": null,
        "openreview_id": "~John_Augustine1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.iitm.ac.in/info/fac/augustine/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=4YaUTfYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "aPgQdvSAuw",
      "title": "On Translation and Reconstruction Guarantees of the Cycle-Consistent Generative Adversarial Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The task of unpaired image-to-image translation has witnessed a revolution with the introduction of the cycle-consistency loss to Generative Adversarial Networks (GANs). Numerous variants, with Cycle-Consistent Adversarial Network (CycleGAN) at their forefront, have shown remarkable empirical performance. The involvement of two unalike data spaces and the existence of multiple solution maps between them are some of the facets that make such architectures unique. In this study, we investigate the statistical properties of such unpaired data translator networks between distinct spaces, bearing the additional responsibility of cycle-consistency. In a density estimation setup, we derive sharp non-asymptotic bounds on the translation errors under suitably characterized models. This, in turn, points out sufficient regularity conditions that maps must obey to carry out successful translations. We further show that cycle-consistency is achieved as a consequence of the data being successfully generated in each space based on observations from the other. In a first-of-its-kind attempt, we also provide deterministic bounds on the cumulative reconstruction error. In the process, we establish tolerable upper bounds on the discrepancy responsible for ill-posedness in such networks.",
      "tldr": "On Statistical Theory of Cycle Consistent I2I Traslators",
      "site_url": "https://nips.cc/virtual/2022/poster/54131",
      "pdf_url": "https://openreview.net/pdf?id=aPgQdvSAuw",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Swagatam_Das2_1",
        "name": "Swagatam Das",
        "name_site": null,
        "openreview_id": "~Swagatam_Das1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.isical.ac.in/~swagatam.das/",
        "dblp_id": "00/3298.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=L8XYpAwAAAAJ",
        "orcid": "0000-0001-6843-4508",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Statistical Institute (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 2.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_K_Iyer2",
        "name": "Rishabh K Iyer",
        "name_site": null,
        "openreview_id": "~Rishabh_K_Iyer2",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://www.rishiyer.com",
        "dblp_id": "37/10544.html",
        "google_scholar_url": "l_XxJ1kAAAAJ",
        "orcid": null,
        "linkedin_url": "rishabh-iyer-36893717/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bGo0A4bJBc",
      "title": "Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training.  Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks.  Our results demonstrate that CSST achieves an improvement over the state-of-the-art in majority of the cases across datasets and objectives.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55114",
      "pdf_url": "https://openreview.net/pdf?id=bGo0A4bJBc",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Chakraborty1",
        "name": "Anirban Chakraborty",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 2.160246899469287,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hYa_lseXK8",
      "title": "Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "During initial iterations of training in most Reinforcement Learning (RL) algorithms, agents perform a significant number of random exploratory steps. In the real world, this can limit the practicality of these algorithms as it can lead to potentially dangerous behavior. Hence safe exploration is a critical issue in applying RL algorithms in the real world. This problem has been recently well studied under the Constrained Markov Decision Process (CMDP) Framework, where in addition to single-stage rewards, an agent receives single-stage costs or penalties as well depending on the state transitions. The prescribed  cost functions are responsible for mapping undesirable behavior at any given time-step to a scalar value. The goal then is to find a feasible policy that maximizes reward returns while constraining the cost returns to be below a prescribed threshold during training as well as deployment.\n\nWe propose an On-policy Model-based Safe Deep RL algorithm in which we learn the transition dynamics of the environment in an online manner as well as find a feasible optimal policy using the Lagrangian Relaxation-based Proximal Policy Optimization. We use an ensemble of neural networks with different initializations to tackle epistemic and aleatoric uncertainty issues faced during environment model learning.  We compare our approach with relevant model-free and model-based approaches in Constrained RL using the  challenging Safe Reinforcement Learning benchmark - the Open AI Safety Gym.  \nWe demonstrate that our algorithm is more sample efficient and results in lower  cumulative hazard violations as compared to constrained model-free approaches. Further, our approach shows better reward performance than other constrained model-based approaches in the literature. ",
      "tldr": "Model based approach for Safe Reinforcement Learning that improves sample efficiency and reduces cumulative hazard violations.",
      "site_url": "https://nips.cc/virtual/2022/poster/55095",
      "pdf_url": "https://openreview.net/pdf?id=hYa_lseXK8",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shalabh_Bhatnagar1",
        "name": "Shalabh Bhatnagar",
        "name_site": null,
        "openreview_id": "~Shalabh_Bhatnagar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~shalabh/",
        "dblp_id": "71/2542",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=cj3fJJsbjAoC",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 40,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "juE5ErmZB61",
      "title": "Polynomial Neural Fields for Subband Decomposition and Manipulation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Neural fields have emerged as a new paradigm for representing signals, thanks to their ability to do it compactly while being easy to optimize. In most applications, however, neural fields are treated like a black box, which precludes many signal manipulation tasks. In this paper, we propose a new class of neural fields called basis-encoded polynomial neural fields (PNFs). The key advantage of a PNF is that it can represent a signal as a composition of a number of manipulable and interpretable components without losing the merits of neural fields representation. We develop a general theoretical framework to analyze and design PNFs. We use this framework to design Fourier PNFs, which match state-of-the-art performance in signal representation tasks that use neural fields. In addition, we empirically demonstrate that Fourier PNFs enable signal manipulation applications such as texture transfer and scale-space interpolation. Code is available at https://github.com/stevenygd/PNF.",
      "tldr": "We propose Polynomial Neural Fields (PNF), a novel neural field architecture that enables decomposing and manipulating signals in terms of subbands.",
      "site_url": "https://nips.cc/virtual/2022/poster/55396",
      "pdf_url": "https://openreview.net/pdf?id=juE5ErmZB61",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Varun_Jampani1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://varunjampani.github.io/",
        "dblp_id": "124/2785",
        "google_scholar_url": "1Cv6Sf4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.75,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "mhP6mHgrg1c",
      "title": "ORIENT: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Real-world machine-learning applications require robust models that generalize well to distribution shift settings, which is typical in real-world situations. Domain adaptation techniques aim to address this issue of distribution shift by minimizing the disparities between domains to ensure that the model trained on the source domain performs well on the target domain. Nevertheless, the existing domain adaptation methods are computationally very expensive. In this work, we aim to improve the efficiency of existing supervised domain adaptation (SDA) methods by using a subset of source data that is similar to target data for faster model training. Specifically, we propose ORIENT, a subset selection framework that uses the submodular mutual information (SMI) functions to select a source data subset similar to the target data for faster training. Additionally, we demonstrate how existing robust subset selection strategies, such as GLISTER, GRADMATCH, and CRAIG, when used with a held-out query set, fit within our proposed framework and demonstrate the connections with them. Finally, we empirically demonstrate that SDA approaches like d-SNE, CCSA, and standard Cross-entropy training, when employed together with ORIENT, achieve a) faster training and b) better performance on the target data.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53997",
      "pdf_url": "https://openreview.net/pdf?id=mhP6mHgrg1c",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_K_Iyer2",
        "name": "Rishabh K Iyer",
        "name_site": null,
        "openreview_id": "~Rishabh_K_Iyer2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.rishiyer.com",
        "dblp_id": "37/10544.html",
        "google_scholar_url": "l_XxJ1kAAAAJ",
        "orcid": null,
        "linkedin_url": "rishabh-iyer-36893717/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.6393596310755,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 14,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nOdfIbo3A-F",
      "title": "Learning Articulated Rigid Body Dynamics with Lagrangian Graph Neural Network",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Lagrangian  and Hamiltonian neural networks LNN and HNNs, respectively) encode strong inductive biases that allow them to outperform other models of physical systems significantly. However, these models have, thus far, mostly been limited to simple systems such as pendulums and springs or a single rigid body such as a gyroscope or a rigid rotor. Here, we present a Lagrangian graph neural network (LGNN) that can learn the dynamics of articulated rigid bodies by exploiting their topology. We demonstrate the performance of LGNN by learning the dynamics of ropes, chains, and trusses with the bars modeled as rigid bodies. LGNN also exhibits generalizability---LGNN trained on chains with a few segments exhibits generalizability to simulate a chain with large number of links and arbitrary link length. We also show that the LGNN can simulate unseen hybrid systems including bars and chains, on which they have not been trained on. Specifically, we show that the LGNN can be used to model the dynamics of complex real-world structures such as the stability of tensegrity structures. Finally, we discuss the non-diagonal nature of the mass matrix and its ability to generalize in complex systems.",
      "tldr": "We present a Lagrangian graph neural network that can learn the dynamics of rigid body and generalize to arbitrary system sizes",
      "site_url": "https://nips.cc/virtual/2022/poster/53306",
      "pdf_url": "https://openreview.net/pdf?id=nOdfIbo3A-F",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.479019945774904,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 31,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nrOLtfeiIdh",
      "title": "Learning Recourse on Instance Environment to Enhance Prediction Accuracy",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of  alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called {\\em RecourseNet} that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier.   Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent --- for example the best camera angle for object recognition could be a function of the object's shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence.",
      "tldr": "Learning to recourse instances through interventions on the environment space so that the recoursed instances deliver enhanced prediction accuracy by the downstream model. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53322",
      "pdf_url": "https://openreview.net/pdf?id=nrOLtfeiIdh",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sunita_Sarawagi1",
        "name": "Sunita Sarawagi",
        "name_site": null,
        "openreview_id": "~Sunita_Sarawagi1",
        "position": 4,
        "gender": "F",
        "homepage_url": "https://www.cse.iitb.ac.in/~sunita/",
        "dblp_id": "s/SunitaSarawagi",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=Hg4HmTAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rP9xfRSF4F",
      "title": "When to Intervene: Learning Optimal Intervention Policies for Critical Events",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Providing a timely intervention before the onset of a critical event, such as a system failure, is of importance in many industrial settings. Before the onset of the critical event, systems typically exhibit behavioral changes which often manifest as stochastic co-variate observations which may be leveraged to trigger intervention. In this paper, for the first time, we formulate the problem of finding an optimally timed intervention (OTI) policy as minimizing the expected residual time to event, subject to a constraint on the probability of missing the event. Existing machine learning approaches to intervention on critical events focus on predicting event occurrence within a pre-defined window (a classification problem) or predicting time-to-event (a regression problem). Interventions are then triggered by setting model thresholds. These are heuristic-driven, lacking guarantees regarding optimality. To model the evolution of system behavior, we introduce the concept of a hazard rate process. We show that the OTI problem is equivalent to an optimal stopping problem on the associated hazard rate process. This key link has not been explored in literature. Under Markovian assumptions on the hazard rate process, we show that an OTI policy at any time can be analytically determined from the conditional hazard rate function at that time. Further, we show that our theory includes, as a special case, the important class of neural hazard rate processes generated by recurrent neural networks (RNNs). To model such processes, we propose a dynamic deep recurrent survival analysis (DDRSA) architecture, introducing an RNN encoder into the static DRSA setting. Finally, we demonstrate RNN-based OTI policies with experiments and show that they outperform popular intervention methods",
      "tldr": "We describe new theory and algorithms for learning optimally timed interventions on critical events from time-series observations leading up to the event.",
      "site_url": "https://nips.cc/virtual/2022/poster/53440",
      "pdf_url": "https://openreview.net/pdf?id=rP9xfRSF4F",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 2.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": null,
        "countries": [],
        "country_codes": []
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "v7SFDrS44Cf",
      "title": "Neural Estimation of Submodular Functions with Applications to Differentiable Subset Selection",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Submodular functions and variants, through their ability to characterize diversity and coverage, have emerged as a key tool for data selection and summarization.  Many recent approaches to learn submodular functions suffer from limited expressiveness. In this work, we propose FlexSubNet, a family of flexible neural models for both monotone and non-monotone submodular functions. To fit a latent submodular function from (set, value) observations, our method applies a concave function on modular functions in a recursive manner. We do not draw the concave function from a restricted family, but rather learn from data using a highly expressive neural network that implements a differentiable quadrature procedure. Such an expressive neural model for concave functions may be of independent interest.  Next, we extend this setup to provide a novel characterization of monotone $\\alpha$-submodular functions, a recently introduced notion of approximate submodular functions.  We then use this characterization to design a novel neural model for such functions. Finally, we consider learning submodular set functions under distant supervision in the form of  (perimeter, high-value-subset) pairs.  This yields a novel subset selection method based on an order-invariant, yet greedy sampler built around the above neural set functions. Our experiments on synthetic and real data show that FlexSubNet outperforms several baselines.\n",
      "tldr": "We design novel neural models for submodular functions and an efficient differentiable methods towards differentiable data subset selection",
      "site_url": "https://nips.cc/virtual/2022/poster/53269",
      "pdf_url": "https://openreview.net/pdf?id=v7SFDrS44Cf",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumen_Chakrabarti1",
        "name": "Soumen Chakrabarti",
        "name_site": null,
        "openreview_id": "~Soumen_Chakrabarti1",
        "position": 2,
        "gender": "Not Specified",
        "homepage_url": "https://www.cse.iitb.ac.in/~soumen/",
        "dblp_id": "c/SChakrabarti",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=LfF2zfQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xbgtFOO9J5D",
      "title": "Fair Rank Aggregation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Ranking algorithms find extensive usage in diverse areas such as web search, employment, college\n    admission, voting, etc.  The related rank aggregation problem deals with combining multiple\n    rankings into a single aggregate ranking.  However, algorithms for both these problems might be\n    biased against some individuals or groups due to implicit prejudice or marginalization in the\n    historical data.  We study ranking and rank aggregation problems from a fairness or diversity\n    perspective, where the candidates (to be ranked) may belong to different groups and each group\n    should have a fair representation in the final ranking. We allow the designer to set the\n    parameters that define fair representation. These parameters specify the allowed range of the\n    number of candidates from a particular group in the top-$k$ positions of the ranking.  Given any\n    ranking, we provide a fast and exact algorithm for finding the closest fair ranking for the\n    Kendall tau metric under {\\em strong fairness}, i.e., when the final ranking is fair for all\n    values of $k$. We also provide an exact algorithm for finding the closest fair ranking for the\n    Ulam metric under strong fairness when there are only $O(1)$ number of groups.  Our\n    algorithms are simple, fast, and might be extendable to other relevant metrics. We also give a\n    novel  meta-algorithm for the general rank aggregation problem under the fairness framework.\n    Surprisingly, this meta-algorithm works for any generalized mean objective (including center and\n    median problems) and any fairness criteria. As a byproduct, we obtain 3-approximation algorithms\n    for both center and median problems, under both Kendall tau and Ulam metrics. Furthermore, using\n    sophisticated techniques we obtain a $(3-\\varepsilon)$-approximation algorithm, for a constant\n    $\\varepsilon>0$,  for the Ulam metric under strong fairness.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53193",
      "pdf_url": "https://openreview.net/pdf?id=xbgtFOO9J5D",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Subramanian1",
        "name": "Aditya Subramanian",
        "name_site": null,
        "openreview_id": "~Aditya_Subramanian1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "217/3188.html",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yRhbHp_Vh8e",
      "title": "Grounded Video Situation Recognition",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Dense video understanding requires answering several questions such as who is doing what to whom, with what, how, why, and where. Recently, Video Situation Recognition (VidSitu) is framed as a task for structured prediction of multiple events, their relationships, and actions and various verb-role pairs attached to descriptive entities. This task poses several challenges in identifying, disambiguating, and co-referencing entities across multiple verb-role pairs, but also faces some challenges of evaluation. In this work, we propose the addition of spatio-temporal grounding as an essential component of the structured prediction task in a weakly supervised setting, and present a novel three stage Transformer model, VideoWhisperer, that is empowered to make joint predictions. In stage one, we learn contextualised embeddings for video features in parallel with key objects that appear in the video clips to enable fine-grained spatio-temporal reasoning. The second stage sees verb-role queries attend and pool information from object embeddings, localising answers to questions posed about the action. The final stage generates these answers as captions to describe each verb-role pair present in the video. Our model operates on a group of events (clips) simultaneously and predicts verbs, verb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on a grounding-augmented version of the VidSitu dataset, we observe a large improvement in entity captioning accuracy, as well as the ability to localize verb-roles without grounding annotations at training time.",
      "tldr": "We present a new task Grounded Video Situation Recognition(GVSR). In addition to predicting the verbs, and semantic roles in the form of captions, we also ground them in the spatio-temporal domain in weakly-supervised setup in an end-to-end fashion. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53297",
      "pdf_url": "https://openreview.net/pdf?id=yRhbHp_Vh8e",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Makarand_Tapaswi1",
        "name": "Makarand Tapaswi",
        "name_site": null,
        "openreview_id": "~Makarand_Tapaswi1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://makarandtapaswi.github.io/",
        "dblp_id": "69/1484",
        "google_scholar_url": "rJotb-YAAAAJ",
        "orcid": "0000-0001-8800-9015",
        "linkedin_url": "makarand-tapaswi/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Wadhwani Institute for Artificial Intelligence (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Mitesh_M_Khapra1",
        "name": "Mitesh M Khapra",
        "name_site": null,
        "openreview_id": "~Mitesh_M_Khapra1",
        "position": 6,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~miteshk",
        "dblp_id": "90/7967",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=DV8z8DYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uP9RiC4uVcR",
      "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind — the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of moral exception question answering (MoralExceptQA) of cases that involve potentially permissible moral exceptions – inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MoralCoT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MoralCoT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using MoralExceptQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT.",
      "tldr": "We present a novel challenge set that highlights the flexibility of the human moral mind, analyze large language models' performance on it, and proposed a Moral Chain-of-Thought prompting strategy.",
      "site_url": "https://nips.cc/virtual/2022/poster/55320",
      "pdf_url": "https://openreview.net/pdf?id=uP9RiC4uVcR",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sydney_Levine1",
        "name": "Sydney Levine",
        "name_site": null,
        "openreview_id": "~Sydney_Levine1",
        "position": 2,
        "gender": "F",
        "homepage_url": "http://sites.google.com/site/sydneymlevine",
        "dblp_id": "175/9604",
        "google_scholar_url": "Yt2H6lwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.875,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 103,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "HCnb1TByvx7",
      "title": "Multilingual Abusive Comment Detection at Scale for Indic Languages",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Social media platforms were conceived to act as online `town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into `mosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.\nTo facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. ",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55666",
      "pdf_url": "https://openreview.net/pdf?id=HCnb1TByvx7",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sumegh_Roychowdhury1",
        "name": "Sumegh Roychowdhury",
        "name_site": null,
        "openreview_id": "~Sumegh_Roychowdhury1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "246/0200",
        "google_scholar_url": "8T4DcYIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8571428571428572,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 4.2,
        "confidence_std": 0.9797958971132712,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 27,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Guttu_Sai_Abhishek1",
        "name": "Guttu Sai Abhishek",
        "name_site": null,
        "openreview_id": "~Guttu_Sai_Abhishek1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "https://in.linkedin.com/in/guttu-sai-abhishek",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8333333333333334,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zXE8iFOZKw",
      "title": "When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Learning effective reinforcement learning (RL) policies to solve real-world complex tasks can be quite challenging without a high-fidelity simulation environment. In most cases, we are only given imperfect simulators with simplified dynamics, which inevitably lead to severe sim-to-real gaps in RL policy learning. The recently emerged field of offline RL provides another possibility to learn policies directly from pre-collected historical data. However, to achieve reasonable performance, existing offline RL algorithms need impractically large offline data with sufficient state-action space coverage for training. This brings up a new question: is it possible to combine learning from limited real data in offline RL and unrestricted exploration through imperfect simulators in online RL to address the drawbacks of both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O) framework to provide an affirmative answer to this question. H2O introduces a dynamics-aware policy evaluation scheme, which adaptively penalizes the Q function learning on simulated state-action pairs with large dynamics gaps, while also simultaneously allowing learning from a fixed real-world dataset. Through extensive simulation and real-world tasks, as well as theoretical analysis, we demonstrate the superior performance of H2O against other cross-domain online and offline RL algorithms. H2O provides a brand new hybrid offline-and-online RL paradigm, which can potentially shed light on future RL algorithm design for solving practical real-world tasks.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54412",
      "pdf_url": "https://openreview.net/pdf?id=zXE8iFOZKw",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shubham_Sharma3",
        "name": "Shubham Sharma",
        "name_site": null,
        "openreview_id": "~Shubham_Sharma3",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "shubh-am-sharma?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B9buzyRZ8SEi8CFfYrtuFjA%3D%3D",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8333333333333334,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 54,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3LBxVcnsEkV",
      "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline.",
      "tldr": "Learning graph and subgraph edit distance using graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/54507",
      "pdf_url": "https://openreview.net/pdf?id=3LBxVcnsEkV",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddharth_Grover1",
        "name": "Siddharth Grover",
        "name_site": null,
        "openreview_id": "~Siddharth_Grover1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "siddharth-grover-173853184",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suvaansh_Bhambri3",
        "name": "Suvaansh Bhambri",
        "name_site": null,
        "openreview_id": "~Akshay_Ravindra_Kulkarni1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://akshayk07.weebly.com/",
        "dblp_id": "324/0660",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=VGztDcYAAAAJ",
        "orcid": "0000-0003-3379-2238",
        "linkedin_url": "akshaykulkarni07/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PuagBLcAf8n",
      "title": "Off-Policy Evaluation for Action-Dependent Non-stationary Environments",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Methods for sequential decision-making are often built upon a foundational assumption that the underlying decision process is stationary. This limits the application of such methods because real-world problems are often subject to changes due to external factors (\\textit{passive} non-stationarity), changes induced by interactions with the system itself (\\textit{active} non-stationarity), or both (\\textit{hybrid} non-stationarity). In this work, we take the first steps towards the fundamental challenge of on-policy and off-policy evaluation amidst structured changes due to active, passive, or hybrid non-stationarity. Towards this goal, we make a \\textit{higher-order stationarity} assumption such that non-stationarity results in changes over time, but the way changes happen is fixed. We propose, OPEN, an algorithm that uses a double application of counterfactual reasoning and a novel importance-weighted instrument-variable regression to obtain both a lower bias and a lower variance estimate of the structure in the changes of a policy's past performances. Finally, we show promising results on how OPEN can be used to predict future performances for several domains inspired by real-world applications that exhibit non-stationarity.",
      "tldr": "How to do off-policy evaluation from data collected amidst non-stationarity that depends on both exogenous factors and past actions/interactions?",
      "site_url": "https://nips.cc/virtual/2022/poster/54093",
      "pdf_url": "https://openreview.net/pdf?id=PuagBLcAf8n",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shiv_Shankar2",
        "name": "Shiv Shankar",
        "name_site": null,
        "openreview_id": "~Shiv_Shankar2",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "203/9123",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Gunjan_Kumar2",
        "name": "Gunjan Kumar",
        "name_site": null,
        "openreview_id": "~Gunjan_Kumar2",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "gunjan-kumar-424b021a6",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Manideep_Ladi1",
        "name": "Manideep Ladi",
        "name_site": null,
        "openreview_id": "~Manideep_Ladi1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "ladi-manideep/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wO53HILzu65",
      "title": "On the Generalizability and Predictability of Recommender Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "While other areas of machine learning have seen more and more automation, designing a high-performing recommender system still requires a high level of human effort. Furthermore, recent work has shown that modern recommender system algorithms do not always improve over well-tuned baselines. A natural follow-up question is, \"how do we choose the right algorithm for a new dataset and performance metric?\" In this work, we start by giving the first large-scale study of recommender system approaches by comparing 24 algorithms and 100 sets of hyperparameters across 85 datasets and 315 metrics. We find that the best algorithms and hyperparameters are highly dependent on the dataset and performance metric. However, there is also a strong correlation between the performance of each algorithm and various meta-features of the datasets. Motivated by these findings, we create RecZilla, a meta-learning approach to recommender systems that uses a model to predict the best algorithm and hyperparameters for new, unseen datasets. By using far more meta-training data than prior work, RecZilla is able to substantially reduce the level of human involvement when faced with a new recommender system application. We not only release our code and pretrained RecZilla models, but also all of our raw experimental results, so that practitioners can train a RecZilla model for their desired performance metric: https://github.com/naszilla/reczilla.",
      "tldr": "We conduct a large-scale study of recommender system algorithms, which motivates the design of RecZilla: an algorithm selection approach based on meta-learning.",
      "site_url": "https://nips.cc/virtual/2022/poster/55096",
      "pdf_url": "https://openreview.net/pdf?id=wO53HILzu65",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sujay_Khandagale1",
        "name": "Sujay Khandagale",
        "name_site": null,
        "openreview_id": "~Sujay_Khandagale1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://suj97.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.fi/citations?user=7fwPm3wAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.75,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "59pMU2xFxG",
      "title": "What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "A multitude of explainability methods has been described to try to help users better understand how modern AI systems make decisions. However, most performance metrics developed to evaluate these methods have remained largely theoretical -- without much consideration for the human end-user. In particular, it is not yet clear (1) how useful current explainability methods are in real-world scenarios; and (2) whether current performance metrics accurately reflect the usefulness of explanation methods for the end user. To fill this gap, we conducted psychophysics experiments at scale ($n=1,150$) to evaluate the usefulness of representative attribution methods in three real-world scenarios. Our results demonstrate that the degree to which individual attribution methods help human participants better understand an AI system varies widely across these scenarios. This suggests the need to move beyond quantitative improvements of current attribution methods, towards the development of complementary approaches that provide qualitatively different sources of information to human end-users.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55282",
      "pdf_url": "https://openreview.net/pdf?id=59pMU2xFxG",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 119,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9DYKrsFSU2",
      "title": "Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converges to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\\% across imbalanced datasets. The code is available at https://github.com/val-iisc/Saddle-LongTail.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53304",
      "pdf_url": "https://openreview.net/pdf?id=9DYKrsFSU2",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumukh_K_Aithal1",
        "name": "Sumukh K Aithal",
        "name_site": null,
        "openreview_id": "~Mayank_Mishra2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://mmayank74567.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DVfZKXSFW5m",
      "title": "Diversity vs. Recognizability: Human-like generalization in one-shot generative models",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Robust generalization to new concepts has long remained a distinctive feature of human intelligence. However, recent progress in deep generative models has now led to neural architectures capable of synthesizing novel instances of unknown visual concepts from a single training example. Yet, a more precise comparison between these models and humans is not possible because existing performance metrics for generative models (i.e., FID, IS, likelihood) are not appropriate for the one-shot generation scenario. Here, we propose a new framework to evaluate one-shot generative models along two axes: sample recognizability vs. diversity  (i.e., intra-class variability). Using this framework, we perform a systematic evaluation of representative one-shot generative models on the Omniglot handwritten dataset. We first show that GAN-like and VAE-like models fall on opposite ends of the diversity-recognizability space. Extensive analyses of the effect of key model parameters further revealed that spatial attention and context integration have a linear contribution to the diversity-recognizability trade-off. In contrast, disentanglement transports the model along a parabolic curve that could be used to maximize recognizability. Using the diversity-recognizability framework, we were able to identify models and parameters that closely approximate human data.",
      "tldr": "We propose and test a new framework to evaluate one-shot image generation models",
      "site_url": "https://nips.cc/virtual/2022/poster/53225",
      "pdf_url": "https://openreview.net/pdf?id=DVfZKXSFW5m",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Lakshya_Singhal1",
        "name": "Lakshya Singhal",
        "name_site": null,
        "openreview_id": "~Lakshya_Singhal1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://github.com/asdas1505",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Manipal Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 9,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EqZuN4V_FLF",
      "title": "A Solver-free Framework for Scalable Learning in Neural ILP Architectures",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There is a recent focus on designing architectures that have an Integer Linear Programming (ILP) layer within a neural model (referred to as \\emph{Neural ILP} in this paper). Neural ILP architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning (ILP). A recent SOTA approach for end-to-end training of Neural ILP explicitly defines gradients through the ILP black box [Paulus et al. [2021]] – this trains extremely slowly, owing to a call to the underlying ILP solver for every training data point in a minibatch. In response, we present an alternative training strategy that is \\emph{solver-free}, i.e., does not call the ILP solver at all at training time. Neural ILP has a set of trainable hyperplanes (for cost and constraints in ILP), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation.  While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an ILP, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual Sudoku, to which the other Neural ILP solver is not able to scale.",
      "tldr": "For learning constraints in a neural ILP architecture, we propose a scalable solver-free framework that doesn't require calling the solver to compute gradients.",
      "site_url": "https://nips.cc/virtual/2022/poster/53251",
      "pdf_url": "https://openreview.net/pdf?id=EqZuN4V_FLF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_Ranjan1",
        "name": "Rishabh Ranjan",
        "name_site": "Rishabh Ranjan, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy, Yogish Sabharwal, Sayan Ranu",
        "openreview_id": "~Rishabh_Ranjan1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://rishabh-ranjan.github.io",
        "dblp_id": null,
        "google_scholar_url": "NNzQUrcAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.0,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aakriti_Lnu1",
        "name": "Aakriti Lnu",
        "name_site": null,
        "openreview_id": "~Aakriti_Lnu1",
        "position": 3,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "aakriti-k-aa53941b1/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lRUCfzs5Hzg",
      "title": "How Transferable are Video Representations Based on Synthetic Data?",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Action recognition has improved dramatically with massive-scale video datasets. Yet, these datasets are accompanied with issues related to curation cost, privacy, ethics, bias, and copyright. Compared to that, only minor efforts have been devoted toward exploring the potential of synthetic video data. In this work, as a stepping stone towards addressing these shortcomings, we study the transferability of video representations learned solely from synthetically-generated video clips, instead of real data. We propose SynAPT, a novel benchmark for action recognition based on a combination of existing synthetic datasets, in which a model is pre-trained on synthetic videos rendered by various graphics simulators, and then transferred to a set of downstream action recognition datasets, containing different categories than the synthetic data. We provide an extensive baseline analysis on SynAPT revealing that the simulation-to-real gap is minor for datasets with low object and scene bias, where models pre-trained with synthetic data even outperform their real data counterparts. We posit that the gap between real and synthetic action representations can be attributed to contextual bias and static objects related to the action, instead of the temporal dynamics of the action itself. The SynAPT benchmark is available at https://github.com/mintjohnkim/SynAPT.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55635",
      "pdf_url": "https://openreview.net/pdf?id=lRUCfzs5Hzg",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Rameswar_Panda1",
        "name": "Rameswar Panda",
        "name_site": null,
        "openreview_id": "~Rameswar_Panda1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://rpand002.github.io/",
        "dblp_id": "126/0986",
        "google_scholar_url": "_ySuu6gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 6.6,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.8,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 36,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nrOLtfeiIdh",
      "title": "Learning Recourse on Instance Environment to Enhance Prediction Accuracy",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of  alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called {\\em RecourseNet} that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier.   Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent --- for example the best camera angle for object recognition could be a function of the object's shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence.",
      "tldr": "Learning to recourse instances through interventions on the environment space so that the recoursed instances deliver enhanced prediction accuracy by the downstream model. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53322",
      "pdf_url": "https://openreview.net/pdf?id=nrOLtfeiIdh",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Guntakanti_Sai_Koushik1",
        "name": "Guntakanti Sai Koushik",
        "name_site": null,
        "openreview_id": "~Guntakanti_Sai_Koushik1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "guntakanti-sai-koushik-7b80781a0/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xbgtFOO9J5D",
      "title": "Fair Rank Aggregation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Ranking algorithms find extensive usage in diverse areas such as web search, employment, college\n    admission, voting, etc.  The related rank aggregation problem deals with combining multiple\n    rankings into a single aggregate ranking.  However, algorithms for both these problems might be\n    biased against some individuals or groups due to implicit prejudice or marginalization in the\n    historical data.  We study ranking and rank aggregation problems from a fairness or diversity\n    perspective, where the candidates (to be ranked) may belong to different groups and each group\n    should have a fair representation in the final ranking. We allow the designer to set the\n    parameters that define fair representation. These parameters specify the allowed range of the\n    number of candidates from a particular group in the top-$k$ positions of the ranking.  Given any\n    ranking, we provide a fast and exact algorithm for finding the closest fair ranking for the\n    Kendall tau metric under {\\em strong fairness}, i.e., when the final ranking is fair for all\n    values of $k$. We also provide an exact algorithm for finding the closest fair ranking for the\n    Ulam metric under strong fairness when there are only $O(1)$ number of groups.  Our\n    algorithms are simple, fast, and might be extendable to other relevant metrics. We also give a\n    novel  meta-algorithm for the general rank aggregation problem under the fairness framework.\n    Surprisingly, this meta-algorithm works for any generalized mean objective (including center and\n    median problems) and any fairness criteria. As a byproduct, we obtain 3-approximation algorithms\n    for both center and median problems, under both Kendall tau and Ulam metrics. Furthermore, using\n    sophisticated techniques we obtain a $(3-\\varepsilon)$-approximation algorithm, for a constant\n    $\\varepsilon>0$,  for the Ulam metric under strong fairness.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53193",
      "pdf_url": "https://openreview.net/pdf?id=xbgtFOO9J5D",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Syamantak_Das1",
        "name": "Syamantak Das",
        "name_site": null,
        "openreview_id": "~Syamantak_Das1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "135/6297.html",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xbhsFMxORxV",
      "title": "Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social Text Classification",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Social media has become the fulcrum of all forms of communication. Classifying social texts such as fake news, rumour, sarcasm, etc. has gained significant attention. The surface-level signals expressed by a social-text itself may not be adequate for such tasks; therefore, recent methods attempted to incorporate other intrinsic signals such as user behavior and the underlying graph structure. Oftentimes, the public wisdom expressed through the comments/replies to a social-text acts as a surrogate of crowd-sourced view and may provide us with complementary signals. State-of-the-art methods on social-text classification tend to ignore such a rich hierarchical signal. Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention network. Hyphen is a fusion of hyperbolic graph representation learning with a novel Fourier co-attention mechanism in an attempt to generalise the social-text classification tasks by incorporating public discourse. We parse public discourse as an Abstract Meaning Representation (AMR) graph and use the powerful hyperbolic geometric representation to model graphs with hierarchical structure. Finally, we equip it with a novel Fourier co-attention mechanism to capture the correlation between the source post and public discourse. Extensive experiments on four different social-text classification tasks, namely detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen generalises well, and achieves state-of-the-art results on ten benchmark datasets. We also employ a sentence-level fact-checked and annotated dataset to evaluate how Hyphen is capable of producing explanations as analogous evidence to the final prediction.",
      "tldr": "Hyperbolic spectral co-attention for public discourse-aware social media text classification.",
      "site_url": "https://nips.cc/virtual/2022/poster/53312",
      "pdf_url": "https://openreview.net/pdf?id=xbhsFMxORxV",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~S_M_Phaneendra_Angara1",
        "name": "S M Phaneendra Angara",
        "name_site": null,
        "openreview_id": "~S_M_Phaneendra_Angara1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "s-m-phaneendra-angara-9b94266/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6666666666666667,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uP9RiC4uVcR",
      "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind — the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of moral exception question answering (MoralExceptQA) of cases that involve potentially permissible moral exceptions – inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MoralCoT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MoralCoT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using MoralExceptQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT.",
      "tldr": "We present a novel challenge set that highlights the flexibility of the human moral mind, analyze large language models' performance on it, and proposed a Moral Chain-of-Thought prompting strategy.",
      "site_url": "https://nips.cc/virtual/2022/poster/55320",
      "pdf_url": "https://openreview.net/pdf?id=uP9RiC4uVcR",
      "github_url": "",
      "total_authors": 9,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ojasv_Kamal1",
        "name": "Ojasv Kamal",
        "name_site": null,
        "openreview_id": "~Ojasv_Kamal1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "34PgtWEAAAAJ",
        "orcid": null,
        "linkedin_url": "ojasv-kamal-996397182/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Michigan (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.625,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 103,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3LBxVcnsEkV",
      "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline.",
      "tldr": "Learning graph and subgraph edit distance using graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/54507",
      "pdf_url": "https://openreview.net/pdf?id=3LBxVcnsEkV",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourav_Medya1",
        "name": "Sourav Medya",
        "name_site": null,
        "openreview_id": "~Sourav_Medya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://souravmedya.github.io/",
        "dblp_id": "178/3021",
        "google_scholar_url": "RCFhOM4AAAAJ",
        "orcid": "0000-0003-0996-2807",
        "linkedin_url": "sourav-medya-35987a49/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Northwestern University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Akshay_Ravindra_Kulkarni1",
        "name": "Akshay Ravindra Kulkarni",
        "name_site": null,
        "openreview_id": "~Hiran_Sarkar1",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?view_op=list_works",
        "orcid": null,
        "linkedin_url": "aroundstar/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Netaji Subhash Engineering College (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Suresh_Bishnoi1",
        "name": "Suresh Bishnoi",
        "name_site": null,
        "openreview_id": "~Suresh_Bishnoi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~srz208500/",
        "dblp_id": "329/6194",
        "google_scholar_url": "Wy6q2QwAAAAJ",
        "orcid": null,
        "linkedin_url": "sureshb1999/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sumit_Negi1",
        "name": "Sumit Negi",
        "name_site": null,
        "openreview_id": "~Sumit_Negi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.linkedin.com/in/sumit-negi-748958202/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.6,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7pNV4PCjbQy",
      "title": "Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "We consider the problem of controlling an unknown stochastic linear system with quadratic costs -- called the adaptive LQ control problem. We re-examine an approach called ``Reward-Biased Maximum Likelihood Estimate'' (RBMLE) that was proposed more than forty years ago, and which predates the ``Upper Confidence Bound'' (UCB) method, as well as the definition of ``regret'' for bandit problems. It simply added a term favoring parameters with larger rewards to the criterion for parameter estimation.  We show how the RBMLE and UCB methods can be reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that combines the penalty of the RBMLE method with the constraints of the UCB method, uniting the two approaches to optimism in the face of uncertainty. We establish that theoretically, this method retains ${\\mathcal{O}}(\\sqrt{T})$ regret, the best known so far. We further compare the empirical performance of the proposed Augmented RBMLE-UCB and the standard RBMLE (without the augmentation) with UCB, Thompson Sampling, Input Perturbation, Randomized Certainty Equivalence and StabL on many real-world examples including flight control of Boeing 747 and Unmanned Aerial Vehicle. We perform extensive simulation studies showing that the Augmented RBMLE consistently outperforms UCB, Thompson Sampling and StabL by a huge margin, while it is marginally better than Input Perturbation and moderately better than Randomized Certainty Equivalence.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53342",
      "pdf_url": "https://openreview.net/pdf?id=7pNV4PCjbQy",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rahul_Singh5",
        "name": "Rahul Singh",
        "name_site": null,
        "openreview_id": "~Rahul_Singh5",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/rsingh12/home",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 1.8330302779823362,
        "confidence_mean": 3.2,
        "confidence_std": 1.32664991614216,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8li9SYYY3eQ",
      "title": "Language Conditioned Spatial Relation Reasoning for 3D Object Grounding",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Localizing objects in 3D scenes based on natural language requires understanding and reasoning about spatial relations. In particular, it is often crucial to distinguish similar objects referred by the text, such as \"the left most chair\" and \"a chair next to the window\". In this work we propose a language-conditioned transformer model for grounding 3D objects and their spatial relations. To this end, we design a spatial self-attention layer that accounts for relative distances and orientations between objects in input 3D point clouds. Training such a layer with visual and language inputs enables to disambiguate spatial relations and to localize objects referred by the text. To facilitate the cross-modal learning of relations, we further propose a teacher-student approach where the teacher model is first trained using ground-truth object labels, and then helps to train a student model using point cloud inputs. We perform ablation studies showing advantages of our approach. We also demonstrate our model to significantly outperform the state of the art on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54818",
      "pdf_url": "https://openreview.net/pdf?id=8li9SYYY3eQ",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Makarand_Tapaswi1",
        "name": "Makarand Tapaswi",
        "name_site": null,
        "openreview_id": "~Makarand_Tapaswi1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://makarandtapaswi.github.io/",
        "dblp_id": "69/1484",
        "google_scholar_url": "rJotb-YAAAAJ",
        "orcid": "0000-0001-8800-9015",
        "linkedin_url": "makarand-tapaswi/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Wadhwani Institute for Artificial Intelligence (France)",
        "countries": [
          "France"
        ],
        "country_codes": [
          "FR"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 88,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "COAcbu3_k4U",
      "title": "Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The graph retrieval problem is to search in a large corpus of graphs for ones that are most similar to a query graph.  A common consideration for scoring similarity is the maximum common subgraph (MCS) between the query and corpus graphs, usually counting the number of common edges (i.e., MCES).  In some applications, it is also desirable that the common subgraph be connected, i.e., the maximum common connected subgraph (MCCS). Finding exact MCES and MCCS is intractable, but may be unnecessary if ranking corpus graphs by relevance is the goal.  We design fast and trainable neural functions that approximate MCES and MCCS well.  Late interaction methods compute dense representations for the query and corpus graph separately, and compare these representations using simple similarity functions at the last stage, leading to highly scalable systems.  Early interaction methods combine information from both graphs right from the input stages, are usually considerably more accurate, but slower.  We propose both late and early interaction neural MCES and MCCS formulations.  They are both based on a continuous relaxation of a node alignment matrix between query and corpus nodes.  For MCCS, we propose a novel differentiable network for estimating the size of the largest connected common subgraph.  Extensive experiments with seven data sets show that our proposals are superior among late interaction models in terms of both accuracy and speed.  Our early interaction models provide accuracy competitive with the state of the art, at substantially greater speeds.",
      "tldr": "It learns late and early interaction models for maximum common subgraph based graph retrieval. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53399",
      "pdf_url": "https://openreview.net/pdf?id=COAcbu3_k4U",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumen_Chakrabarti1",
        "name": "Soumen Chakrabarti",
        "name_site": null,
        "openreview_id": "~Soumen_Chakrabarti1",
        "position": 2,
        "gender": "Not Specified",
        "homepage_url": "https://www.cse.iitb.ac.in/~soumen/",
        "dblp_id": "c/SChakrabarti",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=LfF2zfQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ODkBI1d3phW",
      "title": "Efficient and Effective Augmentation Strategy for Adversarial Training",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the \\emph{joint} learning of the \\emph{diverse augmentations}, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. \nThe code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT",
      "tldr": "We propose an effective augmentation strategy for Adversarial Training that can be integrated with several Adversarial Training algorithms and data augmentations.",
      "site_url": "https://nips.cc/virtual/2022/poster/54553",
      "pdf_url": "https://openreview.net/pdf?id=ODkBI1d3phW",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samyak_Jain1",
        "name": "Samyak Jain",
        "name_site": null,
        "openreview_id": "~Samyak_Jain1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://samyakjain0112.github.io/",
        "dblp_id": "249/4464.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-3785-4782",
        "linkedin_url": "samyak-jain-276738178/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.5,
        "confidence_std": 1.118033988749895,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 62,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Vt3_mJNrjt",
      "title": "Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "This paper presents a new efficient black-box attribution method built on Hilbert-Schmidt Independence Criterion (HSIC). Based on Reproducing Kernel Hilbert Spaces (RKHS), HSIC measures the dependence between regions of an input image and the output of a model using the kernel embedding of their distributions. It thus provides explanations enriched by RKHS representation capabilities. HSIC can be estimated very efficiently, significantly reducing the computational cost compared to other black-box attribution methods.\nOur experiments show that HSIC is up to 8 times faster than the previous best black-box attribution methods while being as faithful.\nIndeed, we improve or match the state-of-the-art of both black-box and white-box attribution methods for several fidelity metrics on Imagenet with various recent model architectures.\nImportantly, we show that these advances can be transposed to efficiently and faithfully explain object detection models such as YOLOv4. \nFinally, we extend the traditional attribution methods by proposing a new kernel enabling an ANOVA-like orthogonal decomposition of importance scores based on HSIC, allowing us to evaluate not only the importance of each image patch but also the importance of their pairwise interactions. Our implementation is available at \\url{https://github.com/paulnovello/HSIC-Attribution-Method}.",
      "tldr": "We explain black-box model by assessing the dependence between their output and local regions of the input using kernel dependence measure",
      "site_url": "https://nips.cc/virtual/2022/poster/55226",
      "pdf_url": "https://openreview.net/pdf?id=Vt3_mJNrjt",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 2.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 31,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "_D4cE66L9x3",
      "title": "Byzantine Spectral Ranking",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "We study the problem of rank aggregation where the goal is to obtain a global ranking by aggregating pair-wise comparisons of voters over a set of items. We consider an adversarial setting where the voters are partitioned into two sets. The first set votes in a stochastic manner according to the popular score-based Bradley-Terry-Luce (BTL) model for pairwise comparisons. The second set comprises malicious Byzantine voters trying to deteriorate the ranking. We consider a strongly-adversarial scenario where the Byzantine voters know the BTL scores, the votes of the good voters, the algorithm, and can collude with each other. We first show that the popular spectral ranking based Rank-Centrality algorithm, though optimal for the BTL model, does not perform well even when a small constant fraction of the voters are Byzantine.\n\nWe introduce the Byzantine Spectral Ranking Algorithm (and a faster variant of it), which produces a reliable ranking when the number of good voters exceeds the number of Byzantine voters. We show that no algorithm can produce a satisfactory ranking with probability > 1/2 for all BTL weights when there are more Byzantine voters than good voters, showing that our algorithm works for all possible population fractions. We support our theoretical results with experimental results on synthetic and real datasets to demonstrate the failure of the Rank-Centrality algorithm under several adversarial scenarios and how the proposed Byzantine Spectral Ranking algorithm is robust in obtaining good rankings.",
      "tldr": "A novel theoretically sound pairwise ranking algorithm that is robust to Byzantine voters.",
      "site_url": "https://nips.cc/virtual/2022/poster/53351",
      "pdf_url": "https://openreview.net/pdf?id=_D4cE66L9x3",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arun_Rajkumar4",
        "name": "Arun Rajkumar",
        "name_site": null,
        "openreview_id": "~Arun_Rajkumar4",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "32/11350",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ganesh_Ramakrishnan1",
        "name": "Ganesh Ramakrishnan",
        "name_site": null,
        "openreview_id": "~Ganesh_Ramakrishnan1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~ganesh/",
        "dblp_id": "r/GaneshRamakrishnan",
        "google_scholar_url": "https://scholar.google.com/scholar?hl=hi",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nOdfIbo3A-F",
      "title": "Learning Articulated Rigid Body Dynamics with Lagrangian Graph Neural Network",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Lagrangian  and Hamiltonian neural networks LNN and HNNs, respectively) encode strong inductive biases that allow them to outperform other models of physical systems significantly. However, these models have, thus far, mostly been limited to simple systems such as pendulums and springs or a single rigid body such as a gyroscope or a rigid rotor. Here, we present a Lagrangian graph neural network (LGNN) that can learn the dynamics of articulated rigid bodies by exploiting their topology. We demonstrate the performance of LGNN by learning the dynamics of ropes, chains, and trusses with the bars modeled as rigid bodies. LGNN also exhibits generalizability---LGNN trained on chains with a few segments exhibits generalizability to simulate a chain with large number of links and arbitrary link length. We also show that the LGNN can simulate unseen hybrid systems including bars and chains, on which they have not been trained on. Specifically, we show that the LGNN can be used to model the dynamics of complex real-world structures such as the stability of tensegrity structures. Finally, we discuss the non-diagonal nature of the mass matrix and its ability to generalize in complex systems.",
      "tldr": "We present a Lagrangian graph neural network that can learn the dynamics of rigid body and generalize to arbitrary system sizes",
      "site_url": "https://nips.cc/virtual/2022/poster/53306",
      "pdf_url": "https://openreview.net/pdf?id=nOdfIbo3A-F",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sayan_Ranu2",
        "name": "Sayan Ranu",
        "name_site": null,
        "openreview_id": "~Sayan_Ranu2",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.cse.iitd.ac.in/~sayan/index.html",
        "dblp_id": "38/768",
        "google_scholar_url": "K4w5qYUAAAAJ",
        "orcid": "0000-0003-4147-9372",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 1.479019945774904,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 31,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "srHMs3mPD5y",
      "title": "FETA: Towards Specializing Foundational Models for Expert Task Applications",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "    Foundational Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, the parameter capacity of FMs is still limited, leading to poor out-of-the-box performance of FMs on many expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for many practical expert tasks currently being `overlooked' by standard benchmarks focusing on common objects.",
      "tldr": "Self-supervised adaptation of foundational models to application domains in the long tail of their pretraining data distribution",
      "site_url": "https://nips.cc/virtual/2022/poster/55625",
      "pdf_url": "https://openreview.net/pdf?id=srHMs3mPD5y",
      "github_url": "",
      "total_authors": 13,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Rameswar_Panda1",
        "name": "Rameswar Panda",
        "name_site": null,
        "openreview_id": "~Rameswar_Panda1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://rpand002.github.io/",
        "dblp_id": "126/0986",
        "google_scholar_url": "_ySuu6gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 3.8,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "wO53HILzu65",
      "title": "On the Generalizability and Predictability of Recommender Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "While other areas of machine learning have seen more and more automation, designing a high-performing recommender system still requires a high level of human effort. Furthermore, recent work has shown that modern recommender system algorithms do not always improve over well-tuned baselines. A natural follow-up question is, \"how do we choose the right algorithm for a new dataset and performance metric?\" In this work, we start by giving the first large-scale study of recommender system approaches by comparing 24 algorithms and 100 sets of hyperparameters across 85 datasets and 315 metrics. We find that the best algorithms and hyperparameters are highly dependent on the dataset and performance metric. However, there is also a strong correlation between the performance of each algorithm and various meta-features of the datasets. Motivated by these findings, we create RecZilla, a meta-learning approach to recommender systems that uses a model to predict the best algorithm and hyperparameters for new, unseen datasets. By using far more meta-training data than prior work, RecZilla is able to substantially reduce the level of human involvement when faced with a new recommender system application. We not only release our code and pretrained RecZilla models, but also all of our raw experimental results, so that practitioners can train a RecZilla model for their desired performance metric: https://github.com/naszilla/reczilla.",
      "tldr": "We conduct a large-scale study of recommender system algorithms, which motivates the design of RecZilla: an algorithm selection approach based on meta-learning.",
      "site_url": "https://nips.cc/virtual/2022/poster/55096",
      "pdf_url": "https://openreview.net/pdf?id=wO53HILzu65",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Jonathan_Valverde1",
        "name": "Jonathan Valverde",
        "name_site": null,
        "openreview_id": "~Jonathan_Valverde1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://jonathan-valverde-l.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 11,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yRhbHp_Vh8e",
      "title": "Grounded Video Situation Recognition",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Dense video understanding requires answering several questions such as who is doing what to whom, with what, how, why, and where. Recently, Video Situation Recognition (VidSitu) is framed as a task for structured prediction of multiple events, their relationships, and actions and various verb-role pairs attached to descriptive entities. This task poses several challenges in identifying, disambiguating, and co-referencing entities across multiple verb-role pairs, but also faces some challenges of evaluation. In this work, we propose the addition of spatio-temporal grounding as an essential component of the structured prediction task in a weakly supervised setting, and present a novel three stage Transformer model, VideoWhisperer, that is empowered to make joint predictions. In stage one, we learn contextualised embeddings for video features in parallel with key objects that appear in the video clips to enable fine-grained spatio-temporal reasoning. The second stage sees verb-role queries attend and pool information from object embeddings, localising answers to questions posed about the action. The final stage generates these answers as captions to describe each verb-role pair present in the video. Our model operates on a group of events (clips) simultaneously and predicts verbs, verb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on a grounding-augmented version of the VidSitu dataset, we observe a large improvement in entity captioning accuracy, as well as the ability to localize verb-roles without grounding annotations at training time.",
      "tldr": "We present a new task Grounded Video Situation Recognition(GVSR). In addition to predicting the verbs, and semantic roles in the form of captions, we also ground them in the spatio-temporal domain in weakly-supervised setup in an end-to-end fashion. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53297",
      "pdf_url": "https://openreview.net/pdf?id=yRhbHp_Vh8e",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~C.V._Jawahar1",
        "name": "C.V. Jawahar",
        "name_site": null,
        "openreview_id": "~C.V._Jawahar1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.iiit.ac.in/~jawahar/",
        "dblp_id": "j/CVJawahar",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=U9dH-DoAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.5,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "3LBxVcnsEkV",
      "title": "GREED: A Neural Framework for Learning Graph Distance Functions",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline.",
      "tldr": "Learning graph and subgraph edit distance using graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/54507",
      "pdf_url": "https://openreview.net/pdf?id=3LBxVcnsEkV",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesan_Chakaravarthy1",
        "name": "Venkatesan Chakaravarthy",
        "name_site": null,
        "openreview_id": "~Venkatesan_Chakaravarthy1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://dblp.org/pid/c/VTChakaravarthy.html",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_3I7KHAAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.4,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 4.666666666666667,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Chakraborty1",
        "name": "Anirban Chakraborty",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 4,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.4,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "bGo0A4bJBc",
      "title": "Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training.  Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks.  Our results demonstrate that CSST achieves an improvement over the state-of-the-art in majority of the cases across datasets and objectives.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/55114",
      "pdf_url": "https://openreview.net/pdf?id=bGo0A4bJBc",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "kato.takashi_01@fujitsu.com",
        "name": "Kato Takashi",
        "name_site": null,
        "openreview_id": "kato.takashi_01@fujitsu.com",
        "position": 4,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.4,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 2.160246899469287,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Ravinder_Bhattoo1",
        "name": "Ravinder Bhattoo",
        "name_site": "Ravinder Bhattoo, Sayan Ranu, N M Anoop Krishnan",
        "openreview_id": "~Ravinder_Bhattoo1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://ravinderbhattoo.github.io",
        "dblp_id": null,
        "google_scholar_url": "lPTdGRMAAAAJ",
        "orcid": "0000-0003-0323-9108",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.4,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Prem_Selvaraj1",
        "name": "Prem Selvaraj",
        "name_site": null,
        "openreview_id": "~Prem_Selvaraj1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://github.com/prem-kumar27",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "AI4Bharat (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.4,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8lQDn9zTQlW",
      "title": "BigBio: A Framework for Data-Centric Biomedical Natural Language Processing",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Training and evaluating language models increasingly requires the construction of meta-datasets -- diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a variety of novel instruction tuning tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBio a community library of 126+ biomedical NLP datasets, currently covering 13 task categories and 10+ languages. BigBio facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBio is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",
      "tldr": "BigBio is a community library of 126+ biomedical NLP datasets, covering 13 tasks and 10 languages. ",
      "site_url": "https://nips.cc/virtual/2022/poster/55683",
      "pdf_url": "https://openreview.net/pdf?id=8lQDn9zTQlW",
      "github_url": "",
      "total_authors": 43,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Robert_Martin3",
        "name": "Robert Martin",
        "name_site": null,
        "openreview_id": "~Robert_Martin3",
        "position": 28,
        "gender": null,
        "homepage_url": "https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Elucidata (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.3571428571428571,
      "reviews": {
        "rating_mean": 7.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 58,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "9DYKrsFSU2",
      "title": "Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converges to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\\% across imbalanced datasets. The code is available at https://github.com/val-iisc/Saddle-LongTail.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53304",
      "pdf_url": "https://openreview.net/pdf?id=9DYKrsFSU2",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Chakraborty1",
        "name": "Anirban Chakraborty",
        "name_site": null,
        "openreview_id": "~Venkatesh_Babu_Radhakrishnan2",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cds.iisc.ac.in/faculty/venky",
        "dblp_id": "20/6289",
        "google_scholar_url": "cVg7HrEAAAAJ",
        "orcid": "0000-0002-1926-1804",
        "linkedin_url": "venkatesh-babu-radhakrishnan-16568939",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.33333333333333337,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 2.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EqZuN4V_FLF",
      "title": "A Solver-free Framework for Scalable Learning in Neural ILP Architectures",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There is a recent focus on designing architectures that have an Integer Linear Programming (ILP) layer within a neural model (referred to as \\emph{Neural ILP} in this paper). Neural ILP architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning (ILP). A recent SOTA approach for end-to-end training of Neural ILP explicitly defines gradients through the ILP black box [Paulus et al. [2021]] – this trains extremely slowly, owing to a call to the underlying ILP solver for every training data point in a minibatch. In response, we present an alternative training strategy that is \\emph{solver-free}, i.e., does not call the ILP solver at all at training time. Neural ILP has a set of trainable hyperplanes (for cost and constraints in ILP), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation.  While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an ILP, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual Sudoku, to which the other Neural ILP solver is not able to scale.",
      "tldr": "For learning constraints in a neural ILP architecture, we propose a scalable solver-free framework that doesn't require calling the solver to compute gradients.",
      "site_url": "https://nips.cc/virtual/2022/poster/53251",
      "pdf_url": "https://openreview.net/pdf?id=EqZuN4V_FLF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mausam_.1",
        "name": "Mausam .",
        "name_site": null,
        "openreview_id": "~Parag_Singla1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.cse.iitd.ac.in/~parags",
        "dblp_id": "14/167",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=V49BsgMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.33333333333333337,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.0,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Alexandre_V._Evfimievski1",
        "name": "Alexandre V. Evfimievski",
        "name_site": null,
        "openreview_id": "~Alexandre_V._Evfimievski1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "e/AlexandreVEvfimievski",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.33333333333333337,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nrOLtfeiIdh",
      "title": "Learning Recourse on Instance Environment to Enhance Prediction Accuracy",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of  alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called {\\em RecourseNet} that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier.   Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent --- for example the best camera angle for object recognition could be a function of the object's shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence.",
      "tldr": "Learning to recourse instances through interventions on the environment space so that the recoursed instances deliver enhanced prediction accuracy by the downstream model. ",
      "site_url": "https://nips.cc/virtual/2022/poster/53322",
      "pdf_url": "https://openreview.net/pdf?id=nrOLtfeiIdh",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 3,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.33333333333333337,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "xbgtFOO9J5D",
      "title": "Fair Rank Aggregation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Ranking algorithms find extensive usage in diverse areas such as web search, employment, college\n    admission, voting, etc.  The related rank aggregation problem deals with combining multiple\n    rankings into a single aggregate ranking.  However, algorithms for both these problems might be\n    biased against some individuals or groups due to implicit prejudice or marginalization in the\n    historical data.  We study ranking and rank aggregation problems from a fairness or diversity\n    perspective, where the candidates (to be ranked) may belong to different groups and each group\n    should have a fair representation in the final ranking. We allow the designer to set the\n    parameters that define fair representation. These parameters specify the allowed range of the\n    number of candidates from a particular group in the top-$k$ positions of the ranking.  Given any\n    ranking, we provide a fast and exact algorithm for finding the closest fair ranking for the\n    Kendall tau metric under {\\em strong fairness}, i.e., when the final ranking is fair for all\n    values of $k$. We also provide an exact algorithm for finding the closest fair ranking for the\n    Ulam metric under strong fairness when there are only $O(1)$ number of groups.  Our\n    algorithms are simple, fast, and might be extendable to other relevant metrics. We also give a\n    novel  meta-algorithm for the general rank aggregation problem under the fairness framework.\n    Surprisingly, this meta-algorithm works for any generalized mean objective (including center and\n    median problems) and any fairness criteria. As a byproduct, we obtain 3-approximation algorithms\n    for both center and median problems, under both Kendall tau and Ulam metrics. Furthermore, using\n    sophisticated techniques we obtain a $(3-\\varepsilon)$-approximation algorithm, for a constant\n    $\\varepsilon>0$,  for the Ulam metric under strong fairness.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/53193",
      "pdf_url": "https://openreview.net/pdf?id=xbgtFOO9J5D",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Arindam_Khan1",
        "name": "Arindam Khan",
        "name_site": null,
        "openreview_id": "~Arindam_Khan1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.csa.iisc.ac.in/~arindamkhan/",
        "dblp_id": "https://dblp.uni-trier.de/pid/96/9083-1.html",
        "google_scholar_url": "yRsbV0AAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.33333333333333337,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "nN3aVRQsxGd",
      "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing---aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to $K$-hop message passing by aggregating information from $K$-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of $K$-hop message passing. In this work, we theoretically characterize the expressive power of $K$-hop message passing. Specifically, we first formally differentiate two different kernels of $K$-hop message passing which are often misused in previous works. We then characterize the expressive power of $K$-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that $K$-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves $K$-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.",
      "tldr": "In this paper, we first theoretically analyze the expressive power and the limitation of K-hop message passing graph neural networks. Then, we propose a novel method to improve the K-hop message passing framework.",
      "site_url": "https://nips.cc/virtual/2022/poster/53551",
      "pdf_url": "https://openreview.net/pdf?id=nN3aVRQsxGd",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anindya_Sarkar2",
        "name": "Anindya Sarkar",
        "name_site": "Anindya Sarkar, Anirban Sarkar, Sowrya Gali, Vineeth N Balasubramanian",
        "openreview_id": "~Anindya_Sarkar2",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/anindya-sarkar/home",
        "dblp_id": null,
        "google_scholar_url": "2hQyYz0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Washington University in St. Louis (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.25,
      "reviews": {
        "rating_mean": 4.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 163,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "5kThooa07pf",
      "title": "Subsidiary Prototype Alignment for Universal Domain Adaptation",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the \"known\" categories or into a single \"unknown\" category. A major problem in UniDA is negative transfer, i.e. misalignment of \"known\" and \"unknown\" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.",
      "tldr": "We address negative-transfer in Universal DA with BoW-inspired word-prototypes and subsidiary alignment via a word-related pretext task.",
      "site_url": "https://nips.cc/virtual/2022/poster/55426",
      "pdf_url": "https://openreview.net/pdf?id=5kThooa07pf",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Varun_Jampani2_4",
        "name": "Varun Jampani",
        "name_site": null,
        "openreview_id": "~SUVAANSH_BHAMBRI2",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "275/7440",
        "google_scholar_url": "UQjAvO8AAAAJ",
        "orcid": "0000-0003-3941-5396",
        "linkedin_url": "suvaansh-bhambri-1784bab7/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.19999999999999996,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 25,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tXEe-Ew_ikh",
      "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
      "tldr": "Benchmarking physics-informed graph neural networks",
      "site_url": "https://nips.cc/virtual/2022/poster/55670",
      "pdf_url": "https://openreview.net/pdf?id=tXEe-Ew_ikh",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~N_M_Anoop_Krishnan1",
        "name": "N M Anoop Krishnan",
        "name_site": null,
        "openreview_id": "~N_M_Anoop_Krishnan1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=fGnjHcEAAAAJ",
        "orcid": "0000-0003-1500-4947",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.19999999999999996,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 2.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 39,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "zBBmV-i84Go",
      "title": "Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.\n\nWe make three contributions.\n- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).\n- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.\n- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.\n\nAll datasets, models, and code has been made open-source via the OpenHands toolkit.",
      "tldr": "We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results",
      "site_url": "https://nips.cc/virtual/2022/poster/55634",
      "pdf_url": "https://openreview.net/pdf?id=zBBmV-i84Go",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Pratyush_Kumar1",
        "name": "Pratyush Kumar",
        "name_site": null,
        "openreview_id": "~Pratyush_Kumar1",
        "position": 5,
        "gender": "M",
        "homepage_url": "http://www.cse.iitm.ac.in/~pratyush/",
        "dblp_id": "25/2468",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.19999999999999996,
      "reviews": {
        "rating_mean": 7.666666666666667,
        "rating_std": 1.3743685418725535,
        "confidence_mean": 4.333333333333333,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ajH17-Pb43A",
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "status": "Accept",
      "normalized_status": "unknown",
      "abstract": "Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset\nfor different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3×-30× while achieving comparable performance to the hyper-parameters found using the entire dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2022/poster/54004",
      "pdf_url": "https://openreview.net/pdf?id=ajH17-Pb43A",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Lucian_Popa1",
        "name": "Lucian Popa",
        "name_site": null,
        "openreview_id": "~Lucian_Popa1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "https://dblp.dagstuhl.de/pers/hd/p/Popa_0001:Lucian",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Business Machines Corporation (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.16666666666666663,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8FuITQn6rG3",
      "title": "CRAFT: explaining using Concepts from Recursive Activation FacTorization",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Despite their considerable potential, concept-based explainability methods have received relatively little attention, and explaining what’s driving models’ decisions and where it’s located in the input is still an open problem. To tackle this, we revisit unsupervised concept extraction techniques for explaining the decisions of deep neural networks and present CRAFT – a framework to generate concept-based explanations for understanding individual predictions and the model’s high-level logic for whole classes. CRAFT takes advantage of a novel method for recursively decomposing higher-level concepts into more elementary ones, combined with a novel approach for better estimating the importance of identified concepts with Sobol indices. Furthermore, we show how implicit differentiation can be used to generate concept-wise attribution explanations for individual images. We further demonstrate through fidelity metrics that our proposed concept importance estimation technique is more faithful to the model than previous methods, and, through human psychophysic experiments, we confirm that our recursive decomposition can generate meaningful and accurate concepts. Finally, we illustrate CRAFT’s potential to enable the understanding of predictions of trained models on multiple use-cases by producing meaningful concept-based explanations.",
      "tldr": "Revisiting ACE to automatically discover Concepts for a classifier. Introduce Sobol total indice for Concept Importance and Concept Attribution Maps.",
      "site_url": "https://openreview.net/forum?id=8FuITQn6rG3",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    }
  ]
}