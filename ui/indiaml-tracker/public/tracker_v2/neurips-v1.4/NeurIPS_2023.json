{
  "conference": "NeurIPS 2023",
  "focus_country": "India",
  "total_papers": 88,
  "generated_at": "2025-07-06T10:39:19.854078",
  "config": {
    "first_author_weight": 3.0,
    "last_author_weight": 2.0,
    "middle_author_weight": 1.0,
    "status_weights": {
      "oral": 10.0,
      "spotlight": 7.5,
      "poster": 5.0,
      "unknown": 1.0
    },
    "output_format": "json",
    "include_review_details": true,
    "include_citation_data": true
  },
  "papers": [
    {
      "paper_id": "1vzF4zWQ1E",
      "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race.  Conventional wisdom dictates that model biases arise from biased training data.  As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition.  In our work, we discover that biases are actually inherent to neural network architectures themselves.  Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes. We release our code, models and raw data files at https://github.com/dooleys/FR-NAS.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72984",
      "pdf_url": "https://openreview.net/pdf?id=1vzF4zWQ1E",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samuel_Dooley1",
        "name": "Samuel Dooley",
        "name_site": null,
        "openreview_id": "~Samuel_Dooley1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Freiburg (Germany)",
        "countries": [
          "Germany"
        ],
        "country_codes": [
          "DE"
        ]
      },
      "sort_score": 30.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 0.8944271909999159,
        "confidence_mean": 4.2,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "KvPwXVcslY",
      "title": "Spatial-frequency channels, shape bias, and adversarial robustness",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or \"channel\") that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51% variance explained) and robustness of adversarially-trained networks (66% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71982",
      "pdf_url": "https://openreview.net/pdf?id=KvPwXVcslY",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ajay_Subramanian1",
        "name": "Ajay Subramanian",
        "name_site": null,
        "openreview_id": "~Ajay_Subramanian1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://ajaysubramanian.com",
        "dblp_id": null,
        "google_scholar_url": "6cyu_EgAAAAJ",
        "orcid": "0000-0003-1017-9000",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "New York University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 30.0,
      "reviews": {
        "rating_mean": 7.0,
        "rating_std": 1.8973665961010275,
        "confidence_mean": 4.4,
        "confidence_std": 0.7999999999999999,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 33,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "DBz9E5aZey",
      "title": "Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Stein Variational Gradient Descent (SVGD) is a popular particle-based variational inference algorithm with impressive empirical performance across various domains. Although the population (i.e, infinite-particle) limit dynamics of SVGD is well characterized, its behavior in the finite-particle regime is far less understood. To this end, our work introduces the notion of *virtual particles* to develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, that are exactly realizable using finite particles. As a result, we design two computationally efficient variants of SVGD, namely VP-SVGD and GB-SVGD, with provably fast finite-particle convergence rates. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinary SVGD. We show that the $n$ particles output by VP-SVGD and GB-SVGD, run for $T$ steps with batch-size $K$, are at-least as good as i.i.d samples from a distribution whose Kernel Stein Discrepancy to the target is at most $O(\\tfrac{d^{1/3}}{(KT)^{1/6}})$ under standard assumptions. Our results also hold under a mild growth condition on the potential function, which is much weaker than the isoperimetric (e.g. Poincare Inequality) or information-transport conditions (e.g. Talagrand's Inequality $\\mathsf{T}_1$) generally considered in prior works. As a corollary, we analyze the convergence of the empirical measure (of the particles output by VP-SVGD and GB-SVGD) to the target distribution and demonstrate a **double exponential improvement** over the best known finite-particle analysis of SVGD. Beyond this, our results present the **first known oracle complexities for this setting with polynomial dimension dependence**, thereby completely eliminating the curse of dimensionality exhibited by previously known finite-particle rates.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72386",
      "pdf_url": "https://openreview.net/pdf?id=DBz9E5aZey",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aniket_Das1",
        "name": "Aniket Das",
        "name_site": "Aniket Das, Bernhard Sch√∂lkopf, Michael Muehlebach",
        "openreview_id": "~Aniket_Das1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://aniket1998.github.io",
        "dblp_id": "248/8281",
        "google_scholar_url": "o8Dyas0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 1.5,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MziFFGjpkb",
      "title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual ``concepts'' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that recast the first step -- concept extraction problem -- as a special case of **dictionary learning**, and we formalize the second step -- concept importance estimation -- as a more general form of **attribution method**.\nThis framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii)  to derive theoretical guarantees regarding the optimality of such methods. \n\nWe further leverage our framework to try to tackle a crucial question in explainability: how to *efficiently* identify clusters of data points that are classified based on a similar shared strategy.\nTo illustrate these findings and to highlight the main strategies of a model, we introduce a visual representation called the strategic cluster graph. Finally, we present Lens, a dedicated website that offers a complete compilation of these visualizations for all classes of the ImageNet dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71885",
      "pdf_url": "https://openreview.net/pdf?id=MziFFGjpkb",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.8660254037844386,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 56,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rUf0GV5CuU",
      "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by  embedded representations, thereby extending set containment to *soft* set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise *vector* *dominance*. This 0/1 property can be relaxed to an asymmetric *hinge* *distance* for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few  simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed *dominance* *similarity* measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70312",
      "pdf_url": "https://openreview.net/pdf?id=rUf0GV5CuU",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Indradyumna_Roy1",
        "name": "Indradyumna Roy",
        "name_site": "Indradyumna Roy, Soumen Chakrabarti, Abir De",
        "openreview_id": "~Indradyumna_Roy1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://indradyumna.github.io/",
        "dblp_id": "124/9185.html",
        "google_scholar_url": "qb70i84AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 22.5,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1jrYSOG7DR",
      "title": "Revealing the unseen: Benchmarking video action recognition under occlusion",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In this work, we study the effect of occlusion on video action recognition. To\nfacilitate this study, we propose three benchmark datasets and experiment with\nseven different video action recognition models. These datasets include two synthetic benchmarks, UCF-101-O and K-400-O, which enabled understanding the\n effects of fundamental properties of occlusion via controlled experiments. We also\n propose a real-world occlusion dataset, UCF-101-Y-OCC, which helps in further\n validating the findings of this study. We find several interesting insights such as 1)\n transformers are more robust than CNN counterparts, 2) pretraining make models\nrobust against occlusions, and 3) augmentation helps, but does not generalize\n well to real-world occlusions. In addition, we propose a simple transformer based\n compositional model, termed as CTx-Net, which generalizes well under this distribution shift. We observe that CTx-Net outperforms models which are trained\n using occlusions as augmentation, performing significantly better under natural\n occlusions. We believe this benchmark will open up interesting future research in\n robust video action recognition",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73717",
      "pdf_url": "https://openreview.net/pdf?id=1jrYSOG7DR",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Shresth_Grover1",
        "name": "Shresth Grover",
        "name_site": "Shresth Grover, Vibhav Vineet, Yogesh Rawat",
        "openreview_id": "~Shresth_Grover1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "shrgo/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2EiqizElGO",
      "title": "ViSt3D: Video Stylization with 3D CNN",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Visual stylization has been a very popular research area in recent times. While image stylization has seen a rapid advancement in the recent past, video stylization, while being more challenging, is relatively less explored. The immediate method of stylizing videos by stylizing each frame independently has been tried with some success. To the best of our knowledge, we present the first approach to video stylization using 3D CNN directly, building upon insights from 2D image stylization. Stylizing video is highly challenging, as the appearance and video motion, which includes both camera and subject motions, are inherently entangled in the representations learnt by a 3D CNN. Hence, a naive extension of 2D CNN stylization methods to 3D CNN does not work. To perform stylization with 3D CNN, we propose to explicitly disentangle motion and appearance, stylize the appearance part, and then add back the motion component and decode the final stylized video. In addition, we propose a dataset, curated from existing datasets, to train video stylization networks. We also provide an independently collected test set to study the generalization of video stylization methods. We provide results on this test dataset comparing the proposed method with 2D stylization methods applied frame by frame. We show successful stylization with 3D CNN for the first time, and obtain better stylization in terms of texture cf.\\ the existing 2D methods.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72962",
      "pdf_url": "https://openreview.net/pdf?id=2EiqizElGO",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ayush_Pande1",
        "name": "Ayush Pande",
        "name_site": "Ayush Pande, Gaurav Sharma",
        "openreview_id": "~Ayush_Pande1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "369/7183",
        "google_scholar_url": "HYr7bkAAAAAJ",
        "orcid": null,
        "linkedin_url": "ayush-pande-a296a063",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kanpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Bipasha_Sen1",
        "name": "Bipasha Sen",
        "name_site": "Bipasha Sen, Gaurav Singh, Aditya Agarwal, Rohith Agaram, Madhava Krishna, Srinath Sridhar",
        "openreview_id": "~Bipasha_Sen1",
        "position": 1,
        "gender": "F",
        "homepage_url": "http://bipashasen.github.io/",
        "dblp_id": null,
        "google_scholar_url": "GZZCH-8AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "INS3ltgjg7",
      "title": "TopoSRL: Topology preserving self-supervised Simplicial Representation Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In this paper, we introduce $\\texttt{TopoSRL}$, a novel self-supervised learning (SSL) method for simplicial complexes to effectively capture higher-order interactions and preserve topology in the learned representations. $\\texttt{TopoSRL}$ addresses the limitations of existing graph-based SSL methods that typically concentrate on pairwise relationships, neglecting long-range dependencies crucial to capture topological information. We propose a new simplicial augmentation technique that generates two views of the simplicial complex that enriches the representations while being efficient. Next, we propose a new simplicial contrastive loss function that contrasts the generated simplices to preserve local and global information present in the simplicial complexes. Extensive experimental results demonstrate the superior performance of $\\texttt{TopoSRL}$ compared to state-of-the-art graph SSL techniques and supervised simplicial neural models across various datasets corroborating the efficacy of $\\texttt{TopoSRL}$ in processing simplicial complex data in a self-supervised setting.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72124",
      "pdf_url": "https://openreview.net/pdf?id=INS3ltgjg7",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Hiren_Madhu1",
        "name": "Hiren Madhu",
        "name_site": "Hiren Madhu, Sundeep Prabhakar Chepuri",
        "openreview_id": "~Hiren_Madhu1",
        "position": 1,
        "gender": "M",
        "homepage_url": "http://hirenmadhu.github.io",
        "dblp_id": null,
        "google_scholar_url": "Bt8Q-x0AAAAJ",
        "orcid": "0000-0002-6701-6782",
        "linkedin_url": "hiren-madhu/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.8944271909999159,
        "confidence_mean": 3.4,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "J7VoDuzuKs",
      "title": "Unlocking Feature Visualization for Deep Network with MAgnitude Constrained Optimization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Feature visualization has gained significant popularity as an explainability method, particularly after the influential work by Olah et al. in 2017. Despite its success, its widespread adoption has been limited due to issues in scaling to deeper neural networks and the reliance on tricks to generate interpretable images. Here, we describe MACO, a simple approach to address these shortcomings. It consists in optimizing solely an image's phase spectrum while keeping its magnitude constant to ensure that the generated explanations lie in the space of natural images. Our approach yields significantly better results -- both qualitatively and quantitatively -- unlocking efficient and interpretable feature visualizations for state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing to augment feature visualizations with spatial importance. Furthermore, we enable quantitative evaluation of feature visualizations by introducing 3 metrics: transferability, plausibility, and alignment with natural images. We validate our method on various applications and we introduce a website featuring MACO visualizations for all classes of the ImageNet dataset, which will be made available upon acceptance. \nOverall, our study unlocks feature visualizations for the largest, state-of-the-art classification networks without resorting to any parametric prior image model, effectively advancing a field that has been stagnating since 2017 (Olah et al, 2017).",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72075",
      "pdf_url": "https://openreview.net/pdf?id=J7VoDuzuKs",
      "github_url": "",
      "total_authors": 12,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 4.8,
        "rating_std": 1.16619037896906,
        "confidence_mean": 4.2,
        "confidence_std": 0.7483314773547882,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 19,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MCkUS1P3Sh",
      "title": "Nash Regret Guarantees for Linear Bandits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening---referred to as Nash regret---is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.    \n\nWe consider the stochastic linear bandits problem over a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward---associated with each arm in ${\\cal X}$---is a non-negative, sub-Poisson random variable. For this setting, we develop an algorithm that achieves a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$. In addition, addressing linear bandit instances in which the set of arms ${\\cal X}$ is not necessarily finite, we obtain a Nash regret upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$. Since bounded random variables are sub-Poisson, these results hold for bounded, non-negative rewards. Our linear bandit algorithm is built upon the successive elimination method with novel technical insights, including tailored concentration bounds and the use of sampling via John ellipsoid in conjunction with the Kiefer‚ÄìWolfowitz optimal design.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71921",
      "pdf_url": "https://openreview.net/pdf?id=MCkUS1P3Sh",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ayush_Sawarni1",
        "name": "Ayush Sawarni",
        "name_site": "Ayush Sawarni, Ayush Sawarni, Soumyabrata Pal, Siddharth Barman",
        "openreview_id": "~Ayush_Sawarni1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sawarniayush.github.io/",
        "dblp_id": null,
        "google_scholar_url": "U8TSPdAAAAAJ",
        "orcid": null,
        "linkedin_url": "ayush-sawarni",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 0.39999999999999997,
        "confidence_mean": 2.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MamHShmHiX",
      "title": "Approximate inference of marginals using the IBIA framework",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF  and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity trade-off is controlled using user-defined clique size bounds. Results for several benchmark sets from recent UAI competitions show that our method gives either better or comparable accuracy than existing variational and sampling based methods, with smaller runtimes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71902",
      "pdf_url": "https://openreview.net/pdf?id=MamHShmHiX",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Shivani_Bathla1",
        "name": "Shivani Bathla",
        "name_site": "Shivani Bathla, Vinita Vasudevan",
        "openreview_id": "~Shivani_Bathla1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": "235/6437",
        "google_scholar_url": "gHsMrPIAAAAJ",
        "orcid": "0000-0002-1109-0836",
        "linkedin_url": "shivani-bathla-aa945514/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.666666666666667,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OXOLiS0ak6",
      "title": "A Dataset for Analyzing Streaming Media Performance over HTTP/3 Browsers",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "HTTP/3 is a new application layer protocol supported by most browsers. It uses QUIC as an underlying transport protocol. QUIC provides multiple benefits, like faster connection establishment, reduced latency, and improved connection migration. Hence, most popular browsers like Chrome/Chromium, Microsoft Edge, Apple Safari, and Mozilla Firefox have started supporting it. In this paper, we present an HTTP/3-supported browser dataset collection tool named H3B. It collects the application and network-level logs during YouTube streaming. We consider YouTube, as it  the most popular video streaming application supporting QUIC. Using this tool, we collected a dataset of over 5936 YouTube sessions covering 5464 hours of streaming over 5 different geographical locations and 5 different bandwidth patterns. We believe our tool and as well as the dataset could be used in multiple applications such as a better configuration of application/transport protocols based on the network conditions, intelligent integration of network and application, predicting YouTube's QoE etc. \nWe analyze the dataset and observe that during an HTTP/3 streaming not all requests are served by HTTP/3. Instead whenever the network condition is not favorable the browser chooses to fallback, and the application requests are transmitted using HTTP/2 over the old-standing transport protocol TCP. We observe that such switching of protocols impacts the performance of video streaming applications.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73606",
      "pdf_url": "https://openreview.net/pdf?id=OXOLiS0ak6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sapna_Chaudhary1",
        "name": "Sapna Chaudhary",
        "name_site": "Sapna Chaudhary, Mukulika Maity, Sandip Chakraborty, Naval Shukla",
        "openreview_id": "~Sapna_Chaudhary1",
        "position": 1,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "l9qK0CkAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 1.7204650534085253,
        "confidence_mean": 3.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PBpEb86bj7",
      "title": "ATMAN: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of additional memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. \nThis makes it difficult, if not impossible, to use explanations in production. \nWe present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method relying on cosine similarity neighborhood in the embedding space. \nOur exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71776",
      "pdf_url": "https://openreview.net/pdf?id=PBpEb86bj7",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Bj√∂rn_Deiseroth1_1",
        "name": "Bj√∂rn Deiseroth",
        "name_site": "Bj√∂rn Deiseroth, Mayukh Deb, Samuel Weinbach, Manuel Brack, Patrick Schramowski, Kristian Kersting",
        "openreview_id": "~Mayukh_Deb2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://mayukhdeb.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": "0000-0003-2826-2857",
        "linkedin_url": "mayukhdeb/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Amrita Vishwa Vidyapeetham (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.3564659966250536,
        "confidence_mean": 3.4,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 30,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Rs6pzz21U4",
      "title": "A Partially-Supervised Reinforcement Learning Framework for Visual Active Search",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Visual active search (VAS) has been proposed as a  modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search approaches by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. In addition, we develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71640",
      "pdf_url": "https://openreview.net/pdf?id=Rs6pzz21U4",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anindya_Sarkar2",
        "name": "Anindya Sarkar",
        "name_site": "Anindya Sarkar, Anirban Sarkar, Sowrya Gali, Vineeth N Balasubramanian",
        "openreview_id": "~Anindya_Sarkar2",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/anindya-sarkar/home",
        "dblp_id": null,
        "google_scholar_url": "2hQyYz0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Washington University in St. Louis (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 3.25,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VNjJAWjuEU",
      "title": "Graph of Circuits with GNN for Exploring the Optimal Design Space",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning  enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71460",
      "pdf_url": "https://openreview.net/pdf?id=VNjJAWjuEU",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Hemant_Shahane1",
        "name": "Aditya Hemant Shahane",
        "name_site": "Aditya Shahane, Saripilli Swapna Manjiri, Ankesh Jain, Sandeep Kumar",
        "openreview_id": "~Aditya_Hemant_Shahane1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "aditya-shahane-211bba158/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.3564659966250536,
        "confidence_mean": 2.4,
        "confidence_std": 1.2000000000000002,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "arkmhtYLL6",
      "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Humans use abstract *concepts* for understanding instead of hard features. Recent interpretability research has focused on human-centered concept explanations of neural networks. Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept. We extend CAVs from post-hoc analysis to ante-hoc training to reduce model bias through fine-tuning using an additional *Concept Loss*. Concepts are defined on the final layer of the network in the past. We generalize it to intermediate layers, including the last convolution layer. We also introduce *Concept Distillation*, a method to define rich and effective concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize a model towards concepts. We show applications of concept-sensitive training to debias several classification problems. We also show a way to induce prior knowledge into a reconstruction problem. We show that concept-sensitive training can improve model interpretability, reduce biases, and induce prior knowledge.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71179",
      "pdf_url": "https://openreview.net/pdf?id=arkmhtYLL6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Avani_Gupta1",
        "name": "Avani Gupta",
        "name_site": "Avani Gupta, Saurabh Saini, P J Narayanan",
        "openreview_id": "~Avani_Gupta1",
        "position": 1,
        "gender": "F",
        "homepage_url": "https://avani17101.github.io/",
        "dblp_id": "319/9403",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0003-1262-4286",
        "linkedin_url": "avani17101-gupta/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.5,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "crNAh1EZKo",
      "title": "No-regret Algorithms for Fair Resource Allocation",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\\alpha$-fair utilities of the agents achieved by an optimal static clairvoyant allocation and the online policy grows sublinearly with time. The problem inherits its difficulty from the non-separable nature of the global $\\alpha$-fairness function. Previously, it was shown that no online policy could achieve a sublinear standard regret in this problem. In this paper, we propose an efficient online resource allocation policy, called Online Fair Allocation ($\\texttt{OFA}$), that achieves sublinear $c_\\alpha$-approximate regret with approximation factor $c_\\alpha=(1-\\alpha)^{-(1-\\alpha)}\\leq 1.445,$ for $0\\leq \\alpha < 1$. Our upper bound on the $c_\\alpha$-regret for this problem exhibits a surprising \\emph{phase transition} phenomenon -- transitioning from a power-law to a constant at the critical exponent $\\alpha=\\frac{1}{2}.$  Our result also resolves an open problem in designing an efficient no-regret policy for the online job scheduling problem in certain parameter regimes. Along the way, we introduce new algorithmic and analytical techniques, including greedy estimation of the future gradients for non-additive global reward functions and bootstrapping second-order regret bounds, which may be of independent interest.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71066",
      "pdf_url": "https://openreview.net/pdf?id=crNAh1EZKo",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhishek_Sinha3",
        "name": "Abhishek Sinha",
        "name_site": null,
        "openreview_id": "~Abhishek_Sinha3",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.tifr.res.in/~abhishek.sinha/",
        "dblp_id": "47/9175",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=Oc7BRX0AAAAJ",
        "orcid": "0000-0001-7220-0691",
        "linkedin_url": "abhishek-sinha-a645291b/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Institute of Fundamental Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "fvm9jVcpBn",
      "title": "Sensitivity in Translation Averaging",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In 3D computer vision, translation averaging solves for absolute translations given a set of pairwise relative translation directions. While there has been much work on robustness to outliers and studies on the uniqueness of the solution, this paper deals with a distinctly different problem of sensitivity in translation averaging under uncertainty. We first analyze sensitivity in estimating scales corresponding to relative directions under small perturbations of the relative directions. Then, we formally define the conditioning of the translation averaging problem, which assesses the reliability of estimated translations based solely on the input directions. We give a sufficient criterion to ensure that the problem is well-conditioned. Subsequently, we provide an efficient algorithm to identify and remove combinations of directions which make the problem ill-conditioned while ensuring uniqueness of the solution. We demonstrate the utility of such analysis in global structure-from-motion pipelines for obtaining 3D reconstructions, which reveals the benefits of filtering the ill-conditioned set of directions in translation averaging in terms of reduced translation errors, a higher number of 3D points triangulated and faster convergence of bundle adjustment.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70880",
      "pdf_url": "https://openreview.net/pdf?id=fvm9jVcpBn",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Lalit_Manam1",
        "name": "Lalit Manam",
        "name_site": "Lalit Manam, Venu Madhav Govindu",
        "openreview_id": "~Lalit_Manam1",
        "position": 1,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "202/6961",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9UpkJwMAAAAJ",
        "orcid": null,
        "linkedin_url": "lalitmanam/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 4.0,
        "confidence_std": 0.7071067811865476,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Eeshaan_Jain1",
        "name": "Eeshaan Jain",
        "name_site": "Eeshaan Jain, Tushar Nandy, Gaurav Aggarwal, Ashish Tendulkar, Rishabh Iyer, Abir De",
        "openreview_id": "~Eeshaan_Jain1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://eeshaanjain.github.io",
        "dblp_id": null,
        "google_scholar_url": "r5rqqJEAAAAJ",
        "orcid": null,
        "linkedin_url": "eeshaanjain/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rUf0GV5CuU",
      "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by  embedded representations, thereby extending set containment to *soft* set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise *vector* *dominance*. This 0/1 property can be relaxed to an asymmetric *hinge* *distance* for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few  simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed *dominance* *similarity* measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70312",
      "pdf_url": "https://openreview.net/pdf?id=rUf0GV5CuU",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tScBQRNgjk",
      "title": "ForecastPFN: Synthetically-Trained Zero-Shot Forecasting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70200",
      "pdf_url": "https://openreview.net/pdf?id=tScBQRNgjk",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Samuel_Dooley1",
        "name": "Samuel Dooley",
        "name_site": null,
        "openreview_id": "~Samuel_Dooley1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 73,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aveen_Dayal1",
        "name": "Aveen Dayal",
        "name_site": "Aveen Dayal, Vimal K B, Linga Reddy Cenkeramaddi, C Mohan, Abhinav Kumar, Vineeth N Balasubramanian",
        "openreview_id": "~Aveen_Dayal1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/iith.ac.in/aveen-dayal/home?authuser=0",
        "dblp_id": "286/2724",
        "google_scholar_url": "KqkN9IgAAAAJ",
        "orcid": "0000-0001-6792-9170",
        "linkedin_url": "aveen-dayal/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yGLokEhdh9",
      "title": "Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of \\textit{virtual stochastic gradient descent (SGD) parameter} computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40\\% compared to the maximally interfered samples retrieval algorithm.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/69957",
      "pdf_url": "https://openreview.net/pdf?id=yGLokEhdh9",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Suresh_kumar_Amalapuram1",
        "name": "Suresh kumar Amalapuram",
        "name_site": "suresh kumar amalapuram, Sumohana Channappayya, Bheemarjuna Reddy Tamma",
        "openreview_id": "~Suresh_kumar_Amalapuram1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://www.linkedin.com/in/suresh-kumar-amalapuram-87a42969/",
        "dblp_id": "305/7572",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=DSHKk8YAAAAJ",
        "orcid": null,
        "linkedin_url": "suresh-kumar-amalapuram-87a42969/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 15.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "1ZzG6td0el",
      "title": "Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We consider distributed parameter estimation using interactive protocols subject to local information constraints such as bandwidth limitations, local differential privacy, and restricted measurements. We provide a unified framework enabling us to derive a variety of (tight) minimax lower bounds for different parametric families of distributions, both continuous and discrete, under any $\\ell_p$ loss. Our lower bound framework is versatile and yields ‚Äúplug-and-play‚Äù bounds that are widely applicable to a large range of estimation problems, and, for the prototypical case of the Gaussian family, circumvents limitations of previous techniques. In particular, our approach recovers bounds obtained using data processing inequalities and Cram√©r‚ÄìRao bounds, two other alternative approaches for proving lower bounds in our setting of interest. Further, for the families considered, we complement our lower bounds with matching upper bounds.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73011",
      "pdf_url": "https://openreview.net/pdf?id=1ZzG6td0el",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Himanshu_Tyagi1",
        "name": "Himanshu Tyagi",
        "name_site": null,
        "openreview_id": "~Himanshu_Tyagi1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "11/4803",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.2,
        "confidence_std": 0.9797958971132712,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 44,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "8Xn3D9OtqI",
      "title": "Mitigating the Effect of Incidental Correlations on Part-based Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Intelligent systems possess a crucial characteristic of breaking complicated problems into smaller reusable components or parts and adjusting to new tasks using these part representations. However, current part-learners encounter difficulties in dealing with incidental correlations resulting from the limited observations of objects that may appear only in specific arrangements or with specific backgrounds. These incidental correlations may have a detrimental impact on the generalization and interpretability of learned part representations. This study asserts that part-based representations could be more interpretable and generalize better with limited data, employing two innovative regularization methods. The first regularization separates foreground and background information's generative process via a unique mixture-of-parts formulation. Structural constraints are imposed on the parts using a weakly-supervised loss, guaranteeing that the mixture-of-parts for foreground and background entails soft, object-agnostic masks. The second regularization assumes the form of a distillation loss, ensuring the invariance of the learned parts to the incidental background correlations. Furthermore, we incorporate sparse and orthogonal constraints to facilitate learning high-quality part representations.\nBy reducing the impact of incidental background correlations on the learned parts, we exhibit state-of-the-art (SoTA) performance on few-shot learning tasks on benchmark datasets, including MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the part-based representations acquired through our approach generalize better than existing techniques, even under domain shifts of the background and common data corruption on the ImageNet-9 dataset.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72642",
      "pdf_url": "https://openreview.net/pdf?id=8Xn3D9OtqI",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineeth_N._Balasubramanian2",
        "name": "Vineeth N. Balasubramanian",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (Canada)",
        "countries": [
          "Canada"
        ],
        "country_codes": [
          "CA"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.0,
        "confidence_mean": 3.75,
        "confidence_std": 0.82915619758885,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Srinath_Sridhar2_1",
        "name": "Srinath Sridhar",
        "name_site": null,
        "openreview_id": "~Srinath_Sridhar1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://srinathsridhar.com",
        "dblp_id": "78/1463-2",
        "google_scholar_url": "qIvZT74AAAAJ",
        "orcid": null,
        "linkedin_url": "srinathsridhar",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Amazon (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "INS3ltgjg7",
      "title": "TopoSRL: Topology preserving self-supervised Simplicial Representation Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "In this paper, we introduce $\\texttt{TopoSRL}$, a novel self-supervised learning (SSL) method for simplicial complexes to effectively capture higher-order interactions and preserve topology in the learned representations. $\\texttt{TopoSRL}$ addresses the limitations of existing graph-based SSL methods that typically concentrate on pairwise relationships, neglecting long-range dependencies crucial to capture topological information. We propose a new simplicial augmentation technique that generates two views of the simplicial complex that enriches the representations while being efficient. Next, we propose a new simplicial contrastive loss function that contrasts the generated simplices to preserve local and global information present in the simplicial complexes. Extensive experimental results demonstrate the superior performance of $\\texttt{TopoSRL}$ compared to state-of-the-art graph SSL techniques and supervised simplicial neural models across various datasets corroborating the efficacy of $\\texttt{TopoSRL}$ in processing simplicial complex data in a self-supervised setting.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72124",
      "pdf_url": "https://openreview.net/pdf?id=INS3ltgjg7",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sundeep_Prabhakar_Chepuri1",
        "name": "Sundeep Prabhakar Chepuri",
        "name_site": null,
        "openreview_id": "~Sundeep_Prabhakar_Chepuri1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://ece.iisc.ac.in/~spchepuri/",
        "dblp_id": "72/10237.html",
        "google_scholar_url": "Gu8FjdwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.8944271909999159,
        "confidence_mean": 3.4,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MCkUS1P3Sh",
      "title": "Nash Regret Guarantees for Linear Bandits",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening---referred to as Nash regret---is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.    \n\nWe consider the stochastic linear bandits problem over a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward---associated with each arm in ${\\cal X}$---is a non-negative, sub-Poisson random variable. For this setting, we develop an algorithm that achieves a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$. In addition, addressing linear bandit instances in which the set of arms ${\\cal X}$ is not necessarily finite, we obtain a Nash regret upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$. Since bounded random variables are sub-Poisson, these results hold for bounded, non-negative rewards. Our linear bandit algorithm is built upon the successive elimination method with novel technical insights, including tailored concentration bounds and the use of sampling via John ellipsoid in conjunction with the Kiefer‚ÄìWolfowitz optimal design.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71921",
      "pdf_url": "https://openreview.net/pdf?id=MCkUS1P3Sh",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Siddharth_Barman1",
        "name": "Siddharth Barman",
        "name_site": null,
        "openreview_id": "~Siddharth_Barman1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~barman/",
        "dblp_id": "63/478.html",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=HcGQSKIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 0.39999999999999997,
        "confidence_mean": 2.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MamHShmHiX",
      "title": "Approximate inference of marginals using the IBIA framework",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF  and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity trade-off is controlled using user-defined clique size bounds. Results for several benchmark sets from recent UAI competitions show that our method gives either better or comparable accuracy than existing variational and sampling based methods, with smaller runtimes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71902",
      "pdf_url": "https://openreview.net/pdf?id=MamHShmHiX",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vinita_Vasudevan1",
        "name": "Vinita Vasudevan",
        "name_site": null,
        "openreview_id": "~Vinita_Vasudevan1",
        "position": 2,
        "gender": null,
        "homepage_url": "https://www.ee.iitm.ac.in/~vinita",
        "dblp_id": "69/2594",
        "google_scholar_url": null,
        "orcid": "0000-0001-7039-3821",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.666666666666667,
        "rating_std": 1.247219128924647,
        "confidence_mean": 4.0,
        "confidence_std": 0.816496580927726,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VNjJAWjuEU",
      "title": "Graph of Circuits with GNN for Exploring the Optimal Design Space",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning  enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71460",
      "pdf_url": "https://openreview.net/pdf?id=VNjJAWjuEU",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sandeep_Kumar8",
        "name": "Sandeep Kumar",
        "name_site": null,
        "openreview_id": "~Sandeep_Kumar8",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/view/sandeepkr/home",
        "dblp_id": null,
        "google_scholar_url": "lycMMW8AAAAJ",
        "orcid": null,
        "linkedin_url": "sandeep-kumar-84463332/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.3564659966250536,
        "confidence_mean": 2.4,
        "confidence_std": 1.2000000000000002,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "XetXfkYZ6i",
      "title": "Deep Recurrent Optimal Stopping",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Deep neural networks (DNNs) have recently emerged as a powerful paradigm for solving Markovian optimal stopping problems. However, a ready extension of DNN-based methods to non-Markovian settings requires significant state and parameter space expansion, manifesting the curse of dimensionality. Further, efficient state-space transformations permitting Markovian approximations, such as those afforded by recurrent neural networks (RNNs), are either structurally infeasible or are confounded by the curse of non-Markovianity. Considering these issues, we introduce, for the first time, an optimal stopping policy gradient algorithm (OSPG) that can leverage RNNs effectively in non-Markovian settings by implicitly optimizing value functions without recursion, mitigating the curse of non-Markovianity. The OSPG algorithm is derived from an inference procedure on a novel Bayesian network representation of discrete-time non-Markovian optimal stopping trajectories and, as a consequence, yields an offline policy gradient algorithm that eliminates expensive Monte Carlo policy rollouts.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71344",
      "pdf_url": "https://openreview.net/pdf?id=XetXfkYZ6i",
      "github_url": "",
      "total_authors": 2,
      "track_name": "Main Conference",
      "author": {
        "id": "~Chiranjib_Bhattacharyya1",
        "name": "Chiranjib Bhattacharyya",
        "name_site": null,
        "openreview_id": "~Chiranjib_Bhattacharyya1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://www.csa.iisc.ac.in/~chiru/",
        "dblp_id": "b/CBhattacharyya",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 2.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "arkmhtYLL6",
      "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Humans use abstract *concepts* for understanding instead of hard features. Recent interpretability research has focused on human-centered concept explanations of neural networks. Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept. We extend CAVs from post-hoc analysis to ante-hoc training to reduce model bias through fine-tuning using an additional *Concept Loss*. Concepts are defined on the final layer of the network in the past. We generalize it to intermediate layers, including the last convolution layer. We also introduce *Concept Distillation*, a method to define rich and effective concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize a model towards concepts. We show applications of concept-sensitive training to debias several classification problems. We also show a way to induce prior knowledge into a reconstruction problem. We show that concept-sensitive training can improve model interpretability, reduce biases, and induce prior knowledge.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71179",
      "pdf_url": "https://openreview.net/pdf?id=arkmhtYLL6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~P_J_Narayanan1",
        "name": "P J Narayanan",
        "name_site": null,
        "openreview_id": "~P_J_Narayanan1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.iiit.ac.in/~pjn/",
        "dblp_id": "n/PJNarayanan",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=3HKjt_IAAAAJ",
        "orcid": "0000-0002-7164-4917",
        "linkedin_url": "pjnarayanan/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.5,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "hiQG8qGxso",
      "title": "Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of knowledge or domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE renders it practical to be used with any off-the-shelf trained model to improve its predictions further.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70773",
      "pdf_url": "https://openreview.net/pdf?id=hiQG8qGxso",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineet_Gandhi1",
        "name": "Vineet Gandhi",
        "name_site": null,
        "openreview_id": "~Vineet_Gandhi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://faculty.iiit.ac.in/~vgandhi/",
        "dblp_id": "117/2021",
        "google_scholar_url": "https://scholar.google.fr/citations?user=PVlBz8oAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.7071067811865476,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "lds9D17HRd",
      "title": "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Text-to-image diffusion models have made significant advances in generating and editing high-quality images.  As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects.  In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS.  We also show that these correspondences can enable interesting applications such as instance swapping in two images. Project page: https://sd-complements-dino.github.io/.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70585",
      "pdf_url": "https://openreview.net/pdf?id=lds9D17HRd",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Varun_Jampani1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://varunjampani.github.io/",
        "dblp_id": "124/2785",
        "google_scholar_url": "1Cv6Sf4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.6,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.8,
        "confidence_std": 1.16619037896906,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 165,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abir_De1",
        "name": "Abir De",
        "name_site": null,
        "openreview_id": "~Abir_De1",
        "position": 6,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "118/7174",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=_9ZKKbIAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rJc5Lsn5QU",
      "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Estimating 3D articulated shapes like animal bodies from monocular images is inherently challenging due to the ambiguities of camera viewpoint, pose, texture, lighting, etc. We propose ARTIC3D, a self-supervised framework to reconstruct per-instance 3D shapes from a sparse image collection in-the-wild. Specifically, ARTIC3D is built upon a skeleton-based surface representation and is further guided by 2D diffusion priors from Stable Diffusion. First, we enhance the input images with occlusions/truncation via 2D diffusion to obtain cleaner mask estimates and semantic features. Second, we perform diffusion-guided 3D optimization to estimate shape and texture that are of high-fidelity and faithful to input images. We also propose a novel technique to calculate more stable image-level gradients via diffusion models compared to existing alternatives. Finally, we produce realistic animations by fine-tuning the rendered shape and texture under rigid part transformations. Extensive evaluations on multiple existing datasets as well as newly introduced noisy web image collections with occlusions and truncation demonstrate that ARTIC3D outputs are more robust to noisy images, higher quality in terms of shape and texture details, and more realistic when animated.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70317",
      "pdf_url": "https://openreview.net/pdf?id=rJc5Lsn5QU",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Venkatesh_Babu_Radhakrishnan2",
        "name": "Venkatesh Babu Radhakrishnan",
        "name_site": null,
        "openreview_id": "~Varun_Jampani1",
        "position": 7,
        "gender": "M",
        "homepage_url": "https://varunjampani.github.io/",
        "dblp_id": "124/2785",
        "google_scholar_url": "1Cv6Sf4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 0.5,
        "confidence_mean": 4.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 23,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vineeth_N._Balasubramanian2",
        "name": "Vineeth N. Balasubramanian",
        "name_site": null,
        "openreview_id": "~Vineeth_Balasubramanian1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/vineethnb/",
        "dblp_id": "88/4691",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=7soDcboAAAAJ",
        "orcid": "0000-0003-2656-0375",
        "linkedin_url": "vineethnb?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yGLokEhdh9",
      "title": "Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of \\textit{virtual stochastic gradient descent (SGD) parameter} computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40\\% compared to the maximally interfered samples retrieval algorithm.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/69957",
      "pdf_url": "https://openreview.net/pdf?id=yGLokEhdh9",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Bheemarjuna_Tamma1",
        "name": "Bheemarjuna Tamma",
        "name_site": null,
        "openreview_id": "~Sumohana_S_Channappayya1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.iith.ac.in/~sumohana",
        "dblp_id": "49/904",
        "google_scholar_url": "_VCOXFwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 10.0,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "uJ3qNIsDGF",
      "title": "Exploring Geometry of Blind Spots in Vision models",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of ‚Äúequi-confidence‚Äù level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70160",
      "pdf_url": "https://openreview.net/pdf?id=uJ3qNIsDGF",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurang_Sriramanan1",
        "name": "Gaurang Sriramanan",
        "name_site": "Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, Venkatesh Babu R",
        "openreview_id": "~Gaurang_Sriramanan1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://gaurangsriramanan.github.io/",
        "dblp_id": "262/3916",
        "google_scholar_url": "t76Uk8oAAAAJ",
        "orcid": null,
        "linkedin_url": "gaurang-sriramanan-16141a1a3/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Maryland (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 5.000000000000001,
      "reviews": {
        "rating_mean": 6.6,
        "rating_std": 1.0198039027185568,
        "confidence_mean": 3.4,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "0H5fRQcpQ7",
      "title": "RoboHive: A Unified Framework for Robot Learning",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We present RoboHive, a comprehensive software platform and ecosystem for research in the field of Robot Learning and Embodied Artificial Intelligence. Our platform encompasses a diverse range of pre-existing and novel environments, including dexterous manipulation with the Shadow Hand, whole-arm manipulation tasks with Franka and Fetch robots, quadruped locomotion, among others. Included environments are organized within and cover multiple domains such as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc. In comparison to prior works, RoboHive offers a streamlined and unified task interface taking dependency on only a minimal set of well-maintained packages, features tasks with high physics fidelity and rich visual diversity, and supports common hardware drivers for real-world deployment. The unified interface of RoboHive offers a convenient and accessible abstraction for algorithmic research in imitation, reinforcement, multi-task, and hierarchical learning. Furthermore, RoboHive includes expert demonstrations and baseline results for most environments, providing a standard for benchmarking and comparisons. Details: https://sites.google.com/view/robohive",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73725",
      "pdf_url": "https://openreview.net/pdf?id=0H5fRQcpQ7",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Rutav_Shah1",
        "name": "Rutav Shah",
        "name_site": null,
        "openreview_id": "~Rutav_Shah1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://shahrutav.github.io",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "rutav-shah-01a2941a7",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Texas at Austin (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 1.118033988749895,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 19,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CjVdXey4zT",
      "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \\emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73658",
      "pdf_url": "https://openreview.net/pdf?id=CjVdXey4zT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sujay_Khandagale1",
        "name": "Sujay Khandagale",
        "name_site": null,
        "openreview_id": "~Sujay_Khandagale1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://suj97.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.fi/citations?user=7fwPm3wAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 6.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 176,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "Iq0DvhB4Kf",
      "title": "Emergent and Predictable Memorization in Large Language Models",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization in the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72096",
      "pdf_url": "https://openreview.net/pdf?id=Iq0DvhB4Kf",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~USVSN_Sai_Prashanth1",
        "name": "USVSN Sai Prashanth",
        "name_site": null,
        "openreview_id": "~USVSN_Sai_Prashanth1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "hh_x9HgAAAAJ",
        "orcid": null,
        "linkedin_url": "usvsnsp",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Matrusri Engineering College (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.166666666666667,
      "reviews": {
        "rating_mean": 6.2,
        "rating_std": 0.9797958971132712,
        "confidence_mean": 3.8,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 175,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Singh3",
        "name": "Gaurav Singh",
        "name_site": null,
        "openreview_id": "~Gaurav_Singh3",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://vanhalen42.github.io/",
        "dblp_id": null,
        "google_scholar_url": "kQCkrnwAAAAJ",
        "orcid": null,
        "linkedin_url": "gaurav-singh-448363207/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Tushar_Nandy1",
        "name": "Tushar Nandy",
        "name_site": null,
        "openreview_id": "~Tushar_Nandy1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "tushar-nandy/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.0,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vimal_K_B1",
        "name": "Vimal K B",
        "name_site": null,
        "openreview_id": "~Vimal_K_B1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "vimalkb07/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 4.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rUf0GV5CuU",
      "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by  embedded representations, thereby extending set containment to *soft* set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise *vector* *dominance*. This 0/1 property can be relaxed to an asymmetric *hinge* *distance* for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few  simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed *dominance* *similarity* measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70312",
      "pdf_url": "https://openreview.net/pdf?id=rUf0GV5CuU",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soumen_Chakrabarti1",
        "name": "Soumen Chakrabarti",
        "name_site": null,
        "openreview_id": "~Soumen_Chakrabarti1",
        "position": 3,
        "gender": "Not Specified",
        "homepage_url": "https://www.cse.iitb.ac.in/~soumen/",
        "dblp_id": "c/SChakrabarti",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=LfF2zfQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "tScBQRNgjk",
      "title": "ForecastPFN: Synthetically-Trained Zero-Shot Forecasting",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70200",
      "pdf_url": "https://openreview.net/pdf?id=tScBQRNgjk",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gurnoor_Singh_Khurana1",
        "name": "Gurnoor Singh Khurana",
        "name_site": null,
        "openreview_id": "~Gurnoor_Singh_Khurana1",
        "position": 2,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "a4aHHUIAAAAJ",
        "orcid": null,
        "linkedin_url": "gurnoor-singh-48b1b7191",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.75,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 73,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CjVdXey4zT",
      "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \\emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73658",
      "pdf_url": "https://openreview.net/pdf?id=CjVdXey4zT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Jonathan_Valverde1",
        "name": "Jonathan Valverde",
        "name_site": null,
        "openreview_id": "~Jonathan_Valverde1",
        "position": 3,
        "gender": null,
        "homepage_url": "https://jonathan-valverde-l.github.io/",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Bombay (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 176,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "D8nAMRRCLS",
      "title": "On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "As large-scale training regimes have gained popularity, the use of pretrained models for downstream tasks has become common practice in machine learning. While pretraining has been shown to enhance the performance of models in practice, the transfer of robustness properties from pretraining to downstream tasks remains poorly understood. In this study, we demonstrate that the robustness of a linear predictor on downstream tasks can be constrained by the robustness of its underlying representation, regardless of the protocol used for pretraining. We prove (i) a bound on the loss that holds independent of any downstream task, as well as (ii) a criterion for robust classification in particular. We validate our theoretical results in practical applications, show how our results can be used for calibrating expectations of downstream robustness, and when our results are useful for optimal transfer learning. Taken together, our results offer an initial step towards characterizing the requirements of the representation function for reliable post-adaptation performance.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72393",
      "pdf_url": "https://openreview.net/pdf?id=D8nAMRRCLS",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harsh_Raj1",
        "name": "Harsh Raj",
        "name_site": null,
        "openreview_id": "~Harsh_Raj1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://harshraj172.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com/citations?view_op=list_works",
        "orcid": null,
        "linkedin_url": "harsh-raj-425593195/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Delhi Technological University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 3.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "EjG2G1PT2v",
      "title": "Information-guided Planning: An Online Approach for Partially Observable Problems",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "This paper presents IB-POMCP, a novel algorithm for online planning under partial observability. Our approach enhances the decision-making process by using estimations of the world belief's entropy to guide a tree search process and surpass the limitations of planning in scenarios with sparse reward configurations. By performing what we denominate as an *information-guided planning process*, the algorithm, which incorporates a novel I-UCB function, shows significant improvements in reward and reasoning time compared to state-of-the-art baselines in several benchmark scenarios, along with theoretical convergence guarantees.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72300",
      "pdf_url": "https://openreview.net/pdf?id=EjG2G1PT2v",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Amokh_Varma1",
        "name": "Amokh Varma",
        "name_site": null,
        "openreview_id": "~Amokh_Varma1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://amokhvarma.github.io",
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.299038105676658,
        "confidence_mean": 3.0,
        "confidence_std": 1.5811388300841898,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OXOLiS0ak6",
      "title": "A Dataset for Analyzing Streaming Media Performance over HTTP/3 Browsers",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "HTTP/3 is a new application layer protocol supported by most browsers. It uses QUIC as an underlying transport protocol. QUIC provides multiple benefits, like faster connection establishment, reduced latency, and improved connection migration. Hence, most popular browsers like Chrome/Chromium, Microsoft Edge, Apple Safari, and Mozilla Firefox have started supporting it. In this paper, we present an HTTP/3-supported browser dataset collection tool named H3B. It collects the application and network-level logs during YouTube streaming. We consider YouTube, as it  the most popular video streaming application supporting QUIC. Using this tool, we collected a dataset of over 5936 YouTube sessions covering 5464 hours of streaming over 5 different geographical locations and 5 different bandwidth patterns. We believe our tool and as well as the dataset could be used in multiple applications such as a better configuration of application/transport protocols based on the network conditions, intelligent integration of network and application, predicting YouTube's QoE etc. \nWe analyze the dataset and observe that during an HTTP/3 streaming not all requests are served by HTTP/3. Instead whenever the network condition is not favorable the browser chooses to fallback, and the application requests are transmitted using HTTP/2 over the old-standing transport protocol TCP. We observe that such switching of protocols impacts the performance of video streaming applications.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73606",
      "pdf_url": "https://openreview.net/pdf?id=OXOLiS0ak6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Mukulika_Maity1",
        "name": "Mukulika Maity",
        "name_site": null,
        "openreview_id": "~Mukulika_Maity1",
        "position": 2,
        "gender": "F",
        "homepage_url": "https://www.iiitd.ac.in/mukulika",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=F5sooVMAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indraprastha Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 1.7204650534085253,
        "confidence_mean": 3.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PTvxck0QDE",
      "title": "Simplicity Bias in 1-Hidden Layer Neural Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Recent works have demonstrated that neural networks exhibit extreme *simplicity bias* (SB). That is,  they learn *only the simplest* features  to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of *features*, these works showcase SB on *semi-synthetic* datasets such as Color-MNIST , MNIST-CIFAR where\n defining features is relatively easier. \n\nIn this work, we rigorously define as well as thoroughly establish SB for *one hidden layer* neural networks in the infinite width regime. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs \n(ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable ($1$-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier,  (iii) empirically, we show that models trained on *real* datasets such as Imagenet and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in  models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71765",
      "pdf_url": "https://openreview.net/pdf?id=PTvxck0QDE",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~jatin_batra1",
        "name": "jatin batra",
        "name_site": null,
        "openreview_id": "~jatin_batra1",
        "position": 2,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "157/6041",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tata Institute of Fundamental Research (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.75,
        "rating_std": 0.4330127018922193,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 15,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VNjJAWjuEU",
      "title": "Graph of Circuits with GNN for Exploring the Optimal Design Space",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning  enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71460",
      "pdf_url": "https://openreview.net/pdf?id=VNjJAWjuEU",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saripilli_Venkata_Swapna_Manjiri1",
        "name": "Saripilli Venkata Swapna Manjiri",
        "name_site": null,
        "openreview_id": "~Saripilli_Venkata_Swapna_Manjiri1",
        "position": 2,
        "gender": "F",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "swapna-manjiri-saripilli-053277184/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.3564659966250536,
        "confidence_mean": 2.4,
        "confidence_std": 1.2000000000000002,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "X4mmXQ4Nxw",
      "title": "Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "One of the most impactful findings in computational neuroscience over the past decade is that the object recognition accuracy of deep neural networks (DNNs) correlates with their ability to predict neural responses to natural images in the inferotemporal (IT) cortex. This discovery supported the long-held theory that object recognition is a core objective of the visual cortex, and suggested that more accurate DNNs would serve as better models of IT neuron responses to images. Since then, deep learning has undergone a revolution of scale: billion parameter-scale DNNs trained on billions of images are rivaling or outperforming humans at visual tasks including object recognition. Have today's DNNs become more accurate at predicting IT neuron responses to images as they have grown more accurate at object recognition?\n\nSurprisingly, across three independent experiments, we find that this is not the case. DNNs have become progressively worse models of IT as their accuracy has increased on ImageNet. To understand why DNNs experience this trade-off and evaluate if they are still an appropriate paradigm for modeling the visual system, we turn to recordings of IT that capture spatially resolved maps of neuronal activity elicited by natural images. These neuronal activity maps reveal that DNNs trained on ImageNet learn to rely on different visual features than those encoded by IT and that this problem worsens as their accuracy increases. We successfully resolved this issue with the neural harmonizer, a plug-and-play training routine for DNNs that aligns their learned representations with humans. Our results suggest that harmonized DNNs break the trade-off between ImageNet accuracy and neural prediction accuracy that assails current DNNs and offer a path to more accurate models of biological vision. Our work indicates that the standard approach for modeling IT with task-optimized DNNs needs revision, and other biological constraints, including human psychophysics data, are needed to accurately reverse-engineer the visual cortex.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71381",
      "pdf_url": "https://openreview.net/pdf?id=X4mmXQ4Nxw",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.5,
        "rating_std": 0.5,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 26,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "guyhQMSp2F",
      "title": "Use perturbations when learning from explanations",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Machine learning from explanations (MLX) is an approach to learning that uses human-provided explanations of relevant or irrelevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely on local model interpretation methods and require strong model smoothing to align model and human explanations, leading to sub-optimal performance. We recast MLX as a robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong model smoothing. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we show how to combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70819",
      "pdf_url": "https://openreview.net/pdf?id=guyhQMSp2F",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vihari_Piratla1",
        "name": "Vihari Piratla",
        "name_site": "Vihari Piratla, Soumen Chakrabarti, Sunita Sarawagi",
        "openreview_id": "~Vihari_Piratla1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://vihari.github.io/",
        "dblp_id": "161/3626",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=DQddccYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Cambridge (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 3.333333333333334,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.0,
        "confidence_mean": 3.0,
        "confidence_std": 1.224744871391589,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 1,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Agarwal1",
        "name": "Aditya Agarwal",
        "name_site": null,
        "openreview_id": "~Aditya_Agarwal1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://skymanaditya1.github.io/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=64Cgbv4AAAAJ",
        "orcid": null,
        "linkedin_url": "skymanaditya1/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "e2wtjx0Yqu",
      "title": "CLadder: Assessing Causal Reasoning in Language Models",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating _commonsense_ causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined _formal rules_. To address this, we propose a new NLP task, _causal inference in natural language_, inspired by the _\"causal inference engine\"_ postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70983",
      "pdf_url": "https://openreview.net/pdf?id=e2wtjx0Yqu",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ojasv_Kamal1",
        "name": "Ojasv Kamal",
        "name_site": null,
        "openreview_id": "~Ojasv_Kamal1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": "34PgtWEAAAAJ",
        "orcid": null,
        "linkedin_url": "ojasv-kamal-996397182/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Hong Kong (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 126,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Gaurav_Aggarwal4",
        "name": "Gaurav Aggarwal",
        "name_site": null,
        "openreview_id": "~Gaurav_Aggarwal4",
        "position": 3,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "14/5218",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=9XiIwDQAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Linga_Reddy_Cenkeramaddi1",
        "name": "Linga Reddy Cenkeramaddi",
        "name_site": null,
        "openreview_id": "~Linga_Reddy_Cenkeramaddi1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://www.uia.no/kk/profil/lingac",
        "dblp_id": "08/5277",
        "google_scholar_url": "n5BfOZYAAAAJ",
        "orcid": "0000-0002-1023-2118",
        "linkedin_url": "iitlingareddy/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Agder (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 3.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "MbwVNEx9KS",
      "title": "Energy Transformer",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71901",
      "pdf_url": "https://openreview.net/pdf?id=MbwVNEx9KS",
      "github_url": "",
      "total_authors": 8,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rameswar_Panda1",
        "name": "Rameswar Panda",
        "name_site": null,
        "openreview_id": "~Rameswar_Panda1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://rpand002.github.io/",
        "dblp_id": "126/0986",
        "google_scholar_url": "_ySuu6gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.8571428571428568,
      "reviews": {
        "rating_mean": 6.25,
        "rating_std": 1.479019945774904,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 60,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "UBbm5embIB",
      "title": "Learning Human Action Recognition Representations Without Real Humans",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the {\\em transferability} of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with {\\em humans removed} and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. \nOur approach outperforms previous baselines by up to 5\\% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73581",
      "pdf_url": "https://openreview.net/pdf?id=UBbm5embIB",
      "github_url": "",
      "total_authors": 10,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Rameswar_Panda1",
        "name": "Rameswar Panda",
        "name_site": null,
        "openreview_id": "~Rameswar_Panda1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://rpand002.github.io/",
        "dblp_id": "126/0986",
        "google_scholar_url": "_ySuu6gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Massachusetts Institute of Technology (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.7777777777777777,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 1.224744871391589,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 5,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ARrwf7Ev2T",
      "title": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "Vision and Language (VL) models offer an effective method for aligning representation spaces of images and text allowing for numerous applications such as cross-modal retrieval, visual and multi-hop question answering, captioning, and many more. However, the aligned image-text spaces learned by all the popular VL models are still suffering from the so-called 'object bias' - their representations behave as 'bags of nouns' mostly ignoring or downsizing the attributes, relations, and states of objects described/appearing in texts/images. Although some great attempts at fixing these `compositional reasoning' issues were proposed in the recent literature, the problem is still far from being solved. In this paper, we uncover two factors limiting the VL models' compositional reasoning performance. These two factors are properties of the paired VL dataset used for finetuning (or pre-training) the VL model: (i) the caption quality, or in other words 'image-alignment', of the texts; and (ii) the 'density' of the captions in the sense of mentioning all the details appearing on the image. We propose a fine-tuning approach for automatically treating these factors on a standard collection of paired VL data (CC3M). Applied to CLIP, we demonstrate its significant compositional reasoning performance increase of up to $\\sim27$\\% over the base model, up to $\\sim20$\\% over the strongest baseline, and by $6.7$\\% on average. Our code is provided in the Supplementary and would be released upon acceptance.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72540",
      "pdf_url": "https://openreview.net/pdf?id=ARrwf7Ev2T",
      "github_url": "",
      "total_authors": 12,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rameswar_Panda1",
        "name": "Rameswar Panda",
        "name_site": null,
        "openreview_id": "~Rameswar_Panda1",
        "position": 8,
        "gender": "M",
        "homepage_url": "https://rpand002.github.io/",
        "dblp_id": "126/0986",
        "google_scholar_url": "_ySuu6gAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Tel Aviv University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.7272727272727275,
      "reviews": {
        "rating_mean": 6.4,
        "rating_std": 1.0198039027185568,
        "confidence_mean": 4.2,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 50,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "7cnMLZvTy9",
      "title": "Certification of Distributional Individual Fairness",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \\textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72696",
      "pdf_url": "https://openreview.net/pdf?id=7cnMLZvTy9",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Vihari_Piratla1",
        "name": "Vihari Piratla",
        "name_site": "Vihari Piratla, Soumen Chakrabarti, Sunita Sarawagi",
        "openreview_id": "~Vihari_Piratla1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://vihari.github.io/",
        "dblp_id": "161/3626",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=DQddccYAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Cambridge (United Kingdom)",
        "countries": [
          "United Kingdom"
        ],
        "country_codes": [
          "GB"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.3333333333333335,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "ByDy2mlkig",
      "title": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable AI techniques for generating saliency maps, and counterfactual explanations. However, saliency maps generated by traditional neural networks are often noisy and provide limited insights. In this paper, we demonstrate that, on the contrary, the saliency maps of 1-Lipschitz neural networks, learnt with the dual loss of an optimal transportation problem, exhibit desirable XAI properties:\nThey are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. We also prove that these maps align unprecedentedly well with human explanations on ImageNet. To explain the particularly beneficial properties of the saliency map for such models, we prove this gradient encodes  both the direction of the transportation plan and the direction towards the nearest adversarial attack. Following the gradient down to the decision boundary is no longer considered an adversarial attack, but rather a counterfactual explanation that explicitly transports the input from one class to another.  Thus, Learning with such a loss jointly optimizes the classification objective and the alignment of the gradient , i.e. the saliency map, to the transportation plan direction. These networks were previously known to be certifiably robust by design, and we demonstrate that they scale well for large problems and models, and are tailored for explainability using a fast and straightforward method.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72454",
      "pdf_url": "https://openreview.net/pdf?id=ByDy2mlkig",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Brown University (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 5.6,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.8,
        "confidence_std": 0.39999999999999997,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 10,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CjVdXey4zT",
      "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \\emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73658",
      "pdf_url": "https://openreview.net/pdf?id=CjVdXey4zT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Vishak_Prasad_C1",
        "name": "Vishak Prasad C",
        "name_site": null,
        "openreview_id": "~Vishak_Prasad_C1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "279/3692.html",
        "google_scholar_url": "OVvf2HQAAAAJ",
        "orcid": null,
        "linkedin_url": "vishak-prasad-777891a3/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "New York University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 6.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 176,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "arkmhtYLL6",
      "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Humans use abstract *concepts* for understanding instead of hard features. Recent interpretability research has focused on human-centered concept explanations of neural networks. Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept. We extend CAVs from post-hoc analysis to ante-hoc training to reduce model bias through fine-tuning using an additional *Concept Loss*. Concepts are defined on the final layer of the network in the past. We generalize it to intermediate layers, including the last convolution layer. We also introduce *Concept Distillation*, a method to define rich and effective concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize a model towards concepts. We show applications of concept-sensitive training to debias several classification problems. We also show a way to induce prior knowledge into a reconstruction problem. We show that concept-sensitive training can improve model interpretability, reduce biases, and induce prior knowledge.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71179",
      "pdf_url": "https://openreview.net/pdf?id=arkmhtYLL6",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Saurabh_Saini1",
        "name": "Saurabh Saini",
        "name_site": null,
        "openreview_id": "~Saurabh_Saini1",
        "position": 2,
        "gender": "M",
        "homepage_url": "http://researchweb.iiit.ac.in/~saurabh.saini",
        "dblp_id": "184/8348",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=OSZDITwAAAAJ",
        "orcid": "0000-0002-8274-2379",
        "linkedin_url": "saurabh0saini/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 5.5,
        "rating_std": 1.5,
        "confidence_mean": 3.25,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 6,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "jooPcatnVF",
      "title": "Implicit Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Deep network models are often purely inductive during both training and inference on unseen data. When these models are used for prediction, but they may fail to capture important semantic information and implicit dependencies within datasets. Recent advancements have shown that combining multiple modalities in large-scale vision and language settings can improve understanding and generalization performance. However, as the model size increases, fine-tuning and deployment become computationally expensive, even for a small number of downstream tasks. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. To address these challenges, we propose a simplified alternative of combining features from pretrained deep networks and freely available semantic explicit knowledge. In order to remove irrelevant explicit knowledge that does not correspond well to the images, we introduce an implicit Differentiable Out-of-Distribution (OOD) detection layer. This layer addresses outlier detection by solving for fixed points of a differentiable function and using the last iterate of fixed point solver to backpropagate. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to design models that perform similarly to state-of-the-art results but with significantly fewer samples and less training time. Our models and code are available here: https://github.com/ellenzhuwang/implicit_vkood",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70674",
      "pdf_url": "https://openreview.net/pdf?id=jooPcatnVF",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sourav_Medya1",
        "name": "Sourav Medya",
        "name_site": null,
        "openreview_id": "~Sourav_Medya1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://souravmedya.github.io/",
        "dblp_id": "178/3021",
        "google_scholar_url": "RCFhOM4AAAAJ",
        "orcid": "0000-0003-0996-2807",
        "linkedin_url": "sourav-medya-35987a49/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of Illinois at Chicago (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 5.0,
        "rating_std": 0.6324555320336759,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 3,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "yGLokEhdh9",
      "title": "Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of \\textit{virtual stochastic gradient descent (SGD) parameter} computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40\\% compared to the maximally interfered samples retrieval algorithm.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/69957",
      "pdf_url": "https://openreview.net/pdf?id=yGLokEhdh9",
      "github_url": "",
      "total_authors": 3,
      "track_name": "Main Conference",
      "author": {
        "id": "~Sumohana_S._Channappayya1",
        "name": "Sumohana S. Channappayya",
        "name_site": null,
        "openreview_id": "~Bheemarjuna_Tamma1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://people.iith.ac.in/tbr/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=FYHCD2kAAAAJ",
        "orcid": "0000-0002-4056-7963",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.5,
      "reviews": {
        "rating_mean": 5.333333333333333,
        "rating_std": 0.9428090415820634,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 1.247219128924647,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 12,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "kaHpo8OZw2",
      "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
      "status": "Oral",
      "normalized_status": "oral",
      "abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance ‚Äì where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives ‚Äì including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73486",
      "pdf_url": "https://openreview.net/pdf?id=kaHpo8OZw2",
      "github_url": "",
      "total_authors": 19,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Zinan_Lin1",
        "name": "Zinan Lin",
        "name_site": "Zinan Lin, Vyas Sekar, Giulia Fanti",
        "openreview_id": "~Zinan_Lin1",
        "position": 15,
        "gender": "M",
        "homepage_url": "https://zinanlin.me/",
        "dblp_id": "64/237-1",
        "google_scholar_url": "67nE-wQ_g_cC",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Carnegie Mellon University (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.2222222222222223,
      "reviews": {
        "rating_mean": 7.6,
        "rating_std": 1.2,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 473,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rohith_Agaram1",
        "name": "Rohith Agaram",
        "name_site": null,
        "openreview_id": "~Rohith_Agaram1",
        "position": 4,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": "335/1286",
        "google_scholar_url": "Ni6qG7wAAAAJ",
        "orcid": null,
        "linkedin_url": "rohith-agaram-812278132/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology, Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ashish_V._Tendulkar1",
        "name": "Ashish V. Tendulkar",
        "name_site": null,
        "openreview_id": "~Ashish_V._Tendulkar1",
        "position": 4,
        "gender": null,
        "homepage_url": null,
        "dblp_id": "08/1521",
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Google (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~C_Krishna_Mohan1",
        "name": "C Krishna Mohan",
        "name_site": null,
        "openreview_id": "~C_Krishna_Mohan1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://www.iith.ac.in/~ckm/",
        "dblp_id": "30/4639",
        "google_scholar_url": "https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-7316-0836",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 2.0,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "rUf0GV5CuU",
      "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
      "status": "Spotlight",
      "normalized_status": "spotlight",
      "abstract": "In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by  embedded representations, thereby extending set containment to *soft* set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise *vector* *dominance*. This 0/1 property can be relaxed to an asymmetric *hinge* *distance* for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few  simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed *dominance* *similarity* measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70312",
      "pdf_url": "https://openreview.net/pdf?id=rUf0GV5CuU",
      "github_url": "",
      "total_authors": 5,
      "track_name": "Main Conference",
      "author": {
        "id": "~Anirban_Dasgupta1",
        "name": "Anirban Dasgupta",
        "name_site": null,
        "openreview_id": "~Anirban_Dasgupta1",
        "position": 4,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/anirbandasgupta",
        "dblp_id": "54/385-1",
        "google_scholar_url": "plJC8R0AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Gandhinagar (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.875,
      "reviews": {
        "rating_mean": 6.333333333333333,
        "rating_std": 0.4714045207910317,
        "confidence_mean": 3.6666666666666665,
        "confidence_std": 0.4714045207910317,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 4,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A6X9y8n4sT",
      "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72556",
      "pdf_url": "https://openreview.net/pdf?id=A6X9y8n4sT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Mukund_Varma_T1",
        "name": "Mukund Varma T",
        "name_site": "Mukund Varma T, Xuxi Chen, Zhenyu Zhang, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang",
        "openreview_id": "~Mukund_Varma_T1",
        "position": 5,
        "gender": "M",
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": "mukundvarmat/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Madras (China)",
        "countries": [
          "China"
        ],
        "country_codes": [
          "CN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 5.2,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 4.4,
        "confidence_std": 0.48989794855663565,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 412,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "CjVdXey4zT",
      "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \\emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73658",
      "pdf_url": "https://openreview.net/pdf?id=CjVdXey4zT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Ganesh_Ramakrishnan1",
        "name": "Ganesh Ramakrishnan",
        "name_site": null,
        "openreview_id": "~Ganesh_Ramakrishnan1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.cse.iitb.ac.in/~ganesh/",
        "dblp_id": "r/GaneshRamakrishnan",
        "google_scholar_url": "https://scholar.google.com/scholar?hl=hi",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Abacus.AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 6.8,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 3.6,
        "confidence_std": 0.4898979485566356,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 176,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "OXOLiS0ak6",
      "title": "A Dataset for Analyzing Streaming Media Performance over HTTP/3 Browsers",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "HTTP/3 is a new application layer protocol supported by most browsers. It uses QUIC as an underlying transport protocol. QUIC provides multiple benefits, like faster connection establishment, reduced latency, and improved connection migration. Hence, most popular browsers like Chrome/Chromium, Microsoft Edge, Apple Safari, and Mozilla Firefox have started supporting it. In this paper, we present an HTTP/3-supported browser dataset collection tool named H3B. It collects the application and network-level logs during YouTube streaming. We consider YouTube, as it  the most popular video streaming application supporting QUIC. Using this tool, we collected a dataset of over 5936 YouTube sessions covering 5464 hours of streaming over 5 different geographical locations and 5 different bandwidth patterns. We believe our tool and as well as the dataset could be used in multiple applications such as a better configuration of application/transport protocols based on the network conditions, intelligent integration of network and application, predicting YouTube's QoE etc. \nWe analyze the dataset and observe that during an HTTP/3 streaming not all requests are served by HTTP/3. Instead whenever the network condition is not favorable the browser chooses to fallback, and the application requests are transmitted using HTTP/2 over the old-standing transport protocol TCP. We observe that such switching of protocols impacts the performance of video streaming applications.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73606",
      "pdf_url": "https://openreview.net/pdf?id=OXOLiS0ak6",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Sandip_Chakraborty1",
        "name": "Sandip Chakraborty",
        "name_site": null,
        "openreview_id": "~Sandip_Chakraborty1",
        "position": 3,
        "gender": "M",
        "homepage_url": "http://cse.iitkgp.ac.in/~sandipc/",
        "dblp_id": "28/9571",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=dEpbTokAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Kharagpur (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 5.8,
        "rating_std": 1.7204650534085253,
        "confidence_mean": 3.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "VNjJAWjuEU",
      "title": "Graph of Circuits with GNN for Exploring the Optimal Design Space",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning  enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/71460",
      "pdf_url": "https://openreview.net/pdf?id=VNjJAWjuEU",
      "github_url": "",
      "total_authors": 4,
      "track_name": "Main Conference",
      "author": {
        "id": "~Ankesh_Jain1",
        "name": "Ankesh Jain",
        "name_site": null,
        "openreview_id": "~Ankesh_Jain1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://web.iitd.ac.in/~ankesh/",
        "dblp_id": "16/10352",
        "google_scholar_url": "https://scholar.google.co.in/citations?user=BP9KOTkAAAAJ",
        "orcid": "0000-0003-4109-6312",
        "linkedin_url": "ankesh-jain-09745614/?originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Delhi (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 1.666666666666667,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 1.3564659966250536,
        "confidence_mean": 2.4,
        "confidence_std": 1.2000000000000002,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 8,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "BExDjNDYkN",
      "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72497",
      "pdf_url": "https://openreview.net/pdf?id=BExDjNDYkN",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Madhava_Krishna2",
        "name": "Madhava Krishna",
        "name_site": null,
        "openreview_id": "~K._Krishna1",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://robotics.iiit.ac.in/",
        "dblp_id": "90/4844",
        "google_scholar_url": "https://scholar.google.com.tw/citations?user=QDuPGHwAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "International Institute of Information Technology (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": 5.25,
        "rating_std": 1.0897247358851685,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 13,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "q3fCWoC9l0",
      "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization.  \nOur experiments show that our model outperforms several methods across several real datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70380",
      "pdf_url": "https://openreview.net/pdf?id=q3fCWoC9l0",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Rishabh_K_Iyer2",
        "name": "Rishabh K Iyer",
        "name_site": null,
        "openreview_id": "~Rishabh_K_Iyer2",
        "position": 5,
        "gender": "M",
        "homepage_url": "https://www.rishiyer.com",
        "dblp_id": "37/10544.html",
        "google_scholar_url": "l_XxJ1kAAAAJ",
        "orcid": null,
        "linkedin_url": "rishabh-iyer-36893717/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Microsoft (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": 6.166666666666667,
        "rating_std": 1.3437096247164249,
        "confidence_mean": 2.8333333333333335,
        "confidence_std": 0.6871842709362768,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 7,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "u6BYyPuD29",
      "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\\mathcal{H}\\Delta\\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/70169",
      "pdf_url": "https://openreview.net/pdf?id=u6BYyPuD29",
      "github_url": "",
      "total_authors": 6,
      "track_name": "Main Conference",
      "author": {
        "id": "~Abhinav_Kumar4",
        "name": "Abhinav Kumar",
        "name_site": null,
        "openreview_id": "~Abhinav_Kumar4",
        "position": 5,
        "gender": null,
        "homepage_url": "https://people.iith.ac.in/abhinavkumar/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?user=kVHpPOcAAAAJ",
        "orcid": "0000-0002-6468-7054",
        "linkedin_url": "abhinav-kumar-91921916/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Indian Institute of Technology Hyderabad (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.9999999999999998,
      "reviews": {
        "rating_mean": 5.4,
        "rating_std": 0.48989794855663565,
        "confidence_mean": 3.0,
        "confidence_std": 0.6324555320336759,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 34,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "A6X9y8n4sT",
      "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/72556",
      "pdf_url": "https://openreview.net/pdf?id=A6X9y8n4sT",
      "github_url": "",
      "total_authors": 7,
      "track_name": "Main Conference",
      "author": {
        "id": "~Zexiang_Xu1",
        "name": "Zexiang Xu",
        "name_site": null,
        "openreview_id": "~Zexiang_Xu1",
        "position": 6,
        "gender": "M",
        "homepage_url": "https://cseweb.ucsd.edu/~zex014/",
        "dblp_id": "154/0366",
        "google_scholar_url": "_RRIYvEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Adobe (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.8333333333333331,
      "reviews": {
        "rating_mean": 5.2,
        "rating_std": 0.7483314773547882,
        "confidence_mean": 4.4,
        "confidence_std": 0.48989794855663565,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 412,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "pRnrg2bWr0",
      "title": "OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects",
      "status": "Poster",
      "normalized_status": "poster",
      "abstract": "We introduce OpenIllumination, a real-world dataset containing over 108K images of 64 objects with diverse materials, captured under 72 camera views and a large number of different illuminations. For each image in the dataset, we provide accurate camera parameters, illumination ground truth, and foreground segmentation masks. Our dataset enables the quantitative evaluation of most inverse rendering and material decomposition methods for real objects. We examine several state-of-the-art inverse rendering methods on our dataset and compare their performances. The dataset and code can be found on the project page: https://oppo-us-research.github.io/OpenIllumination.",
      "tldr": "",
      "site_url": "https://nips.cc/virtual/2023/poster/73468",
      "pdf_url": "https://openreview.net/pdf?id=pRnrg2bWr0",
      "github_url": "",
      "total_authors": 11,
      "track_name": "Datasets & Benchmarks",
      "author": {
        "id": "~Zexiang_Xu1",
        "name": "Zexiang Xu",
        "name_site": null,
        "openreview_id": "~Zexiang_Xu1",
        "position": 10,
        "gender": "M",
        "homepage_url": "https://cseweb.ucsd.edu/~zex014/",
        "dblp_id": "154/0366",
        "google_scholar_url": "_RRIYvEAAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "University of California, San Diego (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.4999999999999999,
      "reviews": {
        "rating_mean": 6.0,
        "rating_std": 0.816496580927726,
        "confidence_mean": 4.0,
        "confidence_std": 0.0,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 20,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2MRz5bSnan",
      "title": "Permutation Decision Trees using Structural Impurity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are \\emph{Shannon entropy} and \\emph{Gini impurity}. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to a permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies.  In this work, we use~\\emph{Effort-To-Compress} (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutation of the same data instances (\\emph{Permutation Decision Trees}). We then introduce the notion of {\\it Permutation Bagging} achieved using permutation decision trees without the need for random feature selection and sub-sampling.  We compare the performance of the proposed permutation bagged decision trees with Random Forest. Our model does not assume independent and identical distribution of data instances. Potential applications include scenarios where a temporal order is present in the data instances.  ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=2MRz5bSnan",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Harikrishnan_N_B1",
        "name": "Harikrishnan N B",
        "name_site": "Harikrishnan N B, Aditi Kathpalia, Nithin Nagaraj",
        "openreview_id": "~Harikrishnan_N_B1",
        "position": 1,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/harikrishnannb8/home",
        "dblp_id": null,
        "google_scholar_url": "9fMmKMEAAAAJ",
        "orcid": "0000-0002-4575-3968",
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science, Pilani (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "2MRz5bSnan",
      "title": "Permutation Decision Trees using Structural Impurity",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are \\emph{Shannon entropy} and \\emph{Gini impurity}. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to a permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies.  In this work, we use~\\emph{Effort-To-Compress} (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutation of the same data instances (\\emph{Permutation Decision Trees}). We then introduce the notion of {\\it Permutation Bagging} achieved using permutation decision trees without the need for random feature selection and sub-sampling.  We compare the performance of the proposed permutation bagged decision trees with Random Forest. Our model does not assume independent and identical distribution of data instances. Potential applications include scenarios where a temporal order is present in the data instances.  ",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=2MRz5bSnan",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Nithin_Nagaraj1",
        "name": "Nithin Nagaraj",
        "name_site": null,
        "openreview_id": "~Nithin_Nagaraj1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://sites.google.com/site/nithinnagaraj2/",
        "dblp_id": null,
        "google_scholar_url": "https://scholar.google.co.in/citations?hl=en",
        "orcid": "0000-0003-0097-4131",
        "linkedin_url": "nithin-nagaraj-14b07934/?trk=profile-badge&originalSubdomain=in",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "National Institute of Advanced Studies (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 2.75,
        "rating_std": 0.82915619758885,
        "confidence_mean": 4.5,
        "confidence_std": 0.5,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "IQ6GI7fM2z",
      "title": "Gradient strikes back: How filtering out high frequencies improves explanations",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones.\nHere, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods.\nThis observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our results will spur renewed interest in simpler and computationally more efficient gradient-based methods for explainability.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=IQ6GI7fM2z",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Thomas_FEL1",
        "name": "Thomas FEL",
        "name_site": "Thomas FEL, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre",
        "openreview_id": "~Thomas_FEL1",
        "position": 3,
        "gender": "M",
        "homepage_url": "https://thomasfel.me",
        "dblp_id": "274/2390",
        "google_scholar_url": "1m5Mlx4AAAAJ",
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Universit√© de Toulouse (France)",
        "countries": [
          "France"
        ],
        "country_codes": [
          "FR"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.25,
        "rating_std": 0.82915619758885,
        "confidence_mean": 3.75,
        "confidence_std": 0.4330127018922193,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 2,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PL4WWjvm9D",
      "title": "Decoupling Quantile Representations from Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The simultaneous quantile regression (SQR) technique has been used to estimate 2 uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile $(\\tau = 0.5)$ must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous\nbinary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier $f(x)$ at the median quantile and generate the full spectrum of SBQR quantile representations at different $\\tau $values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that\nquantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. We conclude with a discussion of several hypotheses arising from these findings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=PL4WWjvm9D",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Aditya_Challa1",
        "name": "Aditya Challa",
        "name_site": null,
        "openreview_id": "~Aditya_Challa1",
        "position": 1,
        "gender": null,
        "homepage_url": null,
        "dblp_id": null,
        "google_scholar_url": null,
        "orcid": null,
        "linkedin_url": null,
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Wadhwani Institute for AI (United States)",
        "countries": [
          "United States"
        ],
        "country_codes": [
          "US"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 2.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    },
    {
      "paper_id": "PL4WWjvm9D",
      "title": "Decoupling Quantile Representations from Loss Function",
      "status": "Reject",
      "normalized_status": "rejected",
      "abstract": "The simultaneous quantile regression (SQR) technique has been used to estimate 2 uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile $(\\tau = 0.5)$ must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous\nbinary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier $f(x)$ at the median quantile and generate the full spectrum of SBQR quantile representations at different $\\tau $values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that\nquantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. We conclude with a discussion of several hypotheses arising from these findings.",
      "tldr": "",
      "site_url": "https://openreview.net/forum?id=PL4WWjvm9D",
      "pdf_url": null,
      "github_url": "",
      "total_authors": 0,
      "track_name": "Main Conference",
      "author": {
        "id": "~Soma_S_Dhavala1",
        "name": "Soma S Dhavala",
        "name_site": null,
        "openreview_id": "~Soma_S_Dhavala1",
        "position": 2,
        "gender": "M",
        "homepage_url": "https://www.linkedin.com/in/somasdhavala/",
        "dblp_id": null,
        "google_scholar_url": "Rkh1zb8AAAAJ",
        "orcid": null,
        "linkedin_url": "somasdhavala/",
        "twitter_url": null,
        "primary_email": null,
        "affiliations": "Birla Institute of Technology and Science (India)",
        "countries": [
          "India"
        ],
        "country_codes": [
          "IN"
        ]
      },
      "sort_score": 0.0,
      "reviews": {
        "rating_mean": 4.2,
        "rating_std": 1.16619037896906,
        "confidence_mean": 2.6,
        "confidence_std": 0.8,
        "total_reviews": null,
        "total_reviewers": null,
        "google_scholar_citations": 0,
        "semantic_scholar_citations": 0
      }
    }
  ]
}