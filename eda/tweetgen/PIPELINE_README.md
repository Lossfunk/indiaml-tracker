# Tweet Generation Pipeline

A comprehensive, production-ready pipeline for generating tweet threads from conference data with robust checkpointing, author enrichment, and analytics integration.

## ğŸš€ Features

### Core Pipeline
- **SQLite Data Extraction**: Reads papers and authors from conference databases
- **Author Profile Enrichment**: Automatically discovers Twitter, Google Scholar, LinkedIn profiles
- **Analytics Integration**: Processes conference analytics for contextual insights
- **Tweet Thread Generation**: Creates structured tweet threads with proper formatting
- **Markdown Export**: Generates comprehensive markdown documentation
- **Robust Checkpointing**: Resume from any step if interrupted
- **Progress Tracking**: Real-time progress monitoring and status reporting

### Advanced Capabilities
- **Concurrent Processing**: Efficient batch processing with rate limiting
- **LLM Integration**: Smart profile verification using OpenAI/OpenRouter
- **Quality Detection**: Identifies spotlight papers and oral presentations
- **Domain Hashtags**: Automatically generates relevant hashtags from paper titles
- **Social Media Integration**: Proper Twitter mentions and handle formatting
- **Error Recovery**: Graceful handling of network failures and timeouts

## ğŸ“‹ Prerequisites

### Required Files
- SQLite database with conference data (in `data/` directory)
- Analytics JSON file (in `ui/indiaml-tracker/public/tracker/` directory)
- Environment configuration (`.env` file)

### Dependencies
```bash
pip install -r pipeline_requirements.txt
playwright install chromium
```

## âš™ï¸ Setup

### 1. Environment Configuration
Create a `.env` file in the `eda/tweetgen/` directory:

```bash
# OpenRouter API (recommended)
OPENROUTER_API_KEY=your_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=gpt-4o-mini

# Alternative: OpenAI API
OPENAI_API_KEY=your_openai_key_here
```

### 2. Verify Data Files
Ensure you have the required data files:
- `data/venues-icml-2025-v2.db` (or appropriate conference database)
- `ui/indiaml-tracker/public/tracker/icml-2025-analytics.json` (or appropriate analytics file)

## ğŸ¯ Usage

### Quick Start
```bash
cd eda/tweetgen
python run_pipeline.py icml-2025
```

### Available Commands

#### Run Pipeline
```bash
# Basic usage
python run_pipeline.py icml-2025

# Force restart from beginning
python run_pipeline.py icml-2025 --force-restart

# Resume from specific step
python run_pipeline.py icml-2025 --resume-from author_enrichment

# Custom output directory
python run_pipeline.py icml-2025 --output-dir /path/to/output
```

#### Check Status
```bash
python run_pipeline.py icml-2025 --status
```

#### List Available Steps
```bash
python run_pipeline.py icml-2025 --list-steps
```

### Supported Conferences
- `icml-2025`
- `icml-2024`
- `iclr-2025`
- `iclr-2024`
- `neurips-2024`

## ğŸ”„ Pipeline Steps

The pipeline consists of 8 sequential steps:

1. **Initialize**: Validate inputs and setup
2. **Data Extraction**: Get conference metadata
3. **SQLite Processing**: Extract papers and authors
4. **Author Enrichment**: Find social media profiles
5. **Analytics Processing**: Process conference analytics
6. **Tweet Generation**: Create tweet thread JSON
7. **Markdown Generation**: Generate documentation
8. **Finalize**: Organize outputs and cleanup

## ğŸ“Š Output Structure

```
eda/tweetgen/outputs/{conference}/
â”œâ”€â”€ checkpoints/                    # Checkpoint files for resuming
â”‚   â”œâ”€â”€ processing_state.json      # Pipeline state
â”‚   â”œâ”€â”€ raw_papers.json            # Extracted papers
â”‚   â”œâ”€â”€ raw_authors.json           # Extracted authors
â”‚   â”œâ”€â”€ enriched_authors.json      # Authors with social profiles
â”‚   â”œâ”€â”€ processed_analytics.json   # Processed analytics
â”‚   â””â”€â”€ tweet_thread.json          # Generated tweet thread
â”œâ”€â”€ tweet_thread.json              # Final tweet thread
â”œâ”€â”€ tweet_thread.md                # Markdown documentation
â”œâ”€â”€ summary.md                     # Executive summary
â””â”€â”€ pipeline_summary.json          # Pipeline metadata
```

## ğŸ“ Output Formats

### Tweet Thread JSON
```json
{
  "conference": "ICML",
  "year": 2025,
  "generated_at": "2025-06-29T14:30:00",
  "metadata": {
    "total_tweets": 15,
    "total_papers": 12,
    "processing_time": "45m"
  },
  "analytics_summary": {
    "india_papers": 12,
    "global_rank": 8,
    "acceptance_rate": "2.1%"
  },
  "tweets": [
    {
      "id": 1,
      "type": "analytics_opener",
      "content": "ğŸ‡®ğŸ‡³ India @ ICML 2025 Thread ğŸ§µ\n\n...",
      "mentions": [],
      "hashtags": ["#ICML2025", "#IndiaML"],
      "metadata": {...}
    }
  ]
}
```

### Individual Tweet Structure
```json
{
  "id": 3,
  "type": "paper",
  "paper_id": "12345",
  "content": "ğŸ“„ Deep Learning for Climate Modeling\n\nğŸŒŸ Spotlight ğŸ‘¥ Authors: @researcher1, Jane Doe\nğŸ›ï¸ IIT Delhi\nğŸ¯ Spotlight Paper\n\n#ICML2025 #DeepLearning",
  "mentions": ["@researcher1"],
  "hashtags": ["#ICML2025", "#DeepLearning"],
  "authors": [
    {
      "name": "John Smith",
      "affiliation": "IIT Delhi",
      "twitter_handle": "@researcher1",
      "google_scholar": "https://scholar.google.com/...",
      "position": 0
    }
  ],
  "metadata": {
    "character_count": 145,
    "is_spotlight": true,
    "is_oral": false,
    "author_count": 2
  }
}
```

## ğŸ”§ Configuration

### Pipeline Configuration
The pipeline automatically maps conferences to their data files:

```python
config_map = {
    "icml-2025": {
        "sqlite_file": "venues-icml-2025-v2.db",
        "analytics_file": "icml-2025-analytics.json"
    }
}
```

### Author Enrichment Settings
- **Concurrent requests**: 3 (configurable)
- **Request timeout**: 30 seconds
- **Rate limiting**: 2 seconds between batches
- **Browser**: Chromium (headless)

### Tweet Generation Settings
- **Max tweet length**: 280 characters
- **Max authors per tweet**: 5
- **Quality paper priority**: Spotlights > Orals > Regular

## ğŸ› ï¸ Advanced Usage

### Resume from Specific Step
If the pipeline fails or is interrupted:

```bash
# Check current status
python run_pipeline.py icml-2025 --status

# Resume from where it left off
python run_pipeline.py icml-2025

# Or resume from specific step
python run_pipeline.py icml-2025 --resume-from analytics_processing
```

### Custom Processing
For advanced users, you can import and use pipeline components directly:

```python
from pipeline.main_pipeline import TweetGenerationPipeline

pipeline = TweetGenerationPipeline("icml-2025")
results = await pipeline.run()
```

## ğŸš¨ Error Handling

The pipeline includes comprehensive error handling:

- **Network failures**: Automatic retries with exponential backoff
- **Missing data**: Graceful degradation with warnings
- **API failures**: Fallback to basic processing
- **Interrupted processing**: Automatic checkpoint saving
- **Invalid inputs**: Clear error messages with suggestions

### Common Issues

#### Missing API Keys
```
âš ï¸ Warning: No OpenAI/OpenRouter API key found. Profile verification will be basic.
```
**Solution**: Add API keys to `.env` file

#### Missing Data Files
```
âŒ Error: SQLite file not found: data/venues-icml-2025-v2.db
```
**Solution**: Ensure data files are in correct locations

#### Network Timeouts
```
âŒ Error processing Author Name: TimeoutError
```
**Solution**: Pipeline continues with basic data; re-run to retry failed authors

## ğŸ“ˆ Performance

### Typical Processing Times
- **Small conference** (50 papers): 10-15 minutes
- **Medium conference** (200 papers): 30-45 minutes  
- **Large conference** (500+ papers): 1-2 hours

### Optimization Tips
- Use SSD storage for faster checkpoint I/O
- Ensure stable internet connection for author enrichment
- Run during off-peak hours for better web scraping success rates

## ğŸ” Monitoring

### Real-time Progress
The pipeline provides detailed progress information:

```
ğŸš€ Starting Tweet Generation Pipeline for icml-2025
============================================================

ğŸ”§ Step 1: Initialize
  âœ… SQLite file: data/venues-icml-2025-v2.db
  âœ… Analytics file: ui/indiaml-tracker/public/tracker/icml-2025-analytics.json
  âœ… Initialization complete

ğŸ—„ï¸ Step 3: SQLite Processing
  ğŸ“„ Found 156 papers with Indian authors
  ğŸ‘¥ Found 312 unique authors
  ğŸ“Š Statistics: 156 papers, 89 Indian authors
  âœ… SQLite processing complete

ğŸ” Step 4: Author Enrichment
  ğŸ” Processing: Dr. Rajesh Kumar
    ğŸ¦ Found Twitter: https://x.com/rajesh_ml
    ğŸ“ Found Scholar: https://scholar.google.com/citations?user=...
  âœ… Author enrichment complete
```

### Status Monitoring
```bash
python run_pipeline.py icml-2025 --status
```

Output:
```
ğŸ“Š Pipeline Status:
========================================
Conference: icml-2025
Status: running
Progress: 62.5%
Current step: author_enrichment

âœ… Completed steps (5):
  â€¢ initialize
  â€¢ data_extraction
  â€¢ sqlite_processing
  â€¢ author_enrichment
  â€¢ analytics_processing

ğŸ“ˆ Progress:
  â€¢ total_papers: 156
  â€¢ processed_authors: 195
  â€¢ enriched_authors: 195
```

## ğŸ¤ Contributing

### Adding New Conferences
1. Add database and analytics files to appropriate directories
2. Update `config_map` in `main_pipeline.py`
3. Test with a small subset of data

### Extending Functionality
- **New social platforms**: Extend `AuthorEnricher` class
- **Custom tweet formats**: Modify `TweetGenerator` class
- **Additional analytics**: Enhance `AnalyticsProcessor` class

## ğŸ“„ License

This pipeline is part of the India@ML project and follows the same licensing terms.

## ğŸ†˜ Support

For issues and questions:
1. Check the error messages and logs
2. Verify all prerequisites are met
3. Try resuming from the last successful step
4. Check network connectivity for author enrichment
5. Ensure API keys are properly configured

## ğŸ”„ Version History

- **v1.0.0**: Initial release with full pipeline functionality
- **v1.1.0**: Added resume capabilities and improved error handling
- **v1.2.0**: Enhanced author enrichment with LLM verification
- **v1.3.0**: Added markdown generation and comprehensive documentation
